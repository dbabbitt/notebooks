{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "trn_dict = {key: value for key, value in zip(list(string.punctuation), [None]*len(string.punctuation))}\n",
    "def process_text(text, stem=True):\n",
    "    \"\"\" Tokenize text and stem words removing punctuation \"\"\"\n",
    "    text = text.translate(trn_dict)\n",
    "    tokens = word_tokenize(text)\n",
    " \n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\577342\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Transform texts to Tf-Idf coordinates\n",
    "vectorizer = TfidfVectorizer(tokenizer=process_text,\n",
    "                             stop_words=stopwords.words('english'),\n",
    "                             max_df=0.5,\n",
    "                             min_df=0.1,\n",
    "                             lowercase=True)\n",
    "tfidf_model = vectorizer.fit_transform(basic_quals_df.qualification_str.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "sents_list = basic_quals_df.qualification_str.tolist()\n",
    "X = pipeline.fit_transform(sents_list).toarray()\n",
    "#print(['X.{}'.format(fn) for fn in dir(X) if fn.startswith('to')])\n",
    "y = basic_quals_df.is_fit.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer=None, preprocessor=None,\n",
    "                             stop_words=None, max_features=5000)\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of\n",
    "# strings.\n",
    "qual_features = vectorizer.fit_transform(sents_list)\n",
    "print(type(qual_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vectorizer._tfidf', 'vectorizer.idf_', 'vectorizer.smooth_idf', 'vectorizer.use_idf']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scipy\n",
    "\n",
    "\n",
    "idf_cut = 90 #percentile, we will only consider words with an idf >= the ___ percentile of the idfs of all words\n",
    "vectorizer = TfidfVectorizer(min_df=1, stop_words='english', ngram_range=(1,2)) #unigrams and bigrams\n",
    "#vectorizer = TfidfVectorizer(min_df=1, stop_words='english', analyzer='word', tokenizer=tokenize) #just unigrams (words)\n",
    "X = vectorizer.fit_transform(sents_list)\n",
    "print(['vectorizer.{}'.format(fn) for fn in dir(vectorizer) if 'idf' in fn.lower()])\n",
    "idf = vectorizer.idf_\n",
    "idf_df = pd.DataFrame(columns=['word','idf'])\n",
    "idf_df.word = vectorizer.get_feature_names()\n",
    "idf_df.idf = idf\n",
    "idf_dict = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "select_terms = list(idf_df.word.loc[idf_df.idf >= scipy.stats.scoreatpercentile(idf, idf_cut)]) #let's only consider words with high idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(min_df=1, vocabulary=select_terms, ngram_range=(1,2), stop_words='english')\n",
    "#vectorizer = CountVectorizer(min_df=1, vocabulary=select_terms, stop_words='english', analyzer='word', tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(sents_list)\n",
    "print(['vectorizer.{}'.format(fn) for fn in dir(vectorizer) if 'idf' in fn.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qual_features.arcsin', 'qual_features.arcsinh', 'qual_features.arctan', 'qual_features.arctanh', 'qual_features.argmax', 'qual_features.argmin', 'qual_features.asformat', 'qual_features.asfptype', 'qual_features.astype', 'qual_features.ceil', 'qual_features.check_format', 'qual_features.conj', 'qual_features.conjugate', 'qual_features.copy', 'qual_features.count_nonzero', 'qual_features.data', 'qual_features.deg2rad', 'qual_features.diagonal', 'qual_features.dot', 'qual_features.dtype', 'qual_features.eliminate_zeros', 'qual_features.expm1', 'qual_features.floor', 'qual_features.format', 'qual_features.getH', 'qual_features.get_shape', 'qual_features.getcol', 'qual_features.getformat', 'qual_features.getmaxprint', 'qual_features.getnnz', 'qual_features.getrow', 'qual_features.has_canonical_format', 'qual_features.has_sorted_indices', 'qual_features.indices', 'qual_features.indptr', 'qual_features.log1p', 'qual_features.max', 'qual_features.maximum', 'qual_features.maxprint', 'qual_features.mean', 'qual_features.min', 'qual_features.minimum', 'qual_features.multiply', 'qual_features.ndim', 'qual_features.nnz', 'qual_features.nonzero', 'qual_features.power', 'qual_features.prune', 'qual_features.rad2deg', 'qual_features.reshape', 'qual_features.resize', 'qual_features.rint', 'qual_features.set_shape', 'qual_features.setdiag', 'qual_features.shape', 'qual_features.sign', 'qual_features.sin', 'qual_features.sinh', 'qual_features.sort_indices', 'qual_features.sorted_indices', 'qual_features.sqrt', 'qual_features.sum', 'qual_features.sum_duplicates', 'qual_features.tan', 'qual_features.tanh', 'qual_features.toarray', 'qual_features.tobsr', 'qual_features.tocoo', 'qual_features.tocsc', 'qual_features.tocsr', 'qual_features.todense', 'qual_features.todia', 'qual_features.todok', 'qual_features.tolil', 'qual_features.transpose', 'qual_features.trunc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(['qual_features.{}'.format(fn) for fn in dir(qual_features) if not fn.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfTransformer' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-657-f0483896e90f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    108\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                     \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfTransformer' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 2]\n",
      " [4 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70         9\n",
      "           1       0.78      0.64      0.70        11\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.71      0.71      0.70        20\n",
      "weighted avg       0.71      0.70      0.70        20\n",
      "\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'abil', 'develop', 'includ', 'knowledg', 'python', 'softwar', 'year']"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
