{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Attack Type using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "encoding = ['latin1', 'iso8859-1', 'utf-8'][1]\n",
    "attack_regex = re.compile(r\"attack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mike's half-way-done tf-idf analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_max_column(df):\n",
    "    max_length = 0\n",
    "    max_column = None\n",
    "    for column in df.columns:\n",
    "        column_length = df[column].astype('str').str.len().mean()\n",
    "        if column_length > max_length:\n",
    "            max_length = column_length\n",
    "            max_column = column\n",
    "            \n",
    "    return(max_column, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "word_analysis_path = \"../data/csv/Word Analysis\"\n",
    "csv_path =  word_analysis_path + '.csv'\n",
    "word_analysis_df = pd.read_csv(csv_path, encoding=encoding, low_memory=False)\n",
    "word_analysis_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attack Type Id</th>\n",
       "      <th>Attack Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Assassination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Armed Assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bombing/Explosion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hijacking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Hostage Taking (Barricade Incident)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Hostage Taking (Kidnapping)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Facility/Infrastructure Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Unarmed Assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attack Type Id                          Attack Type\n",
       "0               1                        Assassination\n",
       "1               2                        Armed Assault\n",
       "2               3                    Bombing/Explosion\n",
       "3               4                            Hijacking\n",
       "4               5  Hostage Taking (Barricade Incident)\n",
       "5               6          Hostage Taking (Kidnapping)\n",
       "6               7       Facility/Infrastructure Attack\n",
       "7               8                      Unarmed Assault\n",
       "8               9                              Unknown"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attack_types_path = \"../data/csv/AttackTypes\"\n",
    "csv_path =  attack_types_path + '.csv'\n",
    "attack_types_df = pd.read_csv(csv_path, encoding=encoding, low_memory=False)\n",
    "attack_types_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attack_types_df['Attack Type'].tolist().index(\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Andrew's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'relid', 'year', 'active_year', 'type_of_violence',\n",
       "       'conflict_dset_id', 'conflict_new_id', 'conflict_name', 'dyad_dset_id',\n",
       "       'dyad_new_id', 'dyad_name', 'side_a_dset_id', 'side_a_new_id', 'side_a',\n",
       "       'side_b_dset_id', 'side_b_new_id', 'side_b', 'number_of_sources',\n",
       "       'source_article', 'source_office', 'source_date', 'source_headline',\n",
       "       'source_original', 'where_prec', 'where_coordinates',\n",
       "       'where_description', 'adm_1', 'adm_2', 'latitude', 'longitude',\n",
       "       'geom_wkt', 'priogrid_gid', 'country', 'region', 'event_clarity',\n",
       "       'date_prec', 'date_start', 'date_end', 'deaths_a', 'deaths_b',\n",
       "       'deaths_civilians', 'deaths_unknown', 'best_est', 'high_est', 'low_est',\n",
       "       'isocc', 'gwno', 'gwab'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ucdp_path = \"../data/csv/UCDP\"\n",
    "csv_path =  ucdp_path + '.csv'\n",
    "ucdp_df = pd.read_csv(csv_path, encoding=encoding, low_memory=False)\n",
    "ucdp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "get_max_column(ucdp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eventid', 'id', 'ccode', 'countryname', 'startdate', 'enddate',\n",
       "       'duration', 'stday', 'stmo', 'styr', 'eday', 'emo', 'eyr', 'etype',\n",
       "       'escalation', 'actor1', 'actor2', 'actor3', 'target1', 'target2',\n",
       "       'cgovtarget', 'rgovtarget', 'npart', 'ndeath', 'repress', 'elocal',\n",
       "       'ilocal', 'sublocal', 'locnum', 'gislocnum', 'issue1', 'issue2',\n",
       "       'issue3', 'issuenote', 'nsource', 'notes', 'coder', 'acd_questionable',\n",
       "       'latitude', 'longitude', 'geo_comments', 'location_precision'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scad_path = \"../data/csv/SCAD\"\n",
    "csv_path =  scad_path + '.csv'\n",
    "scad_df = pd.read_csv(csv_path, encoding=encoding, low_memory=False)\n",
    "scad_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "get_max_column(scad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['startdate', 'city', 'country', 'perpetrator', 'weapon', 'injuries',\n",
       "       'fatalities', 'description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rand_path = \"../data/csv/RAND\"\n",
    "csv_path =  rand_path + '.csv'\n",
    "rand_df = pd.read_csv(csv_path, encoding=encoding, low_memory=False)\n",
    "rand_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "get_max_column(rand_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GWNO', 'EVENT_ID_CNTY', 'EVENT_ID_NO_CNTY', 'EVENT_DATE', 'YEAR',\n",
       "       'TIME_PRECISION', 'EVENT_TYPE', 'ACTOR1', 'ALLY_ACTOR_1', 'INTER1',\n",
       "       'ACTOR1_ID', 'ACTOR2', 'ALLY_ACTOR_2', 'INTER2', 'ACTOR2_ID',\n",
       "       'INTERACTION', 'ACTOR_DYAD_ID', 'COUNTRY', 'ADMIN1', 'ADMIN2', 'ADMIN3',\n",
       "       'LOCATION', 'LATITUDE', 'LONGITUDE', 'GEO_PRECISION', 'SOURCE', 'NOTES',\n",
       "       'FATALITIES'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "acled_path = \"../data/csv/ACLED\"\n",
    "csv_path =  acled_path + '.csv'\n",
    "acled_df = pd.read_csv(csv_path, encoding=encoding, low_memory=False)\n",
    "acled_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "get_max_column(acled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Andrew's training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Assassination', 'Hostage Taking (Kidnapping)', 'Bombing/Explosion',\n",
       "       'Facility/Infrastructure Attack', 'Armed Assault', 'Hijacking',\n",
       "       'Unknown', 'Unarmed Assault', 'Hostage Taking (Barricade Incident)'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_path = \"../data/csv/GTDB\"\n",
    "csv_path =  gtdb_path + '.csv'\n",
    "gtdb_df = pd.read_csv(csv_path, encoding=encoding, low_memory=False)\n",
    "attack_column_list = [column for column in gtdb_df.columns for m in [attack_regex.search(column)] if m]\n",
    "gtdb_df[attack_column_list]['attacktype1_txt'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "get_max_column(gtdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "gtdb_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attacktype1',\n",
       " 'attacktype1_txt',\n",
       " 'attacktype2',\n",
       " 'attacktype2_txt',\n",
       " 'attacktype3',\n",
       " 'attacktype3_txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attack_column_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Get the training data into a 20newsgroups-style bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_nan(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True\n",
    "\n",
    "def is_ns(value):\n",
    "    return len(value) > 3\n",
    "\n",
    "def concat_independendent_variables(df):\n",
    "    X = pd.Series([])\n",
    "    for row_index, row_series in df.iterrows():\n",
    "        row_concat = row_series.astype('str').str.cat(sep=' ')\n",
    "        row_concat = sq_regex.sub(r'', row_concat)\n",
    "        row_concat = nonalpha_regex.sub(r' ', row_concat)\n",
    "        X = X.append(pd.Series([row_concat]), ignore_index=True)\n",
    "    \n",
    "    return X\n",
    "\n",
    "nonalpha_regex = re.compile(r\"[^a-zA-Z]+\")\n",
    "sq_regex = re.compile(r\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "gtdb_df.fillna(value=\"\", inplace=True)\n",
    "\n",
    "important_columns = [column for column in gtdb_df.columns if (column not in attack_column_list)]\n",
    "X = concat_independendent_variables(gtdb_df[important_columns])\n",
    "y = gtdb_df['attacktype1'].map(lambda x: int(x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X = pd.Series([])\n",
    "y = pd.Series([])\n",
    "for csv_file in ['acled', 'rand', 'scad', 'ucdp', 'GTDB']:\n",
    "    if csv_file == \"GTDB\":\n",
    "        gtdb_path = \"../data/csv/GTDB\"\n",
    "        csv_path =  gtdb_path + '.csv'\n",
    "    else:\n",
    "        mike_path = \"../data/csv/mike_\"\n",
    "        csv_path =  mike_path + csv_file + '.csv'\n",
    "    df = pd.read_csv(csv_path, encoding=encoding, low_memory=False)\n",
    "    df.fillna(value=\"\", inplace=True)\n",
    "    if csv_file == \"GTDB\":\n",
    "        important_columns = [column for column in df.columns if (column not in attack_column_list)]\n",
    "    else:\n",
    "        important_columns = df.columns.tolist()[:-1]\n",
    "    X = X.append(concat_independendent_variables(df[important_columns]), ignore_index=True)\n",
    "    if csv_file == \"GTDB\":\n",
    "        y = y.append(df['attacktype1'].map(lambda x: int(x)-1), ignore_index=True)\n",
    "    else:\n",
    "        y = y.append(df[df.columns.tolist()[-1]].map(lambda x: attack_types_df['Attack Type'].tolist().index(x)), \n",
    "                     ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "\n",
    "class Bunch(dict):\n",
    "    \"\"\"Container object for datasets: dictionary-like object that\n",
    "       exposes its keys as attributes.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        dict.__init__(self, kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "csv_path = \"../data/csv/\"\n",
    "csv_files = [join(csv_path, f) for f in listdir(csv_path) if isfile(join(csv_path, f))]\n",
    "gtdb_train = Bunch(filenames=np.asarray(csv_files),\n",
    "                   target_names=attack_types_df['Attack Type'].tolist(),\n",
    "                   DESCR=None,\n",
    "                   target=np.asarray(y_train.tolist()),\n",
    "                   data=X_train.tolist(),\n",
    "                   description=\"The GTDB dataset concatoned into one column (minus the target columns)\")\n",
    "gtdb_test = Bunch(filenames=np.asarray(csv_files),\n",
    "                   target_names=attack_types_df['Attack Type'].tolist(),\n",
    "                   DESCR=None,\n",
    "                   target=np.asarray(y_test.tolist()),\n",
    "                   data=X_test.tolist(),\n",
    "                   description=\"The GTDB dataset concatoned into one column (minus the target columns)\")\n",
    "gtdb_all = Bunch(filenames=np.asarray(csv_files),\n",
    "                   target_names=attack_types_df['Attack Type'].tolist(),\n",
    "                   DESCR=None,\n",
    "                   target=np.asarray(y.tolist()),\n",
    "                   data=X.tolist(),\n",
    "                   description=\"The GTDB dataset concatoned into one column (minus the target columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description\n",
      "DESCR\n",
      "target\n",
      "target_names\n",
      "filenames\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key in gtdb_train.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../data/csv/ACLED.csv', '../data/csv/acled_df.csv',\n",
       "       '../data/csv/AttackTypes.csv', '../data/csv/GTDB.csv',\n",
       "       '../data/csv/gtdb_df.csv', '../data/csv/mike_acled.csv',\n",
       "       '../data/csv/mike_rand.csv', '../data/csv/mike_scad.csv',\n",
       "       '../data/csv/mike_ucdp.csv', '../data/csv/RAND.csv',\n",
       "       '../data/csv/rand_df.csv', '../data/csv/SCAD.csv',\n",
       "       '../data/csv/scad_df.csv', '../data/csv/UCDP.csv',\n",
       "       '../data/csv/ucdp_df.csv', '../data/csv/Word Analysis.csv'], \n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_train.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assassination',\n",
       " 'Armed Assault',\n",
       " 'Bombing/Explosion',\n",
       " 'Hijacking',\n",
       " 'Hostage Taking (Barricade Incident)',\n",
       " 'Hostage Taking (Kidnapping)',\n",
       " 'Facility/Infrastructure Attack',\n",
       " 'Unarmed Assault',\n",
       " 'Unknown']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "gtdb_train.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 1, 6, ..., 1, 2, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157465"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(gtdb_all.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The GTDB dataset concatoned into one column (minus the target columns)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_train.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(gtdb_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Peru South America Ayacucho Ayacucho district Police Police Building headquarters station school Police post Peru Shining Path SL Unknown Attacked Unknown PGIS \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\".join(gtdb_train.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(gtdb_train.target_names[gtdb_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 1, 6, 2, 2, 8, 2, 1, 0, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown\n",
      "Armed Assault\n",
      "Facility/Infrastructure Attack\n",
      "Bombing/Explosion\n",
      "Bombing/Explosion\n",
      "Unknown\n",
      "Bombing/Explosion\n",
      "Armed Assault\n",
      "Assassination\n",
      "Bombing/Explosion\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in gtdb_train.target[:10]:\n",
    "    print(gtdb_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Vectorize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105501, 97921)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(gtdb_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "count_vect.vocabulary_.get(u'hegarty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### TF-IDF the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105501, 97921)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105501, 97921)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Try a Naive Bayes model\n",
    "\n",
    "It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, gtdb_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_test.target[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' India South Asia Jharkhand Siladon Assailants set construction equipment on fire in Siladon area Jharkhand state India There were no reported casualties however construction equipment was damaged in the attack No group claimed responsibility for the incident however sources attributed the attack to the Peoples Liberation Front of India Business Construction Unknown Construction Equipment India Peoples Liberation Front of India The specific motive is unknown however sources posited that the attack was part of a bandh by the Peoples Liberation Front of India in demonstration against the death of two civilians Incendiary Arson Fire Minor likely million Three construction machines and a vehicle were damaged in this attack LWE outfit torch four vehicles in Jharkhands Khunti Hindustan Times September PLFI militants torch five machines and a car ahead of bandh in Jharkhand ZeeNews com September START Primary Collection ' => Bombing/Explosion (Facility/Infrastructure Attack)\n",
      "' Sri Lanka South Asia Eastern Sardhapura Insurgency Guerilla Action Military Military Unit Patrol Convoy Military Unit Sri Lanka Liberation Tigers of Tamil Eelam LTTE Firearms Automatic Weapon Automatic firearm PGIS ' => Armed Assault (Armed Assault)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docs_new = gtdb_test.data[:2]\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category, actual in zip(docs_new, predicted, gtdb_test.target[:2]):\n",
    "    print('%r => %s (%s)' % (doc, gtdb_train.target_names[category], gtdb_test.target_names[actual]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Build a Naive Bayes pipeline and get the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "text_clf = text_clf.fit(gtdb_train.data, gtdb_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66205449926872451"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "docs_test = gtdb_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == gtdb_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Build an SVM pipeline and get various metrics for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80954122084520053"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)),\n",
    "])\n",
    "_ = text_clf.fit(gtdb_train.data, gtdb_train.target)\n",
    "predicted = text_clf.predict(gtdb_test.data)\n",
    "np.mean(predicted == gtdb_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80681699699528919"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "score = text_clf.score(gtdb_train.data, gtdb_train.target)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.10413998, -0.71634984, -1.15611   , -1.01508382, -1.00840007,\n",
       "       -1.12391413, -1.07060169, -1.02702272, -0.90468708])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_clf.decision_function(gtdb_train.data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     precision    recall  f1-score   support\n",
      "\n",
      "                      Assassination       0.72      0.38      0.50      5798\n",
      "                      Armed Assault       0.70      0.86      0.78     12345\n",
      "                  Bombing/Explosion       0.87      0.99      0.93     25217\n",
      "                          Hijacking       0.00      0.00      0.00       187\n",
      "Hostage Taking (Barricade Incident)       0.00      0.00      0.00       273\n",
      "        Hostage Taking (Kidnapping)       0.89      0.67      0.76      3035\n",
      "     Facility/Infrastructure Attack       0.78      0.68      0.73      2905\n",
      "                    Unarmed Assault       0.88      0.12      0.21       370\n",
      "                            Unknown       0.70      0.03      0.05      1834\n",
      "\n",
      "                        avg / total       0.80      0.81      0.78     51964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\577342\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(gtdb_test.target, predicted, target_names=gtdb_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2232,  2368,  1080,     0,     0,    60,    47,     0,    11],\n",
       "       [  411, 10663,   789,     0,     0,    64,   415,     1,     2],\n",
       "       [   10,    89, 25085,     0,     0,     3,    29,     0,     1],\n",
       "       [   10,    51,    83,     0,     0,    34,     9,     0,     0],\n",
       "       [   23,   112,    87,     0,     0,    36,    11,     0,     4],\n",
       "       [  242,   499,   254,     0,     0,  2019,    19,     0,     2],\n",
       "       [    6,   375,   535,     0,     0,     8,  1976,     4,     1],\n",
       "       [   25,    74,   192,     0,     0,    29,     7,    43,     0],\n",
       "       [  149,   923,   676,     0,     0,    25,    11,     1,    49]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metrics.confusion_matrix(gtdb_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tweek the SVM classifier using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(penalty='l2', n_iter=5, random_state=42)),\n",
    "])\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "              'clf__loss': ('log', 'modified_huber'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437.9772319793701 Tue Jun 27 15:30:13 2017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "gs_clf = gs_clf.fit(gtdb_train.data, gtdb_train.target)\n",
    "t1 = time.time()\n",
    "print(t1-t0, time.ctime(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Armed Assault'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_train.target_names[gs_clf.predict([gtdb_test.data[1]])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facility/Infrastructure Attack'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_train.target_names[gtdb_test.target[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85632363674277967"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "clf__loss: 'modified_huber'\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85836348241089988"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predicted = gs_clf.predict(gtdb_test.data)\n",
    "np.mean(predicted == gtdb_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.289114475250244 Tue Jun 27 15:31:44 2017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.24778289,  0.02127555,  0.        ,  0.        ,\n",
       "        0.        ,  0.0912228 ,  0.        ,  0.63971876])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "probabilities = gs_clf.predict_proba(gtdb_train.data)\n",
    "t1 = time.time()\n",
    "print(t1-t0, time.ctime(t1))\n",
    "probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.24778289,  0.02127555,  0.        ,  0.        ,\n",
       "         0.        ,  0.0912228 ,  0.        ,  0.63971876],\n",
       "       [ 0.16890167,  0.6911619 ,  0.00847529,  0.        ,  0.        ,\n",
       "         0.03993636,  0.        ,  0.        ,  0.09152478]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "probabilities[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtdb_train.target[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gs_clf.predict(gtdb_train.data)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'97.9%'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"{0:.1f}%\".format(probabilities[1][gs_clf.predict(gtdb_train.data)[1]]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>13.5608</td>\n",
       "      <td>48.6585</td>\n",
       "      <td>15.0822</td>\n",
       "      <td>45.7717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>5.517</td>\n",
       "      <td>17.1817</td>\n",
       "      <td>5.746</td>\n",
       "      <td>15.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.538951</td>\n",
       "      <td>0.519046</td>\n",
       "      <td>0.578372</td>\n",
       "      <td>0.57955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>0.537147</td>\n",
       "      <td>0.514392</td>\n",
       "      <td>0.578505</td>\n",
       "      <td>0.579046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__loss</th>\n",
       "      <td>log</td>\n",
       "      <td>log</td>\n",
       "      <td>log</td>\n",
       "      <td>log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'vect__ngram_range': (1, 1), 'clf__loss': 'lo...</td>\n",
       "      <td>{'vect__ngram_range': (1, 2), 'tfidf__use_idf'...</td>\n",
       "      <td>{'vect__ngram_range': (1, 1), 'clf__loss': 'lo...</td>\n",
       "      <td>{'vect__ngram_range': (1, 2), 'clf__loss': 'lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.540182</td>\n",
       "      <td>0.520411</td>\n",
       "      <td>0.580264</td>\n",
       "      <td>0.581802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>0.53788</td>\n",
       "      <td>0.514759</td>\n",
       "      <td>0.578551</td>\n",
       "      <td>0.579235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.536625</td>\n",
       "      <td>0.51708</td>\n",
       "      <td>0.575543</td>\n",
       "      <td>0.576683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>0.535527</td>\n",
       "      <td>0.513248</td>\n",
       "      <td>0.577735</td>\n",
       "      <td>0.578006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.519646</td>\n",
       "      <td>0.579309</td>\n",
       "      <td>0.580164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>0.538033</td>\n",
       "      <td>0.515171</td>\n",
       "      <td>0.579229</td>\n",
       "      <td>0.579898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.292153</td>\n",
       "      <td>3.22232</td>\n",
       "      <td>0.71147</td>\n",
       "      <td>0.725204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.0108013</td>\n",
       "      <td>0.949634</td>\n",
       "      <td>0.0637496</td>\n",
       "      <td>0.454173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.00164565</td>\n",
       "      <td>0.00142453</td>\n",
       "      <td>0.00203774</td>\n",
       "      <td>0.00213447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.0011469</td>\n",
       "      <td>0.000826554</td>\n",
       "      <td>0.000610591</td>\n",
       "      <td>0.000784001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         0  \\\n",
       "mean_fit_time                                                      13.5608   \n",
       "mean_score_time                                                      5.517   \n",
       "mean_test_score                                                   0.538951   \n",
       "mean_train_score                                                  0.537147   \n",
       "param_clf__alpha                                                      0.01   \n",
       "param_clf__loss                                                        log   \n",
       "param_tfidf__use_idf                                                  True   \n",
       "param_vect__ngram_range                                             (1, 1)   \n",
       "params                   {'vect__ngram_range': (1, 1), 'clf__loss': 'lo...   \n",
       "rank_test_score                                                         15   \n",
       "split0_test_score                                                 0.540182   \n",
       "split0_train_score                                                 0.53788   \n",
       "split1_test_score                                                 0.536625   \n",
       "split1_train_score                                                0.535527   \n",
       "split2_test_score                                                 0.540046   \n",
       "split2_train_score                                                0.538033   \n",
       "std_fit_time                                                      0.292153   \n",
       "std_score_time                                                   0.0108013   \n",
       "std_test_score                                                  0.00164565   \n",
       "std_train_score                                                  0.0011469   \n",
       "\n",
       "                                                                         1  \\\n",
       "mean_fit_time                                                      48.6585   \n",
       "mean_score_time                                                    17.1817   \n",
       "mean_test_score                                                   0.519046   \n",
       "mean_train_score                                                  0.514392   \n",
       "param_clf__alpha                                                      0.01   \n",
       "param_clf__loss                                                        log   \n",
       "param_tfidf__use_idf                                                  True   \n",
       "param_vect__ngram_range                                             (1, 2)   \n",
       "params                   {'vect__ngram_range': (1, 2), 'tfidf__use_idf'...   \n",
       "rank_test_score                                                         16   \n",
       "split0_test_score                                                 0.520411   \n",
       "split0_train_score                                                0.514759   \n",
       "split1_test_score                                                  0.51708   \n",
       "split1_train_score                                                0.513248   \n",
       "split2_test_score                                                 0.519646   \n",
       "split2_train_score                                                0.515171   \n",
       "std_fit_time                                                       3.22232   \n",
       "std_score_time                                                    0.949634   \n",
       "std_test_score                                                  0.00142453   \n",
       "std_train_score                                                0.000826554   \n",
       "\n",
       "                                                                         2  \\\n",
       "mean_fit_time                                                      15.0822   \n",
       "mean_score_time                                                      5.746   \n",
       "mean_test_score                                                   0.578372   \n",
       "mean_train_score                                                  0.578505   \n",
       "param_clf__alpha                                                      0.01   \n",
       "param_clf__loss                                                        log   \n",
       "param_tfidf__use_idf                                                 False   \n",
       "param_vect__ngram_range                                             (1, 1)   \n",
       "params                   {'vect__ngram_range': (1, 1), 'clf__loss': 'lo...   \n",
       "rank_test_score                                                         14   \n",
       "split0_test_score                                                 0.580264   \n",
       "split0_train_score                                                0.578551   \n",
       "split1_test_score                                                 0.575543   \n",
       "split1_train_score                                                0.577735   \n",
       "split2_test_score                                                 0.579309   \n",
       "split2_train_score                                                0.579229   \n",
       "std_fit_time                                                       0.71147   \n",
       "std_score_time                                                   0.0637496   \n",
       "std_test_score                                                  0.00203774   \n",
       "std_train_score                                                0.000610591   \n",
       "\n",
       "                                                                         3  \n",
       "mean_fit_time                                                      45.7717  \n",
       "mean_score_time                                                     15.907  \n",
       "mean_test_score                                                    0.57955  \n",
       "mean_train_score                                                  0.579046  \n",
       "param_clf__alpha                                                      0.01  \n",
       "param_clf__loss                                                        log  \n",
       "param_tfidf__use_idf                                                 False  \n",
       "param_vect__ngram_range                                             (1, 2)  \n",
       "params                   {'vect__ngram_range': (1, 2), 'clf__loss': 'lo...  \n",
       "rank_test_score                                                         13  \n",
       "split0_test_score                                                 0.581802  \n",
       "split0_train_score                                                0.579235  \n",
       "split1_test_score                                                 0.576683  \n",
       "split1_train_score                                                0.578006  \n",
       "split2_test_score                                                 0.580164  \n",
       "split2_train_score                                                0.579898  \n",
       "std_fit_time                                                      0.725204  \n",
       "std_score_time                                                    0.454173  \n",
       "std_test_score                                                  0.00213447  \n",
       "std_train_score                                                0.000784001  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(gs_clf.cv_results_).head(4).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Make a pipeline with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2050.036012172699 Mon Jun 26 16:57:54 2017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74939394132206694"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3, 1e-4),\n",
    "              'clf__loss': ('log', 'modified_huber'),\n",
    "}\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(penalty='l2', n_iter=5, random_state=42)),\n",
    "])\n",
    "gs_all_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_all_clf = gs_all_clf.fit(gtdb_all.data, gtdb_all.target)\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0, time.ctime(t1))\n",
    "\n",
    "gs_all_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Add a \"predicted\" column to all the datasets and save them as CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258.0249540805817 Mon Jun 26 17:20:43 2017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for df in [acled_df, rand_df, scad_df, ucdp_df, gtdb_df]:\n",
    "    data = concat_independendent_variables(df).tolist()\n",
    "    df['predicted_id'] = gs_all_clf.predict(data)\n",
    "    df['predicted_type'] = df['predicted_id'].map(lambda x: gtdb_all.target_names[x])\n",
    "    df['probabilities'] = pd.Series(list(gs_all_clf.predict_proba(data)))\n",
    "    df['probability'] = df.apply(lambda row: \"{0:.1f}%\".format(row['probabilities'][row['predicted_id']]*100), axis=1)\n",
    "    df.drop(['predicted_id','probabilities'], axis=1, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0, time.ctime(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_folder = \"../data/csv/\"\n",
    "gtdb_df.to_csv(csv_folder+\"gtdb_df.csv\", sep=',', encoding=encoding, index=False)\n",
    "acled_df.to_csv(csv_folder+\"acled_df.csv\", sep=',', encoding=encoding, index=False)\n",
    "rand_df.to_csv(csv_folder+\"rand_df.csv\", sep=',', encoding=encoding, index=False)\n",
    "scad_df.to_csv(csv_folder+\"scad_df.csv\", sep=',', encoding=encoding, index=False)\n",
    "ucdp_df.to_csv(csv_folder+\"ucdp_df.csv\", sep=',', encoding=encoding, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Play around with TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 11.119472668264883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 11.119472668264883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 11.119472668264883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 11.119472668264883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 11.119472668264883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=DEFAULT, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.01)\n",
      "14.0213084208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('../py/tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### The output looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)\n",
    "training_features, testing_features, training_target, testing_target = train_test_split(features, tpot_data['class'], \n",
    "                                                                                        random_state=42)\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.95, loss=\"ls\", max_depth=7, max_features=0.25, \n",
    "                                                          min_samples_leaf=4, min_samples_split=2, n_estimators=100, \n",
    "                                                          subsample=0.55)),\n",
    "    ElasticNetCV(l1_ratio=0.25, tol=0.01)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.75, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "\n",
    "class Bunch(dict):\n",
    "    \"\"\"Container object for datasets: dictionary-like object that\n",
    "       exposes its keys as attributes.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        dict.__init__(self, kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "csv_path = \"../data/csv/\"\n",
    "csv_files = [join(csv_path, f) for f in listdir(csv_path) if isfile(join(csv_path, f))]\n",
    "gtdb_train = Bunch(filenames=np.asarray(csv_files),\n",
    "                   target_names=attack_types_df['Attack Type'].tolist(),\n",
    "                   DESCR=None,\n",
    "                   target=np.asarray(y_train.tolist()),\n",
    "                   data=X_train.tolist(),\n",
    "                   description=\"The GTDB dataset concatoned into one column (minus the target columns)\")\n",
    "gtdb_test = Bunch(filenames=np.asarray(csv_files),\n",
    "                   target_names=attack_types_df['Attack Type'].tolist(),\n",
    "                   DESCR=None,\n",
    "                   target=np.asarray(y_test.tolist()),\n",
    "                   data=X_test.tolist(),\n",
    "                   description=\"The GTDB dataset concatoned into one column (minus the target columns)\")\n",
    "gtdb_all = Bunch(filenames=np.asarray(csv_files),\n",
    "                   target_names=attack_types_df['Attack Type'].tolist(),\n",
    "                   DESCR=None,\n",
    "                   target=np.asarray(y.tolist()),\n",
    "                   data=X.tolist(),\n",
    "                   description=\"The GTDB dataset concatoned into one column (minus the target columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tpot import TPOTClassifier\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(gtdb_train.data)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39193, 55340)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# (105501, 97921) is too big\n",
    "# (53302, 65931) is too big\n",
    "# (39193, 55340)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-790c96608ffd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\577342\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\577342\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\577342\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1037\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-329baf97a830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtpot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\577342\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tpot\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \"\"\"\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;31m# Resets the imputer to be fit for the new dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train_tf.toarray(), y_train)\n",
    "np.array([np.nan, 0], dtype=np.float64)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_test_counts = count_vect.fit_transform(gtdb_test.data)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_test_counts)\n",
    "X_test_tf = tf_transformer.transform(X_test_counts)\n",
    "print(tpot.score(X_test_tf, y_test))\n",
    "\n",
    "tpot.export('../py/tpot_attack_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
