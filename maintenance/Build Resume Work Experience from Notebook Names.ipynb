{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "import sys\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from notebook_utils import NotebookUtilities\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nu = NotebookUtilities(\n",
    "    data_folder_path=osp.abspath('../data'),\n",
    "    saves_folder_path=osp.abspath('../saves')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "black_list = ['.ipynb_checkpoints', '$Recycle.Bin', '.git']\n",
    "github_repos_folder = osp.dirname(osp.abspath(osp.curdir))\n",
    "file_type = '.ipynb'\n",
    "import_statements_list = [\n",
    "    'from apyori import apriori', 'from catsim.cat import generate_item_bank', 'from catsim.irt import icc', 'from collections import defaultdict', 'from datetime import datetime',\n",
    "    'from datetime import timedelta', 'from mlxtend.frequent_patterns import apriori', 'from mlxtend.frequent_patterns import apriori, association_rules',\n",
    "    'from mlxtend.frequent_patterns import association_rules', 'from nltk import pos_tag', 'from nltk import sent_tokenize, word_tokenize', 'from nltk import word_tokenize',\n",
    "    'from nltk import word_tokenize, pos_tag', 'from nltk.chunk import ne_chunk', 'from nltk.corpus import movie_reviews', 'from nltk.corpus import stopwords',\n",
    "    'from nltk.corpus import subjectivity', 'from nltk.corpus import wordnet as wn', 'from nltk.corpus import words', 'from numpy import arange', 'from os import path as osp',\n",
    "    'from sklearn.inspection import permutation_importance', 'from sklearn.model_selection import train_test_split', 'from string import punctuation', 'from tqdm import tqdm',\n",
    "    'import catsim.plot as catplot', 'import copy', 'import csv', 'import docx', 'import en_core_web_sm', 'import humanize', 'import matplotlib.cm as cm',\n",
    "    'import matplotlib.colors as mcolors', 'import matplotlib.pyplot as plt', 'import nltk.classify.util', 'import nltk.help', 'import numpy as np', 'import operator', 'import os',\n",
    "    'import os, sys', 'import os.path as osp', 'import pandas', 'import pandas as pd', 'import platform', 'import pystan', 'import random', 'import re', 'import seaborn as sns',\n",
    "    'import shutil', 'import soundfile as sf', 'import spacy', 'import speech_recognition as sr', 'import string', 'import sys', 'import tempfile', 'from pandas import DataFrame'\n",
    "]\n",
    "comment_regex = re.compile(r'\"\\s*#')\n",
    "uncleanables_list = ['Build Resume Work Experience from Notebook Names.ipynb', 'Attic.ipynb', 'OS Path Navigation.ipynb', 'Installs.ipynb']\n",
    "def get_library_names_list(line, module_vs_nickname=-1):\n",
    "    library_names_list = line.split('\"')[1].split('\\\\n')[0].split('import ')[1].split(', ')\n",
    "    library_names_list = [library_name.split(' as ')[module_vs_nickname] for library_name in library_names_list]\n",
    "    \n",
    "    return library_names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pkgutil\n",
    "import site\n",
    "\n",
    "BUILTIN_LIBRARIES_LIST = []\n",
    "# print(\"Built-in libraries:\")\n",
    "for _, name, _ in pkgutil.iter_modules():\n",
    "    BUILTIN_LIBRARIES_LIST.append(name)\n",
    "BUILTIN_LIBRARIES_LIST += ['sys']\n",
    "modules_list = []\n",
    "# print(\"\\nThird-party modules:\")\n",
    "for path in site.getsitepackages():\n",
    "    for _, name, _ in pkgutil.iter_modules([path]):\n",
    "        modules_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from apyori import apriori ['apriori']\n",
      "from catsim.cat import generate_item_bank ['generate_item_bank']\n",
      "from catsim.irt import icc ['icc']\n",
      "from collections import defaultdict ['defaultdict']\n",
      "from datetime import datetime []\n",
      "from datetime import timedelta ['timedelta']\n",
      "from mlxtend.frequent_patterns import apriori ['apriori']\n",
      "from mlxtend.frequent_patterns import apriori, association_rules ['apriori', 'association_rules']\n",
      "from mlxtend.frequent_patterns import association_rules ['association_rules']\n",
      "from nltk import pos_tag ['pos_tag']\n",
      "from nltk import sent_tokenize, word_tokenize ['sent_tokenize', 'word_tokenize']\n",
      "from nltk import word_tokenize ['word_tokenize']\n",
      "from nltk import word_tokenize, pos_tag ['word_tokenize', 'pos_tag']\n",
      "from nltk.chunk import ne_chunk ['ne_chunk']\n",
      "from nltk.corpus import movie_reviews ['movie_reviews']\n",
      "from nltk.corpus import stopwords ['stopwords']\n",
      "from nltk.corpus import subjectivity ['subjectivity']\n",
      "from nltk.corpus import wordnet as wn ['wn']\n",
      "from nltk.corpus import words ['words']\n",
      "from numpy import arange ['arange']\n",
      "from os import path as osp ['osp']\n",
      "from sklearn.inspection import permutation_importance ['permutation_importance']\n",
      "from sklearn.model_selection import train_test_split ['train_test_split']\n",
      "from string import punctuation ['punctuation']\n",
      "from tqdm import tqdm []\n",
      "import catsim.plot as catplot ['catplot']\n",
      "import copy []\n",
      "import csv []\n",
      "import docx ['docx']\n",
      "import en_core_web_sm ['en_core_web_sm']\n",
      "import humanize []\n",
      "import matplotlib.cm as cm ['cm']\n",
      "import matplotlib.colors as mcolors ['mcolors']\n",
      "import matplotlib.pyplot as plt ['plt']\n",
      "import nltk.classify.util []\n",
      "import nltk.help []\n",
      "import numpy as np ['np']\n",
      "import operator []\n",
      "import os []\n",
      "import os, sys []\n",
      "import os.path as osp ['osp']\n",
      "import pandas []\n",
      "import pandas as pd ['pd']\n",
      "import platform []\n",
      "import pystan []\n",
      "import random []\n",
      "import re []\n",
      "import seaborn as sns ['sns']\n",
      "import shutil []\n",
      "import soundfile as sf ['sf']\n",
      "import spacy []\n",
      "import speech_recognition as sr ['sr']\n",
      "import string []\n",
      "import sys []\n",
      "import tempfile []\n",
      "from pandas import DataFrame ['DataFrame']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for import_statement in import_statements_list:\n",
    "    print(\n",
    "        import_statement,\n",
    "        [library_name.split('.')[0] for library_name in get_library_names_list(f'    \"{import_statement}\\\\n\",\\n') if library_name.split('.')[0] not in BUILTIN_LIBRARIES_LIST]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Highlight imports that are not being used\n",
    "file_dict = {}\n",
    "for sub_directory, directories_list, files_list in os.walk(github_repos_folder):\n",
    "    if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "        for file_name in files_list:\n",
    "            if file_name.endswith(file_type) and file_name not in uncleanables_list:\n",
    "                file_path = osp.join(sub_directory, file_name)\n",
    "                with open(file_path, 'r', encoding=nu.encoding_type) as f:\n",
    "                    lines_list = f.readlines()\n",
    "                    import_indices_list = []\n",
    "                    \n",
    "                    # Find the import line indices\n",
    "                    for i, line in enumerate(lines_list):\n",
    "                        if any(map(lambda x: x in line, import_statements_list)): import_indices_list.append(i)\n",
    "                    \n",
    "                    # Get all the unused imports\n",
    "                    removes_set = set()\n",
    "                    for i in import_indices_list:\n",
    "                        line = lines_list[i]\n",
    "                        if not comment_regex.search(line):\n",
    "                            library_names_list = get_library_names_list(line, -1)\n",
    "                            for library_name in library_names_list:\n",
    "                                call_regex = re.compile(rf'\\b{library_name}(\\.|\\()')\n",
    "                                is_used = False\n",
    "                                for line in lines_list[i:]:\n",
    "                                    if call_regex.search(line): is_used = True\n",
    "                                if not is_used: removes_set.add(library_name)\n",
    "                    if removes_set: file_dict[file_path] = r'\\b(' + '|'.join(sorted(removes_set)) + r')(\\b|\\.)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List imports that are not being used\n",
    "for k, v in file_dict.items(): print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of notebook names\n",
    "documents = []\n",
    "\n",
    "# List of words for each document\n",
    "words_list = []\n",
    "\n",
    "import_regex = re.compile(r'\"\\s*(from [a-z._]+ )?import ([a-z._]+(?:, )?)+( as [a-z]+)?\\\\n\",?')\n",
    "for sub_directory, directories_list, files_list in os.walk(github_repos_folder):\n",
    "    if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "        for file_name in files_list:\n",
    "            if file_name.endswith(file_type) and file_name not in uncleanables_list:\n",
    "                notebook_name = file_name.replace(file_type, '')\n",
    "                # if (notebook_name == 'Untitled'): raise\n",
    "                documents.append(notebook_name)\n",
    "                file_path = osp.join(sub_directory, file_name)\n",
    "                with open(file_path, 'r', encoding=nu.encoding_type) as f:\n",
    "                    lines_list = f.readlines()\n",
    "                    imports_list = []\n",
    "                    for line in lines_list:\n",
    "                        if import_regex.search(line):\n",
    "                            # display(line)\n",
    "                            library_names_list = get_library_names_list(line, 0)\n",
    "                            for library_name in library_names_list:\n",
    "                                if library_name.split('.')[0] not in BUILTIN_LIBRARIES_LIST: imports_list.append(library_name)\n",
    "                    words_list.append(imports_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine document names and words for each document\n",
    "document_data = [' '.join(words_list[i]) for i in range(len(words_list))]\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(document_data)\n",
    "\n",
    "# Get feature names (words) and document names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "document_names = [f'Document_{i}' for i in range(len(documents))]\n",
    "\n",
    "# Create a data frame to display the TF-IDF matrix\n",
    "tfidf_df = DataFrame(data=tfidf_matrix.toarray(), index=documents, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>get_routine_scores</th>\n",
       "      <th>catsim</th>\n",
       "      <th>words</th>\n",
       "      <th>get_first_positions</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>icc</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>plot_transition_matrix</th>\n",
       "      <th>get_element_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tool Applied Exploration</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fix Elapsed Time Simultaneity</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Analyze START Triage vs SALT Triage</th>\n",
       "      <td>0.833476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Build the OSU dataset of FRVRS Logs</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Develop the Patient Accuracy Rate Metric</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Develop the Time to First Treatment Metric</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Analyze Deidentified Simulation Voice Captures</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Analyze Gaze and Intent</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Develop the Treatment Placement Error Metric</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Develop the Triage Accuracy Metric</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  get_routine_scores  \\\n",
       "Tool Applied Exploration                        0.000000                 0.0   \n",
       "Fix Elapsed Time Simultaneity                   0.000000                 0.0   \n",
       "Analyze START Triage vs SALT Triage             0.833476                 0.0   \n",
       "Build the OSU dataset of FRVRS Logs             0.000000                 0.0   \n",
       "Develop the Patient Accuracy Rate Metric        0.000000                 0.0   \n",
       "Develop the Time to First Treatment Metric      0.000000                 0.0   \n",
       "Analyze Deidentified Simulation Voice Captures  0.000000                 0.0   \n",
       "Analyze Gaze and Intent                         0.000000                 0.0   \n",
       "Develop the Treatment Placement Error Metric    0.000000                 0.0   \n",
       "Develop the Triage Accuracy Metric              0.000000                 0.0   \n",
       "\n",
       "                                                catsim  words  \\\n",
       "Tool Applied Exploration                           0.0    0.0   \n",
       "Fix Elapsed Time Simultaneity                      0.0    0.0   \n",
       "Analyze START Triage vs SALT Triage                0.0    0.0   \n",
       "Build the OSU dataset of FRVRS Logs                0.0    0.0   \n",
       "Develop the Patient Accuracy Rate Metric           0.0    0.0   \n",
       "Develop the Time to First Treatment Metric         0.0    0.0   \n",
       "Analyze Deidentified Simulation Voice Captures     0.0    0.0   \n",
       "Analyze Gaze and Intent                            0.0    0.0   \n",
       "Develop the Treatment Placement Error Metric       0.0    0.0   \n",
       "Develop the Triage Accuracy Metric                 0.0    0.0   \n",
       "\n",
       "                                                get_first_positions  \\\n",
       "Tool Applied Exploration                                        0.0   \n",
       "Fix Elapsed Time Simultaneity                                   0.0   \n",
       "Analyze START Triage vs SALT Triage                             0.0   \n",
       "Build the OSU dataset of FRVRS Logs                             0.0   \n",
       "Develop the Patient Accuracy Rate Metric                        0.0   \n",
       "Develop the Time to First Treatment Metric                      0.0   \n",
       "Analyze Deidentified Simulation Voice Captures                  0.0   \n",
       "Analyze Gaze and Intent                                         0.0   \n",
       "Develop the Treatment Placement Error Metric                    0.0   \n",
       "Develop the Triage Accuracy Metric                              0.0   \n",
       "\n",
       "                                                sent_tokenize  icc  pos_tag  \\\n",
       "Tool Applied Exploration                                  0.0  0.0      0.0   \n",
       "Fix Elapsed Time Simultaneity                             0.0  0.0      0.0   \n",
       "Analyze START Triage vs SALT Triage                       0.0  0.0      0.0   \n",
       "Build the OSU dataset of FRVRS Logs                       0.0  0.0      0.0   \n",
       "Develop the Patient Accuracy Rate Metric                  0.0  0.0      0.0   \n",
       "Develop the Time to First Treatment Metric                0.0  0.0      0.0   \n",
       "Analyze Deidentified Simulation Voice Captures            0.0  0.0      0.0   \n",
       "Analyze Gaze and Intent                                   0.0  0.0      0.0   \n",
       "Develop the Treatment Placement Error Metric              0.0  0.0      0.0   \n",
       "Develop the Triage Accuracy Metric                        0.0  0.0      0.0   \n",
       "\n",
       "                                                plot_transition_matrix  \\\n",
       "Tool Applied Exploration                                           0.0   \n",
       "Fix Elapsed Time Simultaneity                                      0.0   \n",
       "Analyze START Triage vs SALT Triage                                0.0   \n",
       "Build the OSU dataset of FRVRS Logs                                0.0   \n",
       "Develop the Patient Accuracy Rate Metric                           0.0   \n",
       "Develop the Time to First Treatment Metric                         0.0   \n",
       "Analyze Deidentified Simulation Voice Captures                     0.0   \n",
       "Analyze Gaze and Intent                                            0.0   \n",
       "Develop the Treatment Placement Error Metric                       0.0   \n",
       "Develop the Triage Accuracy Metric                                 0.0   \n",
       "\n",
       "                                                get_element_counts  \n",
       "Tool Applied Exploration                                       0.0  \n",
       "Fix Elapsed Time Simultaneity                                  0.0  \n",
       "Analyze START Triage vs SALT Triage                            0.0  \n",
       "Build the OSU dataset of FRVRS Logs                            0.0  \n",
       "Develop the Patient Accuracy Rate Metric                       0.0  \n",
       "Develop the Time to First Treatment Metric                     0.0  \n",
       "Analyze Deidentified Simulation Voice Captures                 0.0  \n",
       "Analyze Gaze and Intent                                        0.0  \n",
       "Develop the Treatment Placement Error Metric                   0.0  \n",
       "Develop the Triage Accuracy Metric                             0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(tfidf_df.sample(10).T.sample(10).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get distinctive words for each document\n",
    "def get_distinctive_words(tfidf_matrix, document_names, top_n=5):\n",
    "    distinctive_words = {}\n",
    "\n",
    "    for i, document_name in enumerate(document_names):\n",
    "        # Get the TF-IDF scores for the current document\n",
    "        tfidf_scores = tfidf_matrix.loc[document_name]\n",
    "\n",
    "        # Get the indices of the top N TF-IDF scores\n",
    "        top_indices = tfidf_scores.argsort()[-top_n:][::-1]\n",
    "\n",
    "        # Get the corresponding words for the top N indices\n",
    "        top_words = [feature_names[index] for index in top_indices]\n",
    "\n",
    "        # Store distinctive words for the document\n",
    "        distinctive_words[document_name] = top_words\n",
    "\n",
    "    return distinctive_words\n",
    "\n",
    "# Get distinctive words for each document\n",
    "distinctive_words_per_document = get_distinctive_words(tfidf_df, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert these notebook names (and the libraries they used) into bullet points for a resume describing significant accomplishments for my position as Machine Learning Engineer. Keep the libraries used as a parenthetical suffix on the bullet point:\n",
      "\n",
      "Add a Responder Type Column to the OSU dataset of FRVRS Logs, Analyze Gaze and Intent, Analyze Issue with Logging Multiple TOOL_APPLIEDs, Analyze Preliminary Research Questions, Analyze TOOL_HOVERing as Indicative of Next Patient Choice, Find Negative Metrics in Jeremy's DCEMS Data, Orientation-Normal Sequence Analysis, Tool Applied Exploration, Fix Elapsed Time Simultaneity, Identify any Anomalous Files, Rename Files, Replace the tool applied sender missing patient ID, Reserialize DataFrame pickles, Develop the Correct Count Triage Accuracy Metric, Develop the Number of Patients Treated Metric, Develop the Number of Pulses Taken Metric, Develop the Number of Voice Captures per Session Metric, Develop the Patient Accuracy Rate Metric, Develop the R-squared Triage Accuracy Metric, Develop the Total Number of Teleports Metric, Develop the Treatment Placement Error Metric, Develop the Triage Accuracy Metric, Plot Pie Charts for Tag to SALT Dataset, Visualize every Player Gaze, Visualize Location Points, and Visualize Non-cumulative Teleportation Events: (words, get_modal_state, get_synchrony, get_subsequences, and get_routine_scores)\n",
      "Build a Model to Predict Tag Applied Type: (train_test_split, permutation_importance, words, get_modal_state, and get_subsequences)\n",
      "Build Patient Engagement timeline CSV: (apriori, get_modal_state, get_synchrony, get_subsequences, and get_routine_scores)\n",
      "Build the OSU dataset of FRVRS Logs and How to Use the Data: (en_core_web_sm, words, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Analyze Deidentified Simulation Voice Captures, Analyze Time Spent on Task, Calculate Results for Abstract Submission, Develop the Number of Patients Engaged Metric, Develop the Time to First Engagement Metric, Develop the Time to First Treatment Metric, Develop the Time To Hemorrhage Control Metric, Develop the Total User Actions Taken Metric, Develop the Triage Efficiency Metric, Plot Grouped Box and Whiskers for Time to First Engagement, Visualize Elapsed Time Spent on Patient, and Visualize every Patient Interaction: (timedelta, words, get_first_positions, get_subsequences, and get_routine_scores)\n",
      "Analyze Elapsed Time Estimation for Operations: (association_rules, apriori, timedelta, get_modal_state, and get_subsequences)\n",
      "Analyze Patient Engaged Events: (apriori, association_rules, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Analyze START Triage vs SALT Triage: (path, timedelta, words, get_modal_state, and get_subsequences)\n",
      "Catsim Exploration: (catsim, icc, plot, generate_item_bank, and words)\n",
      "Explore Bleeping Proper Nouns out of Audio Files: (soundfile, speech_recognition, words, get_first_positions, and get_subsequences)\n",
      "Exploring Voice Capture Ngrams: (word_tokenize, words, subjectivity, stopwords, and wordnet)\n",
      "Near-engagement Transactions for Affinity Analysis: (association_rules, apriori, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Rasch Analysis: (generate_item_bank, words, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Walk-Wave Sequence Analysis: (get_synchrony, get_transition_matrix, get_all_ngrams, get_transitions, and plot_element_counts)\n",
      "Rename elapsed time columns in DataFrame pickles and Rename time group columns in DataFrame pickles: (path, words, get_modal_state, get_subsequences, and get_routine_scores)\n",
      "Replace UUIDs with Cleaned and Revised File Info: (en_core_web_sm, path, words, get_modal_state, and get_subsequences)\n",
      "Develop the Correct Count by Tag Triage Accuracy Metric and Plot Stacked Horizontal Bar Charts for Over-Under Triage Errors: (seaborn, words, get_first_positions, get_subsequences, and get_routine_scores)\n",
      "Recompute Visualizations for the SDMPH 2023 Poster: (arange, seaborn, timedelta, words, and get_modal_state)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "d = {k: str(v) for k, v in distinctive_words_per_document.items()}\n",
    "grouped_dict = defaultdict(list)\n",
    "for k, v in d.items(): grouped_dict[v].append(k)\n",
    "\n",
    "# Display distinctive words for each document group\n",
    "print(\n",
    "    'Convert these notebook names (and the libraries they used) into bullet points for a resume describing significant accomplishments'\n",
    "    ' for my position as Machine Learning Engineer. Keep the libraries used as a parenthetical suffix on the bullet point:\\n'\n",
    ")\n",
    "for distinctive_words, document_group in grouped_dict.items():\n",
    "    print(f'{nu.conjunctify_nouns(document_group)}: ({nu.conjunctify_nouns(eval(distinctive_words))})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.5)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
