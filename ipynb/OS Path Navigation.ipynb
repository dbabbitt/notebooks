{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "C:\\Users\\dev\\Documents\\Repositories\\notebooks\\ipynb\\OS Path Navigation.ipynb\n",
      "['s.attempt_to_pickle', 's.data_csv_folder', 's.data_folder', 's.encoding_type', 's.load_csv', 's.load_dataframes', 's.load_object', 's.save_dataframes', 's.saves_csv_folder', 's.saves_folder', 's.saves_pickle_folder', 's.store_objects']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Config', 'In', 'Out', 'RandomForestClassifier', 'SequenceMatcher', 'Storage', 'check_4_doubles', 'check_for_typos', 'conjunctify_list', 'copyfile', 'csv', 'encoding', 'example_iterrows', 'exit', 'filepath_regex', 'get_classifier', 'get_column_descriptions', 'get_data_structs_dataframe', 'get_datastructure_prediction', 'get_dir_tree', 'get_git_lfs_track_commands', 'get_importances', 'get_input_sample', 'get_ipython', 'get_max_rsquared_adj', 'get_module_version', 'get_modules_dataframe', 'get_notebook_path', 'get_page_tables', 'get_specific_gitignore_files', 'get_struct_name', 'humanize_bytes', 'io', 'ipykernel', 'json', 'jupyter_config_dir', 'math', 'notebook_path', 'notebookapp', 'np', 'os', 'pd', 'pickle', 'plt', 'preprocess_data', 'print_all_files_ending_starting_with', 'print_all_files_ending_with', 'print_all_files_starting_with', 'quit', 're', 'remove_empty_folders', 's', 'similar', 'sm', 'sns', 'stats', 'subprocess', 'sys', 'time', 'url_regex', 'urllib']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%run ../load_magic/storage.py\n",
    "%run ../load_magic/dataframes.py\n",
    "%run ../load_magic/paths.py\n",
    "%run ../load_magic/lists.py\n",
    "%run ../load_magic/environment.py\n",
    "%pprint\n",
    "notebook_path = get_notebook_path()\n",
    "print(notebook_path)\n",
    "s = Storage()\n",
    "print(['s.{}'.format(fn) for fn in dir(s) if not fn.startswith('_')])\n",
    "[fn for fn in dir() if not fn.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\dev\\Documents\\Repositories\\covid19\\saves\\pickle\\column_description_dict.pickle\n",
      "C:\\\\Users\\dev\\Documents\\Repositories\\StatsByCountry\\saves\\pickle\\column_description_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_all_files_starting_with(\n",
    "    root_dir=[r'C:\\\\Users\\dev\\Documents\\Repositories'],\n",
    "    starts_with='column_description_dict',\n",
    "    black_list=['$RECYCLE.BIN', '$Recycle.Bin', '.git'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\\\Users\\dev\\Documents\\Repositories\\covid19\\saves\\pickle\\counties_df.pickle\n",
      "counties_df: ['State_Name', 'County_Name', 'Census_2010', 'Estimates_Base', 'Estimate_2010', 'Estimate_2011', 'Estimate_2012', 'Estimate_2013', 'Estimate_2014', 'Estimate_2015', 'Estimate_2016', 'Estimate_2017', 'Estimate_2018', 'Estimate_2019', 'Land_Area', 'Population_Density', 'Wikipedia_URL', 'Wikipedia_ID', 'County_Coordinates', 'Area_Water', 'Area_Total', 'Time_Zones', 'Time_Zones_Summer_DST', 'Founded', 'Seat', 'Website', 'Congressional_Districts', 'Largest_Cities', 'Named_For', 'County', 'Largest_Township', 'Area_Codes', 'FIPS_Codes', 'Largest_Community', 'GNIS_Feature_IDs', 'Incorporated', 'Largest_Village', 'County_Seat', 'Parish', 'Regions', 'Government_Mayor', 'Elevation', 'Mottos', 'Government_Type', 'ZIP_Codes', 'Independent_City', 'Nicknames', 'Largest_Borough', 'Highest_Elevation', 'Designated', 'Government_Body', 'Boroughs', 'Demonyms', 'Established', 'Government_Commissioner', 'Metro_Area', 'Area_Independent_City', 'Shire_Town', 'State_Routes', 'Area_Rank', 'Interstates', 'Settled', 'U_S_Routes', 'Largest_Municipality', 'Waterways', 'Government_Vice_Mayor', 'Indiana_House_Of_Representatives_Districts', 'Largest_CDP', 'Airports', 'Area_Percentage', 'Area_Metro', 'City', 'Government_State_Senate', 'Public_Transit', 'Government_District_Attorney', 'Area_County', 'Area_Rank_Region', 'GDP', 'Indiana_Senate_Districts', 'Incorporated_As_Town', 'GDP_2018', 'Government_Council', 'Incorporated_As_City', 'CSA', 'Government_Manager', 'Government_President', 'Incorporated_Municipalities', 'County_Seat_And_Largest_City', 'Government_County_Board', 'ZIP_Code_Prefixes', 'Metro', 'Metropolitan_Area', 'Incorporated_Cities', 'Government_Board_President', 'Pre_Incorporation_County', 'Government_County_Administrator', 'Primary_Airport', 'Parish_Seat', 'Government_Supervisors', 'Formed', 'Government_Assembly_Members', 'Zip_Code', 'South_Shore_Line_Stations', 'Government_Executive', 'City_And_County', 'Amtrak_Stations', 'Southern_Portion_Summer_DST', 'West_Wendover', 'Lowest_Elevation', 'Government_US_House', 'Kenton_Unofficially', 'Nara_Visa_Unofficially', 'Northern_Portion_Summer_DST', 'Southern_Portion', 'Lowest_Elevation_At_Lake_Michigan', 'Consolidated', 'Organized', 'GDP_Per_Capita', 'Northern_Portion', 'West_Wendover_Summer_DST', 'ZIP', 'Nara_Visa_Unofficially_Summer_DST', 'Secondary_Airport', 'Commuter_Rail', 'Kenton_Unofficially_Summer_DST', 'Major_Airport', 'Elevation_Mean', 'Government_Chief_Executive_Officer', 'Founded_By', 'Government_State_Reps', 'Existed', 'Government_Governing_Body', 'Major_Waterways', 'Area_Land_CDP', 'South_Of_Salmon_River_Summer_DST', 'Abolished', 'GNIS_ID', 'Area_Consolidated_City_County', 'Government_Comptroller', 'Area_City', 'Most_Of_County', 'Type', 'Western_Portion_Summer_DST', 'Incorporated_Independent_City', 'Rapid_Transit', 'Area_Unified_Borough', 'Re_Organized', 'Nan_Summer_DST', 'Western_Part_Of_County_Summer_DST', 'Geocode', 'Government_US_Rep', 'Government_Delegate', 'Government_State_Assemblyman', 'Southern_Fifth', 'Southern_Fifth_Summer_DST', 'Government_President_Board_Of_Aldermen', 'Government_City_Attorney', 'Most_Of_County_Summer_DST', 'Eastern_Part_Of_County', 'Eastern_Portion_Summer_DST', 'Government_County_Council', 'Created', 'Eastern_Portion', 'Area_Urban', 'Western_Portion', 'Government_Virginia_Senate', 'Interstate_And_US_Routes', 'Government_Board_Of_County_Commissioners', 'Government_Administrative_Officer', 'Area_State_Capital_City_And_Borough', 'Primary', 'Highways_Freeways', 'Home_Rule_City', 'Largest_Settlement', 'Northwestern', 'Government_Houses_Of_Delegates', 'Named', 'Government_Alaska_House', 'Government_United_States_Representatives', 'Founded_Date', 'Area_Water_CDP', 'Borough_Seat', 'Government_Commissioner_Of_Revenue', 'U_S_And_State_Routes', 'Area_City_And_County', 'Dimensions_Width', 'Reference_No', 'Area_Urban_CDP', 'Government_US_Senate', 'Historic_Colony', 'Area_Borough', 'Northwestern_Summer_DST', 'American', 'Mission', 'Colonized', 'Districts', 'North_Of_Salmon_River_Summer_DST', 'Borough_Created', 'Government_Council_Chair', 'South_Of_Salmon_River', 'Area_State_Capital_And_Consolidated_City_County', 'Area_Density', 'Highest_Elevation_SW_Galena_Twp', 'North_Of_Salmon_River', 'Government_Alaska_Senate', 'Chartered', 'State_Highways', 'Largest_Place', 'Dimensions_Length', 'English', 'Primary_Summer_DST', 'Western_Part_Of_County', 'ZIP_Code_Format', 'Government_Treasurer']\n",
      "\n",
      "C:\\\\Users\\dev\\Documents\\Repositories\\covid19\\saves\\pickle\\scraped_counties_df.pickle\n",
      "scraped_counties_df: ['state_name', 'county_name', 'column_name', 'column_value']\n",
      "\n",
      "C:\\\\Users\\dev\\Documents\\Repositories\\covid19\\saves\\pickle\\states_covid19_df.pickle\n",
      "states_covid19_df: ['Date', 'State_Name', 'FIPS', 'Confirmed_Cumulative', 'Deaths_Cumulative', 'Deaths_Doubling', 'Confirmed_Doubling', 'State_Abbreviation', 'DD_Smoothed']\n",
      "\n",
      "C:\\\\Users\\dev\\Documents\\Repositories\\covid19\\saves\\pickle\\us_states_df.pickle\n",
      "us_states_df: ['Date', 'State_Name', 'FIPS', 'Confirmed_Cumulative', 'Deaths_Cumulative', 'Deaths_Doubling', 'Confirmed_Doubling', 'State_Abbreviation', 'DD_Smoothed']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "black_list = ['$RECYCLE.BIN', '$Recycle.Bin', '.git']\n",
    "for sub_directory, directories_list, files_list in os.walk(r'C:\\\\Users\\dev\\Documents\\Repositories'):\n",
    "    if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "        for file_name in files_list:\n",
    "            if file_name.endswith('_df.pickle'):\n",
    "                pickle_path = os.path.join(sub_directory, file_name)\n",
    "                try:\n",
    "                    df = pd.read_pickle(pickle_path)\n",
    "                except:\n",
    "                    with open(pickle_path, 'rb') as handle:\n",
    "                        try:\n",
    "                            df = pickle.load(handle)\n",
    "                        except:\n",
    "                            pass\n",
    "                columns_list = df.columns.tolist()\n",
    "                if any(map(lambda x: 'state_name' in str(x).lower(), columns_list)):\n",
    "                    df_name = file_name.split('.')[0]\n",
    "                    print()\n",
    "                    print(pickle_path)\n",
    "                    print(f'{df_name}: {columns_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us_stats_df: ['White_Percent', 'Black_Percent', 'Hispanic_Percent', 'Asian_Percent', 'Native_Percent', 'Islander_Percent', 'Multi_Percent', 'Gini_Index', 'Effectiveness_Rank', 'Health_Care_Score', 'Education_Score', 'Economy_Score', 'Infrastructure_Score', 'Opportunity_Score', 'Fiscal_Stability_Score', 'Crime_Corrections_Score', 'Natural_Environment_Score', 'district_abbreviation', 'GDP_Rank', 'GDP_2018', 'GDP_Percent', 'State_Region', 'Homicide_Rate_2018', 'Homicide_Rate_2017', 'Homicide_Rate_2014', 'Homicide_Rate_2010', 'Homicide_Rate_2005', 'Homicide_Rate_2000', 'Homicide_Rate_1996', 'Guns_Rank', 'Guns_Per_Capita', 'Guns_Registered', 'Suicide_Rate_2017', 'Suicide_Deaths_2017', 'Suicide_Rate_2016', 'Suicide_Deaths_2016', 'Suicide_Rate_2015', 'Suicide_Deaths_2015', 'Suicide_Rate_2014', 'Suicide_Deaths_2014', 'Suicide_Rate_2005', 'Suicide_Deaths_2005', 'Total_Inhabitants_2010', 'Inhabitants_Per_Square_Mile_2010', 'Total_Murder_Deaths_2010', 'Total_Gun_Murder_Deaths_2010', 'Gun_Ownership_Percent_2013', 'Murder_Rate_2010', 'Gun_Murder_Rate_2010', 'State_FIPS', 'State_Population', 'Gun_Suicide_Deaths', 'Gun_Suicide_Rate', 'Google_Suggest_Unique', 'text_x', 'text_y', 'label_line_d', 'Google_Suggest_Common', 'Google_Suggest_First', 'Public_Access_to_Information', 'Political_Financing', 'Electoral_Oversight', 'Executive_Accountability', 'Legislative_Accountability', 'Judicial_Accountability', 'State_Budget_Processes', 'State_Civil_Service_Management', 'Procurement', 'Internal_Auditing', 'Lobbying_Disclosure', 'Ethics_Enforcement_Entities', 'State_Pension_Fund_Management', 'outline_d', 'centroid_x', 'centroid_y']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file_name in os.listdir(s.saves_pickle_folder):\n",
    "    if file_name.endswith('_df.pickle'):\n",
    "        df_name = file_name.split('.')[0]\n",
    "        df = s.load_object(df_name)\n",
    "        columns_list = df.columns.tolist()\n",
    "        if 'district_abbreviation' in columns_list:\n",
    "            print(f'{df_name}: {columns_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "\n",
    "repos_list = [r'C:\\Users\\dev\\Documents\\Repositories\\covid19', r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\flask',\n",
    "              r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\pt', r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\pymc3',\n",
    "              r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\rpc', r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\test',\n",
    "              r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\tf', r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\x']\n",
    "envs_list = ['covid19', 'flask', 'pt', 'pymc3', 'rpc', 'test', 'tf', 'x']\n",
    "for repo_path, env_name in zip(repos_list, envs_list):\n",
    "    print()\n",
    "    print('-'*len(env_name))\n",
    "    print(env_name)\n",
    "    print('-'*len(env_name))\n",
    "    command_str = f'''\n",
    "cd \"{repo_path}\"\n",
    "conda activate {env_name}\n",
    "conda env export --name {env_name} -f tmp_environment.yml\n",
    "conda deactivate\n",
    "'''\n",
    "    process = subprocess.Popen(r'C:\\Program Files\\PowerShell\\7\\pwsh.exe', stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    std_out, std_err = process.communicate(command_str.encode(encoding='utf-8'))\n",
    "    #print(std_out.decode())\n",
    "    if std_err is not None:\n",
    "        print(std_err.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_pkgs_dict = {}\n",
    "for repo_path, env_name in zip(repos_list, envs_list):\n",
    "    file_path = os.path.join(repo_path, 'tmp_environment.yml')\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines_list = file.read().split('\\n')[5:-2]\n",
    "        pkgs_list = [line_str.split('=')[0].split(' ')[-1] for line_str in lines_list]\n",
    "        env_pkgs_dict[env_name] = set(pkgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "union_set = set()\n",
    "for env_name, pkgs_set in env_pkgs_dict.items():\n",
    "    union_set = union_set.union(pkgs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intersection_set = union_set.copy()\n",
    "for env_name, pkgs_set in env_pkgs_dict.items():\n",
    "    intersection_set = intersection_set.intersection(pkgs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - prompt-toolkit\n",
      "  - vs2015_runtime\n",
      "  - backcall\n",
      "  - jedi\n",
      "  - parso\n",
      "(prompt-toolkit|vs2015_runtime|backcall|jedi|parso)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repo_path = r'C:\\Users\\dev\\Documents\\Repositories\\job-hunting'\n",
    "file_path = os.path.join(repo_path, 'tmp_environment.yml')\n",
    "with open(file_path, 'r') as file:\n",
    "    lines_list = file.read().split('\\n')[5:-2]\n",
    "    pkgs_list = [line_str.split('=')[0].split(' ')[-1] for line_str in lines_list]\n",
    "    pkgs_set = set(pkgs_list)\n",
    "missing_set = intersection_set - pkgs_set\n",
    "for pkg_name in missing_set:\n",
    "    print(f'  - {pkg_name}')\n",
    "print(f'({\"|\".join(missing_set)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_files_containing(root_dir=r'C:\\Users\\dev\\Documents\\Repositories\\job-hunting', contains_str='test', black_list=['.git']):\n",
    "    file_path_list = []\n",
    "    if type(root_dir) == list:\n",
    "        root_dir_list = root_dir\n",
    "    else:\n",
    "        root_dir_list = [root_dir]\n",
    "    if type(contains_str) == list:\n",
    "        contains_list = contains_str\n",
    "    else:\n",
    "        contains_list = [contains_str]\n",
    "    for root_dir in root_dir_list:\n",
    "        for sub_directory, directories_list, files_list in os.walk(root_dir):\n",
    "            if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "                for file_name in files_list:\n",
    "                    contains_bool = False\n",
    "                    for contains_str in contains_list:\n",
    "                        contains_bool = contains_bool or (contains_str in file_name)\n",
    "                    if contains_bool:\n",
    "                        file_path = os.path.join(sub_directory, file_name)\n",
    "                        file_path_list.append(file_path)\n",
    "    \n",
    "    return file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contains_str = 'test'\n",
    "file_path_list = get_all_files_containing(contains_str=contains_str)\n",
    "for file_path in file_path_list:\n",
    "    os.rename(file_path, file_path.replace(contains_str, 'job_hunting'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['os._Environ', 'os._putenv', 'os._unsetenv', 'os.environ', 'os.getenv', 'os.putenv', 'os.supports_bytes_environ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(['os.{}'.format(fn) for fn in dir(os) if 'env' in fn.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GDAL_VERSION']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "[key for key in dict(os.environ).keys() if 'gdal' in key.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**C:\\OSGeo4W64\\apps\\Python27\\Lib\\site-packages\\GDAL-2.4.1-py2.7.egg-info**\n",
      "**C:\\OSGeo4W64\\apps\\Python37\\lib\\site-packages\\GDAL-2.4.1-py3.7.egg-info**\n",
      "**C:\\OSGeo4W64\\apps\\qgis\\python\\plugins\\processing\\algs\\gdal**\n",
      "**C:\\OSGeo4W64\\apps\\qgis\\python\\plugins\\processing\\images\\gdaltools**\n",
      "**C:\\OSGeo4W64\\bin\\gdalplugins**\n",
      "**C:\\OSGeo4W64\\share\\gdal**\n",
      "**C:\\Program Files\\Google\\Google Earth Pro\\client\\res\\gdal**\n",
      "**C:\\ProgramData\\Anaconda3\\envs\\covid19\\Lib\\site-packages\\GDAL-3.0.4.dist-info**\n",
      "**C:\\ProgramData\\Anaconda3\\envs\\covid19\\Lib\\site-packages\\osgeo\\data\\gdal**\n",
      "**C:\\ProgramData\\Anaconda3\\envs\\covid19\\Lib\\site-packages\\osgeo\\gdalplugins**\n",
      "**C:\\ProgramData\\Anaconda3\\envs\\covid19\\Lib\\site-packages\\osgeo\\include\\gdal**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\gdal-3.0.2-py37hdf43c64_0**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\gdal-3.0.2-py37hdf43c64_0\\Lib\\site-packages\\GDAL-3.0.2-py3.7-win-amd64.egg-info**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\gdal-3.0.2-py38hdf43c64_0**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\gdal-3.0.2-py38hdf43c64_0\\Lib\\site-packages\\GDAL-3.0.2-py3.8-win-amd64.egg-info**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\gdal-3.0.4-py38h3ba59e7_6**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\gdal-3.0.4-py38h3ba59e7_6\\Lib\\site-packages\\GDAL-3.0.4-py3.8-win-amd64.egg-info**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\libgdal-3.0.2-h1155b67_0**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\libgdal-3.0.2-h1155b67_0\\Library\\share\\doc\\gdal**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\libgdal-3.0.2-h1155b67_0\\Library\\share\\gdal**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\libgdal-3.0.4-h821c9b7_6**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\libgdal-3.0.4-h821c9b7_6\\Library\\share\\doc\\gdal**\n",
      "**C:\\ProgramData\\Anaconda3\\pkgs\\libgdal-3.0.4-h821c9b7_6\\Library\\share\\gdal**\n",
      "**C:\\Users\\577342\\AppData\\Local\\Continuum\\anaconda3\\envs\\df\\Lib\\site-packages\\django\\contrib\\gis\\gdal**\n",
      "**C:\\Users\\577342\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\django\\contrib\\gis\\gdal**\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A GDAL API version must be specified.\n",
    "# Provide a path to gdal-config using a GDAL_CONFIG environment variable or use a GDAL_VERSION environment variable.\n",
    "for root_dir in ['C:\\\\', 'D:\\\\']:\n",
    "    for sub_directory, directories_list, files_list in os.walk(root_dir):\n",
    "        if 'gdal' in sub_directory.split(os.sep)[-1].lower():\n",
    "            print(f'**{sub_directory}**')\n",
    "        else:\n",
    "            for file_name in files_list:\n",
    "                if ('gdal' in file_name.lower()) and ('config' in file_name.lower()):\n",
    "                    file_path = os.path.join(sub_directory, file_name)\n",
    "                    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\n",
      "C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Anaconda3 (64-bit)\n",
      "C:\\Users\\577342\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Anaconda3 (64-bit)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for root_dir in ['C:\\\\', 'D:\\\\']:\n",
    "    for sub_directory, directories_list, files_list in os.walk(root_dir):\n",
    "        if 'Anaconda3' in sub_directory.split(os.sep)[-1]:\n",
    "            print(sub_directory)\n",
    "            !start %windir%\\explorer.exe \"os.path.abspath(sub_directory)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Scripts\\jupyter-lab.exe\n",
      "C:\\ProgramData\\Anaconda3\\Scripts\\jupyter-labextension.exe\n",
      "C:\\ProgramData\\Anaconda3\\Scripts\\jupyter-labhub.exe\n",
      "C:\\Users\\577342\\AppData\\Local\\Continuum\\anaconda3\\Scripts\\jupyter-lab.exe\n",
      "C:\\Users\\577342\\AppData\\Local\\Continuum\\anaconda3\\Scripts\\jupyter-labextension.exe\n",
      "C:\\Users\\577342\\AppData\\Local\\Continuum\\anaconda3\\Scripts\\jupyter-labhub.exe\n",
      "C:\\Users\\577342\\AppData\\Local\\Temp\\pip-uninstall-y0ov4b4u\\jupyter-lab.exe\n",
      "C:\\Users\\577342\\AppData\\Local\\Temp\\pip-uninstall-y0ov4b4u\\jupyter-labextension.exe\n",
      "C:\\Users\\577342\\AppData\\Local\\Temp\\pip-uninstall-y0ov4b4u\\jupyter-labhub.exe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_all_files_ending_starting_with(\n",
    "    root_dir='C:\\\\',\n",
    "    ends_with='.exe',\n",
    "    starts_with='jupyter-lab',\n",
    "    black_list=['$RECYCLE.BIN', '$Recycle.Bin'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Ignore big files (GitHub will warn you when pushing files larger than 50 MB. You will not be allowed to\n",
      "# push files larger than 100 MB.) Tip: If you regularly push large files to GitHub, consider introducing\n",
      "# Git Large File Storage (Git LFS) as part of your workflow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get_specific_gitignore_files('data-foundations', repository_dir=r'D:\\Documents\\Repositories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>19</th>\n",
       "      <th>21</th>\n",
       "      <th>17</th>\n",
       "      <th>15</th>\n",
       "      <th>3</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Updated</th>\n",
       "      <td>2019-05-29 12:10:00</td>\n",
       "      <td>2019-10-11 12:16:00</td>\n",
       "      <td>2020-01-27 20:49:00</td>\n",
       "      <td>2019-09-26 12:16:00</td>\n",
       "      <td>2019-10-09 12:53:00</td>\n",
       "      <td>2019-08-13 15:43:00</td>\n",
       "      <td>2020-04-10 13:33:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Due Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Origin Created Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCCB Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Review Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date of Last Update</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dry-Run: Test Date/Time</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Change completion date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date Needed</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Approved Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline end date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>End Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assigned Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Start Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline start date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Origination Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         4                   19  \\\n",
       "Updated                 2019-05-29 12:10:00 2019-10-11 12:16:00   \n",
       "Due Date                                NaT                 NaT   \n",
       "Origin Created Date                     NaT                 NaT   \n",
       "SCCB Date                               NaT                 NaT   \n",
       "Review Date                             NaT                 NaT   \n",
       "Date of Last Update                     NaT                 NaT   \n",
       "Dry-Run: Test Date/Time                 NaT                 NaT   \n",
       "Change completion date                  NaT                 NaT   \n",
       "Date Needed                             NaT                 NaT   \n",
       "Approved Date                           NaT                 NaT   \n",
       "Baseline end date                       NaT                 NaT   \n",
       "End Date                                NaT                 NaT   \n",
       "Assigned Date                           NaT                 NaT   \n",
       "Start Date                              NaT                 NaT   \n",
       "Baseline start date                     NaT                 NaT   \n",
       "Origination Date                        NaT                 NaT   \n",
       "\n",
       "                                         21                  17  \\\n",
       "Updated                 2020-01-27 20:49:00 2019-09-26 12:16:00   \n",
       "Due Date                                NaT                 NaT   \n",
       "Origin Created Date                     NaT                 NaT   \n",
       "SCCB Date                               NaT                 NaT   \n",
       "Review Date                             NaT                 NaT   \n",
       "Date of Last Update                     NaT                 NaT   \n",
       "Dry-Run: Test Date/Time                 NaT                 NaT   \n",
       "Change completion date                  NaT                 NaT   \n",
       "Date Needed                             NaT                 NaT   \n",
       "Approved Date                           NaT                 NaT   \n",
       "Baseline end date                       NaT                 NaT   \n",
       "End Date                                NaT                 NaT   \n",
       "Assigned Date                           NaT                 NaT   \n",
       "Start Date                              NaT                 NaT   \n",
       "Baseline start date                     NaT                 NaT   \n",
       "Origination Date                        NaT                 NaT   \n",
       "\n",
       "                                         15                  3   \\\n",
       "Updated                 2019-10-09 12:53:00 2019-08-13 15:43:00   \n",
       "Due Date                                NaT                 NaT   \n",
       "Origin Created Date                     NaT                 NaT   \n",
       "SCCB Date                               NaT                 NaT   \n",
       "Review Date                             NaT                 NaT   \n",
       "Date of Last Update                     NaT                 NaT   \n",
       "Dry-Run: Test Date/Time                 NaT                 NaT   \n",
       "Change completion date                  NaT                 NaT   \n",
       "Date Needed                             NaT                 NaT   \n",
       "Approved Date                           NaT                 NaT   \n",
       "Baseline end date                       NaT                 NaT   \n",
       "End Date                                NaT                 NaT   \n",
       "Assigned Date                           NaT                 NaT   \n",
       "Start Date                              NaT                 NaT   \n",
       "Baseline start date                     NaT                 NaT   \n",
       "Origination Date                        NaT                 NaT   \n",
       "\n",
       "                                         27  \n",
       "Updated                 2020-04-10 13:33:00  \n",
       "Due Date                                NaT  \n",
       "Origin Created Date                     NaT  \n",
       "SCCB Date                               NaT  \n",
       "Review Date                             NaT  \n",
       "Date of Last Update                     NaT  \n",
       "Dry-Run: Test Date/Time                 NaT  \n",
       "Change completion date                  NaT  \n",
       "Date Needed                             NaT  \n",
       "Approved Date                           NaT  \n",
       "Baseline end date                       NaT  \n",
       "End Date                                NaT  \n",
       "Assigned Date                           NaT  \n",
       "Start Date                              NaT  \n",
       "Baseline start date                     NaT  \n",
       "Origination Date                        NaT  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xlsx_dir = os.path.join(s.data_folder, 'xlsx')\n",
    "file_path = os.path.join(xlsx_dir, 'Copy of DI2E Framework Jira 2020-04-30T16_03_25-0400.xlsx')\n",
    "jira_df = pd.read_excel(os.path.abspath(file_path))\n",
    "jira_df.columns = ['Project', 'Key', 'Summary', 'Issue Type', 'Status', 'Priority', 'Resolution', 'Assignee', 'Reporter',\n",
    "                   'Creator', 'Created', 'Last Viewed', 'Updated', 'Resolved', 'Affects Version/s', 'Fix Version/s',\n",
    "                   'Component/s', 'Due Date', 'Votes', 'Watchers', 'Images', 'Original Estimate', 'Remaining Estimate',\n",
    "                   'Time Spent', 'Work Ratio', 'Sub-Tasks', 'Linked Issues', 'Environment', 'Description', 'Security Level',\n",
    "                   'Progress', 'Σ Progress', 'Σ Time Spent', 'Σ Remaining Estimate', 'Σ Original Estimate',\n",
    "                   'Labels', 'Time to First Response - SSA', 'Components Hack',\n",
    "                   'Time to First Response - ISEA', 'Time to Resolution - ISEA', 'Time to Resolution - T3',\n",
    "                   'Test Plan', 'Time to Resolution - T2', 'Time to Resolution - T1', 'SMC/TMC Comment', 'Pending',\n",
    "                   'PMO Disposition', 'Issue Priority', 'Requirements', 'Assignee - Optional',\n",
    "                   'Pending customer > 3 work days', 'Created By', 'Ops Priority', 'SCCB Comment',\n",
    "                   'Time Waiting on Customer', 'ECM Comment', 'Origin Created Date', 'Total Cost',\n",
    "                   'Waiting for Customer (3 Days)', 'Waiting for Customer (5 Days)', '3rd party development team',\n",
    "                   'Total time open < day', 'Total time open < week', 'My Watchers', 'Parent Link', 'Priority Override',\n",
    "                   'SCCB Dispostion', 'Team', 'SCCB Date', 'Assign CCM tech within 2 hours', 'CA Ticket #', 'Requirement Number',\n",
    "                   'Total time open < month', 'Change Order', 'Progress.1', 'Definition of Done', 'Requirement', 'Tester', 'Network',\n",
    "                   'Requester', 'Requirement Reference', 'Requirement Type', 'Found In Version', 'Original story points', 'Target end',\n",
    "                   'Target start', 'Review Date', 'Customer', 'Originating Site', 'External Parties', 'WP', 'Potential Resources',\n",
    "                   'Risk Consequence', 'Risk Probability', 'Engineer', 'SCR CAT Level', 'EPR #', 'Environment (select list)',\n",
    "                   'Customer Priority', 'Section', 'Email', 'Software Patch #', 'TPR Originator', 'Parent Issue Key',\n",
    "                   'Problem Statement', 'Project Manager', 'Insight Test Delete Me', 'Staging URL', 'Engineering Priority',\n",
    "                   'Peer Review Comments', 'Date of Last Update', 'Level of Test', 'Last Comment', 'Citizenship', 'ISEA T2FR',\n",
    "                   'SSA T2R', 'SSA T2FR', 'Requester Organization', 'References', 'PC time WSM to Implementation',\n",
    "                   'Contractor Test: Assigned Test Engineer(s)', 'PC time Implementation to Ready to Verify',\n",
    "                   'PC time Open to Site Working', 'Assumptions & Constraints', 'General Support T2R',\n",
    "                   'PC time Ready to Verify to Verified', 'Email Users', 'General Support T2FR', 'PC time Verified to Close',\n",
    "                   'Verification Method(s)', 'Impact (text)', 'SSA T2R - T3', 'TPR Status', 'SSA T2R - T2', 'SSA T2R - T1',\n",
    "                   'Approvers', 'ISEA T2R', 'TPR Tracking Number', 'External Tracking #', 'IAVA', 'DoD IT Registration No',\n",
    "                   'Development', 'Weighted Priority', 'Test Data', 'Tasked Team', 'Resolution Details', 'Units', 'PercentDone',\n",
    "                   'Dry-Run: Bugs Discovered', 'DueTime', 'Epic Color', 'Contractor Test: Bugs Discovered', 'Component & Category',\n",
    "                   'Rank (Obsolete)', 'Sprint', 'Epic Link', 'External ID', 'zStatus', 'Epic Name', 'Issue Source', 'Epic Status',\n",
    "                   'Epic Colour', 'zResolution', 'Release', 'Reviewer', 'Test Procedure/s Affected', 'Request participants',\n",
    "                   'Customer Request Type', 'Organizations', 'Satisfaction', 'PC time open to site working',\n",
    "                   'Auto close after 2 working days at Resolved', 'DevTool', 'Dry-Run: Test Date/Time', 'Flagged', 'Epic/Theme',\n",
    "                   'Pending customer response > 3 working days', 'Story Points', 'Success Criteria',\n",
    "                   'Pending customer response > 2 working days', 'Version', 'AFGIMSSD_Time to Resolution', 'Group Reporter',\n",
    "                   'Justification', 'Time to Resolution for Service Requests', 'Problem Time to Resolution', 'Time to resolution',\n",
    "                   'Project Category', 'Product categorization', 'Change reason', 'Change completion date', 'Urgency', 'Objective',\n",
    "                   'Impact', 'Product', 'System / Project Name', 'Operational categorization', 'Change managers', 'TestInsight',\n",
    "                   'Impacts Project(s)', 'Time to first response', 'Time to close after resolution', 'Time to approve normal change',\n",
    "                   'CAB', 'Time in Escalation', 'Source', 'Investigation reason', 'Root cause', 'Time in T2 (14 days)',\n",
    "                   'Time in T2 (7 days)', 'Time in T2 (7 days & 14 days)', 'Time to first assign for Problem',\n",
    "                   'Time to first Assigned for Problem', 'Developer', 'Fixed In Version', 'CCB Approved', 'Date Needed',\n",
    "                   'Pending customer > 5 work days', 'Auto close after 3 working days', 'Approved Date',\n",
    "                   'Auto close after 3 working days at Resolved', 'Rank', 'WorkAround', 'Time to finish access request',\n",
    "                   'time to work access request from di2e agent perspective', 'Test Resolved to Closed', 'Project Key',\n",
    "                   'Unable to Connect', 'Acceptance Criteria', 'Baseline end date', 'End Date', 'Username',\n",
    "                   'Time to resolution for Problem', 'Time to first response for Problem', 'Email Users Hack', 'OriginatorID',\n",
    "                   'Notes', 'UC2 Verification Method', 'Assigned Date', 'Project Priority', 'Start Date', 'Baseline start date',\n",
    "                   'RFC Number', 'Risk Index', 'Req Version Object', 'Objects Required', 'Origination Date',\n",
    "                   'Auto Close after 5 mins at Resolved', 'Issue Story Points', 'Level of Effort', 'Reviewer(s)', 'Priority Level',\n",
    "                   'Contributor', 'Origination', 'Tag', 'Service Desk Agents']\n",
    "#jira_df.Key.to_dict()\n",
    "idx_list = list(range(3, 29))\n",
    "mask_series = jira_df.index.isin(idx_list)\n",
    "jira_df = jira_df[mask_series]\n",
    "columns_list = [cn for cn in jira_df.columns if 'date' in cn.lower()]\n",
    "jira_df[columns_list].sample(7).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "\n",
    "columns_list = jira_df.columns.tolist()\n",
    "[item for item, count in collections.Counter(columns_list).items() if count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "      <th>count_blanks</th>\n",
       "      <th>count_uniques</th>\n",
       "      <th>count_zeroes</th>\n",
       "      <th>has_dates</th>\n",
       "      <th>min_value</th>\n",
       "      <th>max_value</th>\n",
       "      <th>only_integers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Created</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-04-19 11:37:00</td>\n",
       "      <td>2020-01-13 09:25:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Updated</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-05-29 12:10:00</td>\n",
       "      <td>2020-04-14 08:09:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Resolved</td>\n",
       "      <td>object</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fix Version/s</td>\n",
       "      <td>object</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      column_name   dtype  count_blanks  count_uniques  count_zeroes  \\\n",
       "10        Created  object             0             26             0   \n",
       "12        Updated  object             0             19             0   \n",
       "13       Resolved  object             2             25             0   \n",
       "15  Fix Version/s  object            16              3             0   \n",
       "\n",
       "    has_dates            min_value            max_value only_integers  \n",
       "10       True  2019-04-19 11:37:00  2020-01-13 09:25:00           NaN  \n",
       "12       True  2019-05-29 12:10:00  2020-04-14 08:09:00           NaN  \n",
       "13       True                  NaN                  NaN           NaN  \n",
       "15       True                  NaN                  NaN           NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "column_descriptions_df = get_column_descriptions(jira_df)\n",
    "mask_series = column_descriptions_df.has_dates & column_descriptions_df.only_integers.isnull()\n",
    "column_descriptions_df[mask_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#print(['date_obj.{}'.format(fn) for fn in dir(date_obj) if not fn.startswith('_')])\n",
    "expenses_dir = r'D:\\Documents\\Administrivia\\Expense Reports'\n",
    "file_path = os.path.join(expenses_dir, 'ExpenseReports.xlsx')\n",
    "expenses_df_dict = pd.read_excel(os.path.abspath(file_path), sheet_name=None)\n",
    "len(expenses_df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(expenses_df_dict.keys())\n",
    "expenses_df = expenses_df_dict['Sheet2'].copy()\n",
    "print(expenses_df.columns.tolist())\n",
    "expenses_df.columns = ['Date_TRF_Received', 'Program_Name', 'Company_Name', 'Travelers_Names', 'To_Location',\n",
    "                       'Travel_Start_Date', 'Travel_End_Date', 'Expense_Report_Name']\n",
    "def f(x):\n",
    "    \n",
    "    return 'babbitt' in str(x).lower()\n",
    "\n",
    "mask_series = expenses_df.Travelers_Names.map(f)\n",
    "expenses_df[mask_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "jira_mask_series = False\n",
    "for row_index, row_series in expenses_df[mask_series].iterrows():\n",
    "    start_date = row_series.Travel_Start_Date\n",
    "    stop_date = row_series.Travel_End_Date + timedelta(days=7)\n",
    "    jira_mask_series = jira_mask_series | (jira_df.Updated >= start_date) & (jira_df.Updated <= stop_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Updated': {8: datetime.datetime(2019, 9, 10, 13, 58), 14: datetime.datetime(2019, 9, 9, 15, 4)}, 'Summary': {8: 'As a CIE user, I want to automatically install Consul UNCLASS', 14: 'As a Watchman developer, I need a deployment pipeline that verifies the cluster is ready, so that I can deploy my application (UNCLASS)'}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = ['Updated', 'Summary']\n",
    "jira_df[jira_mask_series][columns_list].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Repositories\\notebooks\\age-of-empires-ii\\ipynb\\AoE2 Civilization Bonus Feature Selection.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt_regex = re.compile(r'([\\r\\n]+)    \"import matplotlib\\.pyplot as plt\\\\n\",')\n",
    "notebooks_dir = r'D:\\Documents\\Repositories\\notebooks'\n",
    "stopped = False\n",
    "for sub_directory, directories_list, files_list in os.walk(notebooks_dir):\n",
    "    if stopped:\n",
    "        break\n",
    "    for file_name in files_list:\n",
    "        if stopped:\n",
    "            break\n",
    "        if file_name.endswith('.ipynb'):\n",
    "            file_path = os.path.join(sub_directory, file_name)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    file_str = f.read()\n",
    "                    if plt_regex.search(file_str):\n",
    "                        print(file_path)\n",
    "                        stopped = True\n",
    "                        break\n",
    "            except UnicodeDecodeError as e:\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        file_str = f.read().decode('utf-8')\n",
    "                        if plt_regex.search(file_str):\n",
    "                            print(file_path)\n",
    "                except Exception as e:\n",
    "                    message = str(e).strip()\n",
    "                    print()\n",
    "                    print('{} had an error after trying to decode: {}'.format(file_path, message))\n",
    "                    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyperclip\n",
    "\n",
    "pyperclip.copy(str(file_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(os.path.dirname(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gi = get_ipython()\n",
    "#print([fn for fn in dir(gi) if not fn.startswith('_')])\n",
    "gi.run_line_magic('who', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "black_list = ['.ipynb_checkpoints']\n",
    "print_all_files_ending_starting_with(root_dir=r'D:\\Documents\\Repositories', ends_with=notebook_path.split('.')[1],\n",
    "                                     starts_with=notebook_path.split('.')[0].split(os.sep)[-1],\n",
    "                                     black_list=black_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_list = []\n",
    "magic_dir = r'D:\\Documents\\Repositories\\notebooks\\load_magic'\n",
    "for file_name in os.listdir(magic_dir):\n",
    "    if file_name.endswith('.py'):\n",
    "        f_list.append(file_name)\n",
    "root_dir_list = ['D:\\Documents\\Repositories']\n",
    "dir_list = ['py', 'load_magic']\n",
    "magic_dict = {file_name: [] for file_name in f_list}\n",
    "for root_dir in root_dir_list:\n",
    "    for sub_directory, directories_list, files_list in os.walk(root_dir):\n",
    "        if sub_directory.split(os.sep)[-1] in dir_list:\n",
    "            for file_name in files_list:\n",
    "                if file_name in f_list:\n",
    "                    file_path = os.path.join(sub_directory, file_name)\n",
    "                    magic_dict_list = magic_dict[file_name]\n",
    "                    magic_dict_list.append(file_path)\n",
    "                    magic_dict[file_name] = magic_dict_list\n",
    "comparator_path = 'wincmp3.exe'\n",
    "print(magic_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "magic_dict_list = magic_dict['environment.py']\n",
    "print(len(magic_dict_list))\n",
    "subprocess.run([comparator_path, os.path.abspath(magic_dict_list[0]), os.path.abspath(magic_dict_list[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remove_empty_folders(folder_path=r'D:\\VirtualBox VMs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_all_files_ending_with(root_dir=r'D:\\\\', ends_with='.box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('The support modules are:')\n",
    "mov_dir = r'D:\\Vagrant_Projects\\local-vagrant\\afdcgscicd-1323-setup\\osif\\support\\modules'\n",
    "for file_name in os.listdir(mov_dir):\n",
    "    if file_name.endswith('.tar.gz'):\n",
    "        print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('The puppet modules are:')\n",
    "mov_dir = r'D:\\Vagrant_Projects\\local-vagrant\\afdcgscicd-1323-setup\\osif\\puppet\\modules'\n",
    "for sub_directory, directories_list, files_list in os.walk('../osif/puppet/modules'):\n",
    "    for file_name in files_list:\n",
    "        if file_name.endswith('.tar.gz'):\n",
    "            print('{}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "for key, value in sys.modules.items():\n",
    "    if 'xdist' in key.lower():\n",
    "        #print('{}: {}'.format(key, value))\n",
    "        print('{}'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_name = 'test'\n",
    "'{}.xlsx'.format('_'.join([test_name, 'keywords']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for root_dir in ['C:\\\\', 'D:\\\\']:\n",
    "    for sub_directory, directories_list, files_list in os.walk(root_dir):\n",
    "        if 'pysvg' in sub_directory.split(os.sep)[-1].lower():\n",
    "            print(sub_directory)\n",
    "            !start %windir%\\explorer.exe \"{sub_directory}\"\n",
    "        else:\n",
    "            for file_name in files_list:\n",
    "                if 'pysvg' in file_name.lower():\n",
    "                    file_path = os.path.join(sub_directory, file_name)\n",
    "                    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "black_list = ['.ipynb_checkpoints']\n",
    "print_all_files_ending_starting_with(root_dir=r'C:\\Users\\dev\\Documents\\repositories', ends_with='.exe',\n",
    "                                     starts_with='chromedriver',\n",
    "                                     black_list=black_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_all_files_starting_with(root_dir=['C:\\\\', 'D:\\\\'], starts_with='jupyter_notebook_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_all_files_ending_with(root_dir=r'C:\\Users\\dev\\Documents\\repositories', ends_with='Navigation.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "magic_dict_list = magic_dict['test.py']\n",
    "print(len(magic_dict_list))\n",
    "subprocess.run([comparator_path, os.path.abspath(magic_dict_list[0]), os.path.abspath(magic_dict_list[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remove_empty_folders(folder_path=r'C:\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_specific_gitignore_files('notebooks', repository_dir=r'C:\\Users\\dev\\Documents\\repositories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "for key, value in sys.modules.items():\n",
    "    if 'xdist' in key.lower():\n",
    "        #print('{}: {}'.format(key, value))\n",
    "        print('{}'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for root_dir in ['C:\\\\', 'D:\\\\']:\n",
    "    for sub_directory, directories_list, files_list in os.walk(root_dir):\n",
    "        for file_name in files_list:\n",
    "            if file_name == 'git.exe':\n",
    "                file_path = os.path.join(sub_directory, file_name)\n",
    "                print('Write-Host \"#################################################################################\"')\n",
    "                print('Write-Host \"    Configuring {}\"'.format(file_path))\n",
    "                print('Write-Host \"#################################################################################\"')\n",
    "                print('cd \"{}\"'.format(sub_directory))\n",
    "                print('git.exe config --global core.autocrlf input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n'.join(sorted('''**/.ipynb_checkpoints/\n",
    "*.ipynb_checkpoints\n",
    "*/.ipynb_checkpoints\n",
    ".ipynb_checkpoints\n",
    ".ipynb_checkpoints*\n",
    ".ipynb_checkpoints/\n",
    ".ipynb_checkpoints/*\n",
    "/.ipynb_checkpoints'''.split('\\n'), key=lambda x: x[::-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
