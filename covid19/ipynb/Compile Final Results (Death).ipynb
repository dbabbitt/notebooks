{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats as sps\n",
    "from IPython.display import clear_output\n",
    "%run ../../load_magic/storage.py\n",
    "s = Storage()\n",
    "states_stats_df = s.load_object('states_stats_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_posteriors(sr, sigma=0.15):\n",
    "\n",
    "    # (1) Calculate Lambda\n",
    "    lam = sr[:-1].values * np.exp(GAMMA * (r_t_range[:, None] - 1))\n",
    "\n",
    "    \n",
    "    # (2) Calculate each day's likelihood\n",
    "    likelihoods = pd.DataFrame(\n",
    "        data = sps.poisson.pmf(sr[1:].values, lam),\n",
    "        index = r_t_range,\n",
    "        columns = sr.index[1:])\n",
    "    \n",
    "    # (3) Create the Gaussian Matrix\n",
    "    process_matrix = sps.norm(loc=r_t_range,\n",
    "                              scale=sigma\n",
    "                             ).pdf(r_t_range[:, None]) \n",
    "\n",
    "    # (3a) Normalize all rows to sum to 1\n",
    "    process_matrix /= process_matrix.sum(axis=0)\n",
    "    \n",
    "    # (4) Calculate the initial prior\n",
    "    prior0 = sps.gamma(a=4).pdf(r_t_range)\n",
    "    prior0 /= prior0.sum()\n",
    "\n",
    "    # Create a DataFrame that will hold our posteriors for each day\n",
    "    # Insert our prior as the first posterior.\n",
    "    posteriors_df = pd.DataFrame(\n",
    "        index=r_t_range,\n",
    "        columns=sr.index,\n",
    "        data={sr.index[0]: prior0}\n",
    "    )\n",
    "    \n",
    "    # We said we'd keep track of the sum of the log of the probability\n",
    "    # of the data for maximum likelihood calculation.\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    # (5) Iteratively apply Bayes' rule\n",
    "    for previous_day, current_day in zip(sr.index[:-1], sr.index[1:]):\n",
    "\n",
    "        #(5a) Calculate the new prior\n",
    "        current_prior = process_matrix @ posteriors_df[previous_day]\n",
    "        \n",
    "        #(5b) Calculate the numerator of Bayes' Rule: P(k|R_t)P(R_t)\n",
    "        numerator = likelihoods[current_day] * current_prior\n",
    "        \n",
    "        #(5c) Calcluate the denominator of Bayes' Rule P(k)\n",
    "        denominator = np.sum(numerator)\n",
    "        \n",
    "        # Execute full Bayes' Rule\n",
    "        posteriors_df[current_day] = numerator/denominator\n",
    "        \n",
    "        # Add to the running sum of log likelihoods\n",
    "        log_likelihood += np.log(denominator)\n",
    "    \n",
    "    return posteriors_df, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_cases(cases, cutoff=25):\n",
    "    new_cases = cases.diff()\n",
    "\n",
    "    smoothed = new_cases.rolling(7,\n",
    "        win_type='gaussian',\n",
    "        min_periods=1,\n",
    "        center=True).mean(std=2).round()\n",
    "    \n",
    "    idx_start = np.searchsorted(smoothed, cutoff)\n",
    "    \n",
    "    smoothed = smoothed.iloc[idx_start:]\n",
    "    original = new_cases.loc[smoothed.index]\n",
    "    \n",
    "    return original, smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def highest_density_interval(pmf, p=.9):\n",
    "    \n",
    "    # If we pass a DataFrame, just call this recursively on the columns\n",
    "    if(isinstance(pmf, pd.DataFrame)):\n",
    "        \n",
    "        return pd.DataFrame([highest_density_interval(pmf[col], p=p) for col in pmf],\n",
    "                            index=pmf.columns)\n",
    "    \n",
    "    cumsum = np.cumsum(pmf.values)\n",
    "    \n",
    "    # N x N matrix of total probability mass for each low, high\n",
    "    total_p = cumsum - cumsum[:, None]\n",
    "    \n",
    "    # Return all indices with total_p > p\n",
    "    lows, highs = (total_p > p).nonzero()\n",
    "    if (lows.size == 0) or (highs.size == 0):\n",
    "    \n",
    "        return pd.Series([np.nan, np.nan], index=[f'Low_{p*100:.0f}', f'High_{p*100:.0f}'], dtype='float64')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Find the smallest range (highest density)\n",
    "        best = (highs - lows).argmin()\n",
    "\n",
    "        low = pmf.index[lows[best]]\n",
    "        high = pmf.index[highs[best]]\n",
    "\n",
    "        return pd.Series([low, high],\n",
    "                         index=[f'Low_{p*100:.0f}',\n",
    "                                f'High_{p*100:.0f}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Final Results\n",
    "\n",
    "Given that we've selected the optimal $\\sigma$, let's grab the precalculated posterior corresponding to that value of $\\sigma$ for each state. Let's also calculate the 90% and 50% highest density intervals (this takes a little while) and also the most likely value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We create an array for every possible value of Rt\n",
    "R_T_MAX = 12\n",
    "r_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)\n",
    "\n",
    "# Gamma is 1/serial interval\n",
    "# https://wwwnc.cdc.gov/eid/article/26/7/20-0282_article\n",
    "# https://www.nejm.org/doi/full/10.1056/NEJMoa2001316\n",
    "GAMMA = 1/7\n",
    "\n",
    "FILTERED_REGION_CODES = ['AS', 'GU', 'PR', 'VI', 'MP']\n",
    "url = 'https://covidtracking.com/api/v1/states/daily.csv'\n",
    "states = pd.read_csv(url,\n",
    "                     usecols=['date', 'state', 'death'],\n",
    "                     parse_dates=['date'],\n",
    "                     index_col=['state', 'date'],\n",
    "                     squeeze=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sigmas = np.linspace(1/20, 1, 20)\n",
    "targets = ~states.index.get_level_values('state').isin(FILTERED_REGION_CODES)\n",
    "states_to_process = states.loc[targets]\n",
    "results = {}\n",
    "n_min = 25\n",
    "for state_name, cases in states_to_process.groupby(level='state'):\n",
    "    print(state_name)\n",
    "    n = 7\n",
    "    new, smoothed = prepare_cases(cases, cutoff=n)\n",
    "    while len(smoothed) == 0:\n",
    "        n -= 1\n",
    "        new, smoothed = prepare_cases(cases, cutoff=n)\n",
    "    result_dict = {}\n",
    "    n_min = min(n_min, n)\n",
    "    \n",
    "    # Holds all posteriors with every given value of sigma\n",
    "    result_dict['posteriors'] = []\n",
    "    \n",
    "    # Holds the log likelihood across all k for each value of sigma\n",
    "    result_dict['log_likelihoods'] = []\n",
    "    \n",
    "    for sigma in sigmas:\n",
    "        posteriors, log_likelihood = get_posteriors(smoothed, sigma=sigma)\n",
    "        result_dict['posteriors'].append(posteriors)\n",
    "        result_dict['log_likelihoods'].append(log_likelihood)\n",
    "    \n",
    "    # Store all results keyed off of state name\n",
    "    results[state_name] = result_dict\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f'{n_min} Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Each index of this array holds the total of the log likelihoods for\n",
    "# the corresponding index of the sigmas array.\n",
    "total_log_likelihoods = np.zeros_like(sigmas)\n",
    "\n",
    "# Loop through each state's results and add the log likelihoods to the running total.\n",
    "for state_name, result_series in results.items():\n",
    "    total_log_likelihoods += result_series['log_likelihoods']\n",
    "\n",
    "# Select the index with the largest log likelihood total\n",
    "max_likelihood_index = total_log_likelihoods.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_results = None\n",
    "\n",
    "for state_name, result in results.items():\n",
    "    print(state_name)\n",
    "    posteriors = result['posteriors'][max_likelihood_index]\n",
    "    hdis_90 = highest_density_interval(posteriors, p=.9)\n",
    "    hdis_50 = highest_density_interval(posteriors, p=.5)\n",
    "    most_likely = posteriors.idxmax().rename('ML')\n",
    "    result_df = pd.concat([most_likely, hdis_90, hdis_50], axis=1)\n",
    "    mask_series = result_df.Low_90.isnull() | result_df.High_90.isnull()\n",
    "    result_df = result_df[~mask_series]\n",
    "    if final_results is None:\n",
    "        final_results = result_df\n",
    "    else:\n",
    "        final_results = pd.concat([final_results, result_df])\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\final_results.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s.store_objects(final_results=final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
