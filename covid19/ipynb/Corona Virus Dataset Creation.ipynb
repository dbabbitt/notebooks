{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The R0 is between 2 and 6, meaning each infected person infects 2–6 others, which can cause the number of infected to double every few days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://informationisbeautiful.net/visualizations/covid-19-coronavirus-infographic-datapack/\n",
    "# https://docs.google.com/spreadsheets/d/1g_YxmDfQx7aOU2DKzNZo9b-NTk62Bju6X3z6OuCa6gw/edit#gid=515684451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "command_str = '{sys.executable} -m pip install --upgrade statsmodels'.format(sys=sys)\n",
    "print(command_str)\n",
    "!{command_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "D:\\Documents\\Repositories\\notebooks\\covid19\\ipynb\\Corona Virus Dataset Creation.ipynb\n",
      "['s.attempt_to_pickle', 's.data_csv_folder', 's.data_folder', 's.encoding_type', 's.load_csv', 's.load_dataframes', 's.load_object', 's.save_dataframes', 's.saves_csv_folder', 's.saves_folder', 's.saves_pickle_folder', 's.store_objects']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Config', 'In', 'Out', 'RandomForestClassifier', 'SequenceMatcher', 'Storage', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__nonzero__', '__package__', '__spec__', '__warningregistry__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'bs', 'check_4_doubles', 'check_for_typos', 'conjunctify_list', 'copyfile', 'csv', 'encoding', 'example_iterrows', 'exit', 'filepath_regex', 'get_classifier', 'get_column_descriptions', 'get_data_structs_dataframe', 'get_datastructure_prediction', 'get_dir_tree', 'get_git_lfs_track_commands', 'get_importances', 'get_input_sample', 'get_ipython', 'get_max_rsquared_adj', 'get_module_version', 'get_notebook_path', 'get_page_tables', 'get_specific_gitignore_files', 'get_struct_name', 'humanize_bytes', 'io', 'ipykernel', 'json', 'jupyter_config_dir', 'math', 'notebook_path', 'notebookapp', 'nx', 'os', 'pd', 'pickle', 'plt', 'preprocess_data', 'print_all_files_ending_starting_with', 'print_all_files_ending_with', 'print_all_files_starting_with', 'quit', 're', 'remove_empty_folders', 's', 'scraping_utils', 'similar', 'sm', 'sns', 'stats', 'subprocess', 'sys', 'time', 'url_regex', 'urllib', 'wikipedia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "%run ../../load_magic/storage.py\n",
    "%run ../../load_magic/paths.py\n",
    "%run ../../load_magic/lists.py\n",
    "%run ../../load_magic/environment.py\n",
    "%run ../../load_magic/dataframes.py\n",
    "\n",
    "import scraping_utils\n",
    "wikipedia = scraping_utils.wikipedia\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import networkx as nx\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "notebook_path = get_notebook_path()\n",
    "print(notebook_path)\n",
    "s = Storage()\n",
    "#pandemic_df = s.load_object('pandemic_df')\n",
    "print(['s.{}'.format(fn) for fn in dir(s) if not fn.startswith('_')])\n",
    "\n",
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!start %windir%\\explorer.exe \"{os.path.abspath(os.path.dirname(notebook_path))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Scrape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, (249, 6)), (2, (36, 3)), (0, (6, 7)), (3, (7, 2)), (4, (2, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/ISO_3166-1'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\country_abbreviations_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "country_abbreviations_df = tables_list[1].copy()\n",
    "country_abbreviations_df.set_index('English short name (using title case)', drop=True, inplace=True)\n",
    "country_abbreviations_dict = country_abbreviations_df['Alpha-2 code'].to_dict()\n",
    "s.store_objects(country_abbreviations_dict=country_abbreviations_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = os.path.join(s.data_csv_folder, 'Confirmed_Cumulative.csv')\n",
    "world_cumulative_df = s.load_csv('Confirmed_Cumulative')\n",
    "date_format = '%b %d, %Y'\n",
    "for column_name in world_cumulative_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        world_cumulative_df[column_name] = world_cumulative_df[column_name].map(lambda x: datetime.strptime('{}'.format(x), date_format))\n",
    "        world_cumulative_df[column_name] = pd.to_datetime(world_cumulative_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        world_cumulative_df[column_name] = world_cumulative_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        world_cumulative_df[column_name] = pd.to_numeric(world_cumulative_df[column_name], errors='coerce')\n",
    "s.store_objects(world_cumulative_df=world_cumulative_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, (2622, 14)), (1, (58, 15)), (4, (56, 14)), (5, (58, 6)), (0, (12, 3)), (2, (2, 12))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "url = 'https://docs.google.com/spreadsheets/u/2/d/e/2PACX-1vRwAqp96T9sYYq2-i7Tj0pvTf6XVHjDSMIKBdZHXiCGGdNC0ypEU9NbngS8mxea55JuCFuua1MUeOj5/pubhtml'\n",
    "tables_list = get_page_tables(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Pending</td>\n",
       "      <td>HospitalizedÂ â Currently</td>\n",
       "      <td>Hospitalized â Cumulative</td>\n",
       "      <td>In ICU âÂ Currently</td>\n",
       "      <td>In ICU âÂ Cumulative</td>\n",
       "      <td>On Ventilator âÂ Currently</td>\n",
       "      <td>On Ventilator âÂ Cumulative</td>\n",
       "      <td>Recovered</td>\n",
       "      <td>Deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>799717</td>\n",
       "      <td>3355461</td>\n",
       "      <td>3956</td>\n",
       "      <td>58468</td>\n",
       "      <td>84292</td>\n",
       "      <td>15228</td>\n",
       "      <td>2315</td>\n",
       "      <td>5514</td>\n",
       "      <td>214</td>\n",
       "      <td>58117</td>\n",
       "      <td>39995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Unnamed: 1 Unnamed: 2 Unnamed: 3                   Unnamed: 4  \\\n",
       "0           1   Positive   Negative    Pending  HospitalizedÂ â Currently   \n",
       "1           2     799717    3355461       3956                        58468   \n",
       "\n",
       "                    Unnamed: 5             Unnamed: 6              Unnamed: 7  \\\n",
       "0  Hospitalized â Cumulative  In ICU âÂ Currently  In ICU âÂ Cumulative   \n",
       "1                        84292                  15228                    2315   \n",
       "\n",
       "                     Unnamed: 8                     Unnamed: 9 Unnamed: 10  \\\n",
       "0  On Ventilator âÂ Currently  On Ventilator âÂ Cumulative   Recovered   \n",
       "1                          5514                            214       58117   \n",
       "\n",
       "  Unnamed: 11  \n",
       "0      Deaths  \n",
       "1       39995  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tables_list[2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\pandemic_countries_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County_Name</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "      <th>Recovered_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>735074.0</td>\n",
       "      <td>38568.0</td>\n",
       "      <td>63655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Italy</td>\n",
       "      <td>175927.0</td>\n",
       "      <td>23227.0</td>\n",
       "      <td>44927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>192367.0</td>\n",
       "      <td>20181.0</td>\n",
       "      <td>74662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>111821.0</td>\n",
       "      <td>19323.0</td>\n",
       "      <td>35983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>114217.0</td>\n",
       "      <td>15464.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>37183.0</td>\n",
       "      <td>5453.0</td>\n",
       "      <td>8348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Iran</td>\n",
       "      <td>80868.0</td>\n",
       "      <td>5031.0</td>\n",
       "      <td>55987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>China</td>\n",
       "      <td>82719.0</td>\n",
       "      <td>4632.0</td>\n",
       "      <td>77029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>143216.0</td>\n",
       "      <td>4453.0</td>\n",
       "      <td>77147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>31589.0</td>\n",
       "      <td>3601.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       County_Name  Confirmed_Cumulative  Deaths_Cumulative  \\\n",
       "0    United States              735074.0            38568.0   \n",
       "2            Italy              175927.0            23227.0   \n",
       "1            Spain              192367.0            20181.0   \n",
       "5           France              111821.0            19323.0   \n",
       "4   United Kingdom              114217.0            15464.0   \n",
       "9          Belgium               37183.0             5453.0   \n",
       "8             Iran               80868.0             5031.0   \n",
       "6            China               82719.0             4632.0   \n",
       "3          Germany              143216.0             4453.0   \n",
       "13     Netherlands               31589.0             3601.0   \n",
       "\n",
       "    Recovered_Cumulative  \n",
       "0                63655.0  \n",
       "2                44927.0  \n",
       "1                74662.0  \n",
       "5                35983.0  \n",
       "4                    NaN  \n",
       "9                 8348.0  \n",
       "8                55987.0  \n",
       "6                77029.0  \n",
       "3                77147.0  \n",
       "13                   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "covidtracking_df.columns = ['County_Name1', 'County_Name', 'Confirmed_Cumulative', 'Deaths_Cumulative', 'Recovered_Cumulative', 'References']\n",
    "covidtracking_df = covidtracking_df.iloc[:-2]\n",
    "columns_list = ['County_Name', 'Confirmed_Cumulative', 'Deaths_Cumulative', 'Recovered_Cumulative']\n",
    "covidtracking_df = covidtracking_df[columns_list]\n",
    "for column_name in covidtracking_df.columns:\n",
    "    if column_name == 'County_Name':\n",
    "        covidtracking_df[column_name] = covidtracking_df[column_name].map(lambda x: x.split('[')[0])\n",
    "    else:\n",
    "        covidtracking_df[column_name] = covidtracking_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('[')[0]))\n",
    "        covidtracking_df[column_name] = pd.to_numeric(covidtracking_df[column_name], errors='coerce')\n",
    "s.store_objects(covidtracking_df=covidtracking_df)\n",
    "covidtracking_df.sort_values('Deaths_Cumulative', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (235, 6)), (6, (22, 6)), (9, (65, 2)), (1, (61, 2)), (7, (11, 7)), (4, (11, 6)), (2, (15, 4)), (11, (20, 2)), (12, (19, 2)), (23, (8, 2)), (24, (7, 2)), (26, (7, 2)), (3, (2, 6)), (13, (6, 2)), (17, (6, 2)), (27, (6, 2)), (18, (5, 2)), (16, (4, 2)), (19, (4, 2)), (20, (3, 2)), (21, (3, 2)), (15, (2, 2)), (22, (2, 2)), (25, (2, 2)), (5, (1, 2)), (8, (1, 2)), (10, (1, 2)), (14, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic_by_country_and_territory'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\pandemic_countries_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County_Name</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "      <th>Recovered_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>735074.0</td>\n",
       "      <td>38568.0</td>\n",
       "      <td>63655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Italy</td>\n",
       "      <td>175927.0</td>\n",
       "      <td>23227.0</td>\n",
       "      <td>44927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>192367.0</td>\n",
       "      <td>20181.0</td>\n",
       "      <td>74662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>111821.0</td>\n",
       "      <td>19323.0</td>\n",
       "      <td>35983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>114217.0</td>\n",
       "      <td>15464.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>37183.0</td>\n",
       "      <td>5453.0</td>\n",
       "      <td>8348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Iran</td>\n",
       "      <td>80868.0</td>\n",
       "      <td>5031.0</td>\n",
       "      <td>55987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>China</td>\n",
       "      <td>82719.0</td>\n",
       "      <td>4632.0</td>\n",
       "      <td>77029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>143216.0</td>\n",
       "      <td>4453.0</td>\n",
       "      <td>77147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>31589.0</td>\n",
       "      <td>3601.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       County_Name  Confirmed_Cumulative  Deaths_Cumulative  \\\n",
       "0    United States              735074.0            38568.0   \n",
       "2            Italy              175927.0            23227.0   \n",
       "1            Spain              192367.0            20181.0   \n",
       "5           France              111821.0            19323.0   \n",
       "4   United Kingdom              114217.0            15464.0   \n",
       "9          Belgium               37183.0             5453.0   \n",
       "8             Iran               80868.0             5031.0   \n",
       "6            China               82719.0             4632.0   \n",
       "3          Germany              143216.0             4453.0   \n",
       "13     Netherlands               31589.0             3601.0   \n",
       "\n",
       "    Recovered_Cumulative  \n",
       "0                63655.0  \n",
       "2                44927.0  \n",
       "1                74662.0  \n",
       "5                35983.0  \n",
       "4                    NaN  \n",
       "9                 8348.0  \n",
       "8                55987.0  \n",
       "6                77029.0  \n",
       "3                77147.0  \n",
       "13                   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pandemic_countries_df = tables_list[0].copy()\n",
    "pandemic_countries_df.columns = ['County_Name1', 'County_Name', 'Confirmed_Cumulative', 'Deaths_Cumulative', 'Recovered_Cumulative', 'References']\n",
    "pandemic_countries_df = pandemic_countries_df.iloc[:-2]\n",
    "columns_list = ['County_Name', 'Confirmed_Cumulative', 'Deaths_Cumulative', 'Recovered_Cumulative']\n",
    "pandemic_countries_df = pandemic_countries_df[columns_list]\n",
    "for column_name in pandemic_countries_df.columns:\n",
    "    if column_name == 'County_Name':\n",
    "        pandemic_countries_df[column_name] = pandemic_countries_df[column_name].map(lambda x: x.split('[')[0])\n",
    "    else:\n",
    "        pandemic_countries_df[column_name] = pandemic_countries_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('[')[0]))\n",
    "        pandemic_countries_df[column_name] = pd.to_numeric(pandemic_countries_df[column_name], errors='coerce')\n",
    "s.store_objects(pandemic_countries_df=pandemic_countries_df)\n",
    "pandemic_countries_df.sort_values('Deaths_Cumulative', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for country_name in pandemic_countries_df.sort_values('Deaths_Cumulative', ascending=False).County_Name.tolist()[:5]:\n",
    "    country_name = re.sub(' ', '_', str(country_name))\n",
    "    rm_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_{}'.format(country_name)\n",
    "    !\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" {rm_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (242, 6)), (1, (12, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\country_populations_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "country_populations_df = tables_list[0].copy()\n",
    "country_populations_df.columns = ['Rank', 'Country', 'Population', 'world_population_percent', 'Date', 'Source']\n",
    "country_populations_df = country_populations_df.loc[:240]\n",
    "for column_name in country_populations_df.columns:\n",
    "    if column_name == 'Country':\n",
    "        country_populations_df[column_name] = country_populations_df[column_name].map(lambda x: str(x).split('[')[0])\n",
    "    elif column_name in ['Population', 'Rank']:\n",
    "        country_populations_df[column_name] = country_populations_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        country_populations_df[column_name] = pd.to_numeric(country_populations_df[column_name], errors='coerce')\n",
    "country_populations_df.set_index('Country', drop=True, inplace=True)\n",
    "columns_list = ['Rank', 'Population', 'Date', 'Source']\n",
    "country_populations_df = country_populations_df[columns_list]\n",
    "s.store_objects(country_populations_df=country_populations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Iran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, (35, 37)), (3, (61, 6)), (1, (66, 5)), (7, (66, 2)), (27, (45, 2)), (9, (21, 2)), (10, (20, 2)), (28, (14, 2)), (29, (13, 2)), (0, (12, 2)), (38, (10, 2)), (39, (9, 2)), (22, (8, 2)), (11, (7, 2)), (23, (7, 2)), (25, (7, 2)), (16, (6, 2)), (26, (6, 2)), (17, (5, 2)), (36, (5, 2)), (15, (4, 2)), (18, (4, 2)), (34, (4, 2)), (37, (4, 2)), (40, (4, 2)), (19, (3, 2)), (20, (3, 2)), (30, (3, 2)), (31, (3, 2)), (35, (3, 2)), (14, (2, 2)), (21, (2, 2)), (24, (2, 2)), (32, (2, 2)), (41, (2, 2)), (2, (1, 2)), (5, (1, 2)), (6, (1, 2)), (8, (1, 2)), (12, (1, 2)), (13, (1, 2)), (33, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "page_tables_list = get_page_tables('https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Iran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\iran_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_Total</th>\n",
       "      <th>Deaths_Total</th>\n",
       "      <th>Confirmed_New</th>\n",
       "      <th>Deaths_New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>17361</td>\n",
       "      <td>1135</td>\n",
       "      <td>1192</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>18407</td>\n",
       "      <td>1284</td>\n",
       "      <td>1046</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>19644</td>\n",
       "      <td>1433</td>\n",
       "      <td>1237</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>20610</td>\n",
       "      <td>1556</td>\n",
       "      <td>966</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>21638</td>\n",
       "      <td>1685</td>\n",
       "      <td>1028</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_Total  Deaths_Total  Confirmed_New  Deaths_New\n",
       "28 2020-03-18            17361          1135           1192       147.0\n",
       "29 2020-03-19            18407          1284           1046       149.0\n",
       "30 2020-03-20            19644          1433           1237       149.0\n",
       "31 2020-03-21            20610          1556            966       123.0\n",
       "32 2020-03-22            21638          1685           1028       129.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "iran_df = page_tables_list[4].copy()\n",
    "iran_df.columns = ['Date', 'Region_1_Qom', 'Region_1_Teh', 'Region_1_Maz', 'Region_1_Alb', 'Region_1_Sem', 'Region_1_Gol',\n",
    "                   'Region_1_Qaz', 'Region_2_Esf', 'Region_2_Frs', 'Region_2_Hor', 'Region_2_Koh', 'Region_2_Cha', 'Region_2_Bus',\n",
    "                   'Region_3_Gil', 'Region_3_Ard', 'Region_3_Azs', 'Region_3_Azg', 'Region_3_Kur', 'Region_3_Zan', 'Region_4_Mar',\n",
    "                   'Region_4_Ham', 'Region_4_Khz', 'Region_4_Krs', 'Region_4_Lor', 'Region_4_Ilm', 'Region_5_Khr', 'Region_5_Sis',\n",
    "                   'Region_5_Yaz', 'Region_5_Khs', 'Region_5_Ker', 'Region_5_Khn', 'Confirmed_New', 'Confirmed_Total',\n",
    "                   'Deaths_New', 'Deaths_Total', 'Sources']\n",
    "iran_df = iran_df.loc[:32]\n",
    "#date_format = '%Y/%M/%d'\n",
    "for column_name in iran_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        #iran_df[column_name] = iran_df[column_name].map(lambda x: datetime.strptime('{}'.format(x), date_format))\n",
    "        iran_df[column_name] = pd.to_datetime(iran_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        iran_df[column_name] = iran_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        iran_df[column_name] = pd.to_numeric(iran_df[column_name], errors='coerce')\n",
    "s.store_objects(iran_df=iran_df)\n",
    "columns_list = ['Date', 'Confirmed_Total', 'Deaths_Total', 'Confirmed_New', 'Deaths_New']\n",
    "iran_df[columns_list].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, (84, 37)), (1, (60, 5)), (4, (66, 2)), (6, (21, 2)), (7, (20, 2)), (0, (13, 2)), (19, (8, 2)), (8, (7, 2)), (20, (7, 2)), (22, (7, 2)), (13, (6, 2)), (23, (6, 2)), (14, (5, 2)), (12, (4, 2)), (15, (4, 2)), (16, (3, 2)), (17, (3, 2)), (11, (2, 2)), (18, (2, 2)), (21, (2, 2)), (3, (1, 2)), (5, (1, 2)), (9, (1, 2)), (10, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "page_tables_list = get_page_tables('https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Sweden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\sweden_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_New</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_New</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>688</td>\n",
       "      <td>13746</td>\n",
       "      <td>49</td>\n",
       "      <td>1628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>531</td>\n",
       "      <td>14277</td>\n",
       "      <td>51</td>\n",
       "      <td>1679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2020-04-19</td>\n",
       "      <td>389</td>\n",
       "      <td>14666</td>\n",
       "      <td>43</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>462</td>\n",
       "      <td>15128</td>\n",
       "      <td>21</td>\n",
       "      <td>1743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>194</td>\n",
       "      <td>15322</td>\n",
       "      <td>3</td>\n",
       "      <td>1765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_New  Confirmed_Cumulative  Deaths_New  \\\n",
       "56 2020-04-17            688                 13746          49   \n",
       "57 2020-04-18            531                 14277          51   \n",
       "58 2020-04-19            389                 14666          43   \n",
       "59 2020-04-20            462                 15128          21   \n",
       "60 2020-04-21            194                 15322           3   \n",
       "\n",
       "    Deaths_Cumulative  \n",
       "56               1628  \n",
       "57               1679  \n",
       "58               1722  \n",
       "59               1743  \n",
       "60               1765  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sweden_df = page_tables_list[2].copy()\n",
    "[re.sub(' ', '_', '{}_{}_{}'.format(column_tuple[0].split('[')[0],\n",
    "                                    column_tuple[1], column_tuple[2])) for column_tuple in sweden_df.columns.tolist()]\n",
    "sweden_df.columns = ['Date', 'County_Blekinge', 'County_Dalarna', 'County_Gotland', 'County_Gävleborg', 'County_Halland', 'County_Jämtland',\n",
    "                     'County_Jönköping', 'County_Kalmar', 'County_Kronoberg', 'County_Norrbotten', 'County_Skåne', 'County_Stockholm',\n",
    "                     'County_Södermanland', 'County_Uppsala', 'County_Värmland', 'County_Västerbotten', 'County_Västernorrland',\n",
    "                     'County_Västmanland', 'County_Västra_Götaland', 'County_Örebro', 'County_Östergötland', 'Confirmed_New',\n",
    "                     'Confirmed_Cumulative', 'Confirmed_Diff', 'Confirmed_7d_Ave', 'Deaths_New', 'Deaths_Cumulative', 'Deaths_Diff',\n",
    "                     'Deaths_7d_Ave', 'ICU_New', 'ICU_Cumulative', 'ICU_Diff', 'ICU_7d_Ave', 'Samples_New', 'Samples_Cumulative', 'Samples_Week']\n",
    "#sweden_df.Date.to_dict()\n",
    "idx_list = [2] + list(range(5, 61))\n",
    "mask_series = sweden_df.index.isin(idx_list)\n",
    "sweden_df = sweden_df[mask_series]\n",
    "columns_list = list(set(sweden_df.columns) - set(['Date', 'Confirmed_Diff', 'Deaths_Diff', 'ICU_Diff']))\n",
    "for column_name in sweden_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        sweden_df[column_name] = sweden_df[column_name].map(lambda x: str(x).split('[')[0])\n",
    "        sweden_df[column_name] = pd.to_datetime(sweden_df[column_name])\n",
    "        pass\n",
    "    elif column_name in columns_list:\n",
    "        sweden_df[column_name] = sweden_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split(' ')[0]))\n",
    "        sweden_df[column_name] = pd.to_numeric(sweden_df[column_name], errors='coerce')\n",
    "new_columns_list = [cn for cn in sweden_df.columns if cn.endswith('_New')]\n",
    "for column_name in new_columns_list:\n",
    "    sweden_df[column_name].fillna(value=0, inplace=True)\n",
    "    sweden_df[column_name] = sweden_df[column_name].map(lambda x: int(x))\n",
    "    sweden_df[column_name] = pd.to_numeric(sweden_df[column_name], errors='coerce')\n",
    "cumulative_columns_list = [cn for cn in sweden_df.columns if cn.endswith('_Cumulative')]\n",
    "for column_name in cumulative_columns_list:\n",
    "    sweden_df[column_name].ffill(inplace=True)\n",
    "    sweden_df[column_name].fillna(value=0, inplace=True)\n",
    "    sweden_df[column_name] = sweden_df[column_name].map(lambda x: int(x))\n",
    "    sweden_df[column_name] = pd.to_numeric(sweden_df[column_name], errors='coerce')\n",
    "s.store_objects(sweden_df=sweden_df)\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "sweden_df[columns_list].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (60, 12)), (2, (71, 10)), (4, (11, 2)), (1, (7, 1)), (3, (2, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\state_populations_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Census_Population_2010</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>3751351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>814180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>37254523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>12830632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>18801310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Census_Population_2010\n",
       "State_Name                          \n",
       "Oklahoma                     3751351\n",
       "South Dakota                  814180\n",
       "California                  37254523\n",
       "Illinois                    12830632\n",
       "Florida                     18801310"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_populations_df = tables_list[0].copy()\n",
    "#['{}_{}'.format(column_tuple[0], column_tuple[1]) for column_tuple in state_populations_df.columns.tolist()]\n",
    "state_populations_df.columns = ['Rank_Current', 'Rank_2010', 'State_Name', 'Census_Estimate_2019', 'Census_Population_2010', 'Change_Percent',\n",
    "                                'Change_Absolute', 'House_Seats_Total', 'Population_Per_Electoral_Vote_2019', 'Population_Per_House_Seat_2019',\n",
    "                                'Population_Per_House_Seat_2010', 'Percent_Total_2019']\n",
    "state_populations_df = state_populations_df.iloc[:-4]\n",
    "state_populations_df.set_index('State_Name', drop=True, inplace=True)\n",
    "state_populations_df.Census_Population_2010 = state_populations_df.Census_Population_2010.map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "state_populations_df.Census_Population_2010 = pd.to_numeric(state_populations_df.Census_Population_2010, errors='coerce')\n",
    "state_populations_df.Census_Population_2010.fillna(value=0, inplace=True)\n",
    "state_populations_df.Census_Population_2010 = state_populations_df.Census_Population_2010.map(lambda x: int(x))\n",
    "columns_list = ['Census_Population_2010']\n",
    "s.store_objects(state_populations_df=state_populations_df)\n",
    "state_populations_df[columns_list].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (60, 12)), (1, (9, 12)), (2, (4, 12)), (3, (11, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_area'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\state_populations_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>State_Name</th>\n",
       "      <th>Maine</th>\n",
       "      <th>Guam</th>\n",
       "      <th>Oregon</th>\n",
       "      <th>Virginia</th>\n",
       "      <th>Vermont</th>\n",
       "      <th>Montana</th>\n",
       "      <th>Iowa</th>\n",
       "      <th>Alabama</th>\n",
       "      <th>North Carolina</th>\n",
       "      <th>Wisconsin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Change_Percent</th>\n",
       "      <td>1.2%</td>\n",
       "      <td>4.0%</td>\n",
       "      <td>10.1%</td>\n",
       "      <td>6.7%</td>\n",
       "      <td>-0.3%</td>\n",
       "      <td>8.0%</td>\n",
       "      <td>3.6%</td>\n",
       "      <td>2.6%</td>\n",
       "      <td>10.0%</td>\n",
       "      <td>2.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Census_Estimate_2019</th>\n",
       "      <td>1344212</td>\n",
       "      <td>165718</td>\n",
       "      <td>4217737</td>\n",
       "      <td>8535519</td>\n",
       "      <td>623989</td>\n",
       "      <td>1068778</td>\n",
       "      <td>3155070</td>\n",
       "      <td>4903185</td>\n",
       "      <td>10488084</td>\n",
       "      <td>5822434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population_Per_Electoral_Vote_2019</th>\n",
       "      <td>336053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>602534</td>\n",
       "      <td>656578</td>\n",
       "      <td>207996</td>\n",
       "      <td>356259</td>\n",
       "      <td>525845</td>\n",
       "      <td>544798</td>\n",
       "      <td>699206</td>\n",
       "      <td>582243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank_2010</th>\n",
       "      <td>42</td>\n",
       "      <td>53</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water_sq_mi</th>\n",
       "      <td>4536.82</td>\n",
       "      <td>360.82</td>\n",
       "      <td>2390.53</td>\n",
       "      <td>3284.84</td>\n",
       "      <td>399.71</td>\n",
       "      <td>1493.91</td>\n",
       "      <td>415.68</td>\n",
       "      <td>1774.74</td>\n",
       "      <td>5201.25</td>\n",
       "      <td>11338.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Area_sq_mi</th>\n",
       "      <td>35379.7</td>\n",
       "      <td>570.62</td>\n",
       "      <td>98378.5</td>\n",
       "      <td>42774.9</td>\n",
       "      <td>9616.36</td>\n",
       "      <td>147040</td>\n",
       "      <td>56272.8</td>\n",
       "      <td>52420.1</td>\n",
       "      <td>53819.2</td>\n",
       "      <td>65496.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Land_Area_sq_mi</th>\n",
       "      <td>30842.9</td>\n",
       "      <td>209.8</td>\n",
       "      <td>95988</td>\n",
       "      <td>39490.1</td>\n",
       "      <td>9216.66</td>\n",
       "      <td>145546</td>\n",
       "      <td>55857.1</td>\n",
       "      <td>50645.3</td>\n",
       "      <td>48617.9</td>\n",
       "      <td>54157.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water_Percent</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>House_Seats_Total</th>\n",
       "      <td>2</td>\n",
       "      <td>1 (non-voting)</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Land_Area_Percent</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "State_Name                            Maine            Guam   Oregon Virginia  \\\n",
       "Change_Percent                         1.2%            4.0%    10.1%     6.7%   \n",
       "Census_Estimate_2019                1344212          165718  4217737  8535519   \n",
       "Population_Per_Electoral_Vote_2019   336053             NaN   602534   656578   \n",
       "Rank_2010                                42              53       27       12   \n",
       "Water_sq_mi                         4536.82          360.82  2390.53  3284.84   \n",
       "Total_Area_sq_mi                    35379.7          570.62  98378.5  42774.9   \n",
       "Land_Area_sq_mi                     30842.9           209.8    95988  39490.1   \n",
       "Water_Percent                           NaN             NaN      NaN      NaN   \n",
       "House_Seats_Total                         2  1 (non-voting)        5       11   \n",
       "Land_Area_Percent                       NaN             NaN      NaN      NaN   \n",
       "\n",
       "State_Name                          Vermont  Montana     Iowa  Alabama  \\\n",
       "Change_Percent                        -0.3%     8.0%     3.6%     2.6%   \n",
       "Census_Estimate_2019                 623989  1068778  3155070  4903185   \n",
       "Population_Per_Electoral_Vote_2019   207996   356259   525845   544798   \n",
       "Rank_2010                                50       45       31       23   \n",
       "Water_sq_mi                          399.71  1493.91   415.68  1774.74   \n",
       "Total_Area_sq_mi                    9616.36   147040  56272.8  52420.1   \n",
       "Land_Area_sq_mi                     9216.66   145546  55857.1  50645.3   \n",
       "Water_Percent                           NaN      NaN      NaN      NaN   \n",
       "House_Seats_Total                         1        1        4        7   \n",
       "Land_Area_Percent                       NaN      NaN      NaN      NaN   \n",
       "\n",
       "State_Name                         North Carolina Wisconsin  \n",
       "Change_Percent                              10.0%      2.4%  \n",
       "Census_Estimate_2019                     10488084   5822434  \n",
       "Population_Per_Electoral_Vote_2019         699206    582243  \n",
       "Rank_2010                                      10        20  \n",
       "Water_sq_mi                               5201.25   11338.6  \n",
       "Total_Area_sq_mi                          53819.2   65496.4  \n",
       "Land_Area_sq_mi                           48617.9   54157.8  \n",
       "Water_Percent                                 NaN       NaN  \n",
       "House_Seats_Total                              13         8  \n",
       "Land_Area_Percent                             NaN       NaN  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "us_states_area_df = tables_list[0].copy()\n",
    "us_states_area_df.columns = ['State_Name', 'Total_Area_Rank', 'Total_Area_sq_mi', 'Total_Area_sq_km', 'Land_Area_Rank',\n",
    "                             'Land_Area_sq_mi', 'Land_Area_sq_km', 'Land_Area_Percent', 'Water_Rank', 'Water_sq_mi',\n",
    "                             'Water_sq_km', 'Water_Percent']\n",
    "us_states_area_df = us_states_area_df.iloc[:-3]\n",
    "for column_name in us_states_area_df.columns:\n",
    "    if column_name == 'State_Name':\n",
    "        us_states_area_df[column_name] = us_states_area_df[column_name].map(lambda x: str(x).split('[')[0])\n",
    "    else:\n",
    "        us_states_area_df[column_name] = us_states_area_df[column_name].map(lambda x: re.sub('[^\\d\\.]+', '', str(x)))\n",
    "        us_states_area_df[column_name] = pd.to_numeric(us_states_area_df[column_name], errors='coerce')\n",
    "mask_series = (us_states_area_df.State_Name == 'United States Virgin Islands')\n",
    "us_states_area_df.loc[mask_series, 'State_Name'] = 'U.S. Virgin Islands'\n",
    "us_states_area_df.set_index('State_Name', drop=True, inplace=True)\n",
    "state_populations_df = s.load_object('state_populations_df')\n",
    "state_populations_df = state_populations_df.merge(us_states_area_df, left_index=True, right_index=True)\n",
    "s.store_objects(state_populations_df=state_populations_df)\n",
    "state_populations_df.sample(10).T.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, (58, 8)), (2, (73, 5)), (8, (65, 2)), (5, (42, 2)), (10, (20, 2)), (11, (19, 2)), (1, (12, 2)), (27, (10, 2)), (22, (8, 2)), (23, (7, 2)), (25, (7, 2)), (12, (6, 2)), (16, (6, 2)), (26, (6, 2)), (17, (5, 2)), (15, (4, 2)), (18, (4, 2)), (19, (3, 2)), (20, (3, 2)), (28, (3, 2)), (4, (2, 2)), (14, (2, 2)), (21, (2, 2)), (24, (2, 2)), (29, (2, 2)), (0, (1, 2)), (3, (1, 2)), (7, (1, 2)), (9, (1, 2)), (13, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_the_United_States'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\states_current_numbers_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "      <th>Recovered_Cumulative</th>\n",
       "      <th>Hospitalized_Cumulative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>9047</td>\n",
       "      <td>391</td>\n",
       "      <td>0</td>\n",
       "      <td>1755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>341</td>\n",
       "      <td>9</td>\n",
       "      <td>138</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>287</td>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>27575</td>\n",
       "      <td>1134</td>\n",
       "      <td>0</td>\n",
       "      <td>4423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>871</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Confirmed_Cumulative  Deaths_Cumulative  Recovered_Cumulative  \\\n",
       "State_Name                                                                    \n",
       "Colorado                      9047                391                     0   \n",
       "North Dakota                   341                  9                   138   \n",
       "Wyoming                        287                  2                   164   \n",
       "Illinois                     27575               1134                     0   \n",
       "Nebraska                       871                 18                     0   \n",
       "\n",
       "              Hospitalized_Cumulative  \n",
       "State_Name                             \n",
       "Colorado                         1755  \n",
       "North Dakota                       42  \n",
       "Wyoming                             0  \n",
       "Illinois                         4423  \n",
       "Nebraska                            0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "states_current_numbers_df = tables_list[4].copy()\n",
    "states_current_numbers_df.columns = ['d1', 'State_Name', 'Confirmed_Cumulative', 'Deaths_Cumulative', 'Recovered_Cumulative', 'Hospitalized_Cumulative',\n",
    "                              'd2', 'd3']\n",
    "columns_list = ['State_Name', 'Confirmed_Cumulative', 'Deaths_Cumulative', 'Recovered_Cumulative', 'Hospitalized_Cumulative']\n",
    "states_current_numbers_df = states_current_numbers_df[columns_list]\n",
    "states_current_numbers_df = states_current_numbers_df.iloc[:-2]\n",
    "states_current_numbers_df.set_index('State_Name', drop=True, inplace=True)\n",
    "for column_name in states_current_numbers_df.columns:\n",
    "    states_current_numbers_df[column_name] = states_current_numbers_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "    states_current_numbers_df[column_name] = pd.to_numeric(states_current_numbers_df[column_name], errors='coerce')\n",
    "    states_current_numbers_df[column_name].fillna(value=0, inplace=True)\n",
    "    states_current_numbers_df[column_name] = states_current_numbers_df[column_name].map(lambda x: int(x))\n",
    "s.store_objects(states_current_numbers_df=states_current_numbers_df)\n",
    "states_current_numbers_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\states_covid19_lockdowns_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Lockdown_Enacted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>2020-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>2020-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>2020-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>2020-03-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date_Lockdown_Enacted\n",
       "State_Name                      \n",
       "Alabama               2020-04-04\n",
       "Alaska                2020-03-28\n",
       "Arizona               2020-03-31\n",
       "California            2020-03-19\n",
       "Colorado              2020-03-26"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lockdowns_df = tables_list[5].copy()\n",
    "lockdowns_df.columns = ['State_Name', 'Date_Lockdown_Enacted']\n",
    "date_format = '%B %d, %Y'\n",
    "lockdowns_df.Date_Lockdown_Enacted = lockdowns_df.Date_Lockdown_Enacted.map(lambda x: datetime.strptime('{}'.format(x), date_format))\n",
    "lockdowns_df.Date_Lockdown_Enacted = pd.to_datetime(lockdowns_df.Date_Lockdown_Enacted)\n",
    "lockdowns_df.set_index('State_Name', drop=True, inplace=True)\n",
    "s.store_objects(states_covid19_lockdowns_df=lockdowns_df)\n",
    "lockdowns_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (77, 63)), (1, (60, 59)), (2, (66, 2)), (4, (21, 2)), (5, (20, 2)), (17, (8, 2)), (6, (7, 2)), (18, (7, 2)), (20, (7, 2)), (11, (6, 2)), (21, (6, 2)), (12, (5, 2)), (10, (4, 2)), (13, (4, 2)), (14, (3, 2)), (15, (3, 2)), (9, (2, 2)), (16, (2, 2)), (19, (2, 2)), (3, (1, 2)), (7, (1, 2)), (8, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/United_States_medical_cases'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\usa_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_New</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_New</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>30915</td>\n",
       "      <td>696621</td>\n",
       "      <td>2080</td>\n",
       "      <td>32435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>28084</td>\n",
       "      <td>724705</td>\n",
       "      <td>1743</td>\n",
       "      <td>34178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2020-04-19</td>\n",
       "      <td>26013</td>\n",
       "      <td>750718</td>\n",
       "      <td>1634</td>\n",
       "      <td>35812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>25093</td>\n",
       "      <td>775811</td>\n",
       "      <td>1643</td>\n",
       "      <td>37455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>25311</td>\n",
       "      <td>801122</td>\n",
       "      <td>2627</td>\n",
       "      <td>40082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_New  Confirmed_Cumulative  Deaths_New  \\\n",
       "67 2020-04-17          30915                696621        2080   \n",
       "68 2020-04-18          28084                724705        1743   \n",
       "69 2020-04-19          26013                750718        1634   \n",
       "70 2020-04-20          25093                775811        1643   \n",
       "71 2020-04-21          25311                801122        2627   \n",
       "\n",
       "    Deaths_Cumulative  \n",
       "67              32435  \n",
       "68              34178  \n",
       "69              35812  \n",
       "70              37455  \n",
       "71              40082  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "usa_df = tables_list[0].copy()\n",
    "usa_df.columns = ['Date', 'West_AK', 'West_AZ', 'West_CA', 'West_CO', 'West_HI', 'West_ID', 'West_MT', 'West_NM', 'West_NV', 'West_OR',\n",
    "                  'West_UT', 'West_WA', 'West_WY', 'Midwest_IA', 'Midwest_IL', 'Midwest_IN', 'Midwest_KS', 'Midwest_MI', 'Midwest_MN',\n",
    "                  'Midwest_MO', 'Midwest_ND', 'Midwest_NE', 'Midwest_OH', 'Midwest_OK', 'Midwest_SD', 'Midwest_WI', 'South_AL',\n",
    "                  'South_AR', 'South_FL', 'South_GA', 'South_KY', 'South_LA', 'South_MS', 'South_NC', 'South_SC', 'South_TN', 'South_TX',\n",
    "                  'South_VA', 'South_WV', 'Northeast_CT', 'Northeast_DC', 'Northeast_DE', 'Northeast_MA', 'Northeast_MD',\n",
    "                  'Northeast_ME', 'Northeast_NH', 'Northeast_NJ', 'Northeast_NY', 'Northeast_PA', 'Northeast_RI', 'Northeast_VT',\n",
    "                  'Territories_GU', 'Territories_MP', 'Territories_PR', 'Territories_VI', 'Date_Date.1', 'Confirmed_New',\n",
    "                  'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative', 'Recovered_New', 'Recovered_Cumulative']\n",
    "usa_df = usa_df.iloc[:-5]\n",
    "columns_list = ['Date', 'West_AK', 'West_AZ', 'West_CA', 'West_CO', 'West_HI', 'West_ID', 'West_MT', 'West_NM', 'West_NV', 'West_OR',\n",
    "                'West_UT', 'West_WA', 'West_WY', 'Midwest_IA', 'Midwest_IL', 'Midwest_IN', 'Midwest_KS', 'Midwest_MI', 'Midwest_MN',\n",
    "                'Midwest_MO', 'Midwest_ND', 'Midwest_NE', 'Midwest_OH', 'Midwest_OK', 'Midwest_SD', 'Midwest_WI', 'South_AL',\n",
    "                'South_AR', 'South_FL', 'South_GA', 'South_KY', 'South_LA', 'South_MS', 'South_NC', 'South_SC', 'South_TN', 'South_TX',\n",
    "                'South_VA', 'South_WV', 'Northeast_CT', 'Northeast_DC', 'Northeast_DE', 'Northeast_MA', 'Northeast_MD',\n",
    "                'Northeast_ME', 'Northeast_NH', 'Northeast_NJ', 'Northeast_NY', 'Northeast_PA', 'Northeast_RI', 'Northeast_VT',\n",
    "                'Territories_GU', 'Territories_MP', 'Territories_PR', 'Territories_VI', 'Confirmed_New',\n",
    "                'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative', 'Recovered_New', 'Recovered_Cumulative']\n",
    "usa_df = usa_df[columns_list]\n",
    "idx_list = list(range(16)) + list(range(18, 49)) + list(range(51, 72))\n",
    "mask_series = usa_df.index.isin(idx_list)\n",
    "usa_df = usa_df[mask_series]\n",
    "date_format = '%b %d, %Y'\n",
    "for column_name in usa_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        usa_df[column_name] = usa_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        usa_df[column_name] = pd.to_datetime(usa_df[column_name])\n",
    "    else:\n",
    "        usa_df[column_name] = usa_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        usa_df[column_name] = pd.to_numeric(usa_df[column_name], errors='coerce')\n",
    "for column_name in [cn for cn in usa_df.columns if cn.endswith('_Cumulative')]:\n",
    "    usa_df[column_name].ffill(inplace=True)\n",
    "for column_name in [cn for cn in usa_df.columns if cn.endswith('_New') or cn.endswith('_Cumulative')]:\n",
    "    usa_df[column_name].fillna(value=0, inplace=True)\n",
    "    usa_df[column_name] = usa_df[column_name].map(lambda x: int(x))\n",
    "    usa_df[column_name] = pd.to_numeric(usa_df[column_name], errors='coerce')\n",
    "s.store_objects(usa_df=usa_df)\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "usa_df.tail(5)[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "us_deaths_df = tables_list[1].copy()\n",
    "us_deaths_df.columns = ['Date', 'West_AK', 'West_AZ', 'West_CA', 'West_CO', 'West_HI', 'West_ID', 'West_MT', 'West_NM', 'West_NV',\n",
    "                        'West_OR', 'West_UT', 'West_WA', 'West_WY', 'Midwest_IA', 'Midwest_IL', 'Midwest_IN', 'Midwest_KS',\n",
    "                        'Midwest_MI', 'Midwest_MN', 'Midwest_MO', 'Midwest_ND', 'Midwest_NE', 'Midwest_OH', 'Midwest_OK', 'Midwest_SD',\n",
    "                        'Midwest_WI', 'South_AL', 'South_AR', 'South_FL', 'South_GA', 'South_KY', 'South_LA', 'South_MS', 'South_NC',\n",
    "                        'South_SC', 'South_TN', 'South_TX', 'South_VA', 'South_WV', 'Northeast_CT', 'Northeast_DC', 'Northeast_DE',\n",
    "                        'Northeast_MA', 'Northeast_MD', 'Northeast_ME', 'Northeast_NH', 'Northeast_NJ', 'Northeast_NY',\n",
    "                        'Northeast_PA', 'Northeast_RI', 'Northeast_VT', 'Territories_GU', 'Territories_MP', 'Territories_PR', 'Territories_VI',\n",
    "                        'Deaths_New', 'Deaths_Cumulative']\n",
    "us_deaths_df = us_deaths_df.iloc[:-5]\n",
    "date_format = '%b %d, %Y'\n",
    "for column_name in us_deaths_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        us_deaths_df[column_name] = us_deaths_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        us_deaths_df[column_name] = pd.to_datetime(us_deaths_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        us_deaths_df[column_name] = us_deaths_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        us_deaths_df[column_name] = pd.to_numeric(us_deaths_df[column_name], errors='coerce')\n",
    "#us_deaths_df.set_index('Date', drop=True, inplace=True)\n",
    "s.store_objects(us_deaths_df=us_deaths_df)\n",
    "#columns_list = ['Date', 'Deaths_New', 'Deaths_Cumulative']\n",
    "#us_deaths_df[columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Italy'\n",
    "tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/Italy_medical_cases'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\italy_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_New</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_New</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2020-04-12</td>\n",
       "      <td>4092</td>\n",
       "      <td>156363</td>\n",
       "      <td>431.0</td>\n",
       "      <td>19899.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>3153</td>\n",
       "      <td>159516</td>\n",
       "      <td>566.0</td>\n",
       "      <td>20465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>2972</td>\n",
       "      <td>162488</td>\n",
       "      <td>602.0</td>\n",
       "      <td>21067.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>2667</td>\n",
       "      <td>165155</td>\n",
       "      <td>578.0</td>\n",
       "      <td>21645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>3786</td>\n",
       "      <td>168941</td>\n",
       "      <td>525.0</td>\n",
       "      <td>22170.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_New  Confirmed_Cumulative  Deaths_New  \\\n",
       "53 2020-04-12           4092                156363       431.0   \n",
       "54 2020-04-13           3153                159516       566.0   \n",
       "55 2020-04-14           2972                162488       602.0   \n",
       "56 2020-04-15           2667                165155       578.0   \n",
       "57 2020-04-16           3786                168941       525.0   \n",
       "\n",
       "    Deaths_Cumulative  \n",
       "53            19899.0  \n",
       "54            20465.0  \n",
       "55            21067.0  \n",
       "56            21645.0  \n",
       "57            22170.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xlsx_dir = os.path.join(s.data_folder, 'xlsx')\n",
    "file_path = os.path.join(xlsx_dir, 'italy.xlsx')\n",
    "italy_df = pd.read_excel(file_path)\n",
    "italy_df.columns = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative', 'Active_ICU', 'Active_Cumulative',\n",
    "                    'Recovered_Cumulative', 'Tested_Cumulative']\n",
    "for column_name in italy_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        italy_df[column_name] = pd.to_datetime(italy_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        italy_df[column_name] = italy_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        italy_df[column_name] = pd.to_numeric(italy_df[column_name], errors='coerce')\n",
    "s.store_objects(italy_df=italy_df)\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "italy_df[columns_list].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "italy_df = s.load_object('italy_df')\n",
    "italy_df.set_index('Date', drop=True, inplace=True)\n",
    "columns_list = ['Deaths_New', 'Deaths_Cumulative']\n",
    "tracking_df = us_deaths_df[columns_list].merge(italy_df[columns_list], how='outer',\n",
    "                                               left_index=True, right_index=True, suffixes=('_usa', '_italy'))\n",
    "for column_name in tracking_df.columns:\n",
    "    tracking_df[column_name] = pd.to_numeric(tracking_df[column_name], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, (64, 30)), (7, (61, 30)), (2, (62, 5)), (15, (65, 2)), (5, (21, 6)), (9, (12, 10)), (10, (12, 10)), (11, (12, 10)), (17, (20, 2)), (18, (19, 2)), (1, (11, 2)), (12, (6, 3)), (13, (6, 3)), (29, (8, 2)), (30, (7, 2)), (32, (7, 2)), (19, (6, 2)), (23, (6, 2)), (33, (6, 2)), (24, (5, 2)), (22, (4, 2)), (25, (4, 2)), (26, (3, 2)), (27, (3, 2)), (21, (2, 2)), (28, (2, 2)), (31, (2, 2)), (0, (1, 2)), (3, (1, 2)), (8, (1, 2)), (14, (1, 2)), (16, (1, 2)), (20, (1, 2)), (4, (1, 1))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Spain'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\spain_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_New</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_New</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2020-04-12</td>\n",
       "      <td>3477</td>\n",
       "      <td>169496.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>17489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>3045</td>\n",
       "      <td>172541.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>18056.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>5092</td>\n",
       "      <td>177633.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>18579.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>5183</td>\n",
       "      <td>182816.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>19130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>5252</td>\n",
       "      <td>188068.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>19478.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_New  Confirmed_Cumulative  Deaths_New  \\\n",
       "50 2020-04-12           3477              169496.0       517.0   \n",
       "51 2020-04-13           3045              172541.0       567.0   \n",
       "52 2020-04-14           5092              177633.0       523.0   \n",
       "53 2020-04-15           5183              182816.0       551.0   \n",
       "54 2020-04-16           5252              188068.0       348.0   \n",
       "\n",
       "    Deaths_Cumulative  \n",
       "50            17489.0  \n",
       "51            18056.0  \n",
       "52            18579.0  \n",
       "53            19130.0  \n",
       "54            19478.0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spain_df = tables_list[6].copy()\n",
    "districts_list = ['AN', 'AR', 'AS', 'IB', 'CN', 'CB', 'CM', 'CL', 'CT', 'CE', 'VC', 'EX', 'GA', 'MD', 'ML', 'MU', 'NA',\n",
    "                  'PV', 'RI']\n",
    "summaries_list = ['Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative', 'ICU_Cumulative',\n",
    "                  'Recovered_Cumulative', 'Tested_Cumulative']\n",
    "spain_df.columns = ['Date', 'Time'] + districts_list + summaries_list + ['Refs', 'Notes']\n",
    "spain_df = spain_df.loc[3:]\n",
    "spain_df = spain_df.iloc[:-6]\n",
    "for column_name in spain_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        spain_df[column_name] = pd.to_datetime(spain_df[column_name])\n",
    "        pass\n",
    "    elif column_name == 'Time':\n",
    "        spain_df[column_name] = pd.to_timedelta(spain_df[column_name]+':00')\n",
    "        pass\n",
    "    else:\n",
    "        spain_df[column_name] = spain_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        spain_df[column_name] = pd.to_numeric(spain_df[column_name], errors='coerce')\n",
    "spain_df.reset_index(level=0, inplace=True)\n",
    "s.store_objects(spain_df=spain_df)\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "spain_df.tail(5)[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/Spain_medical_cases'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_columns_list = districts_list + summaries_list\n",
    "df = pd.DataFrame([], columns=df_columns_list)\n",
    "summed_list = districts_list + ['Confirmed_New', 'Deaths_New']\n",
    "for column_name in summed_list:\n",
    "    new_group = spain_df.groupby('Date')[column_name]\n",
    "    df[column_name] = new_group.agg(np.sum)\n",
    "for column_name in ['Confirmed_Cumulative', 'Deaths_Cumulative', 'ICU_Cumulative', 'Recovered_Cumulative', 'Tested_Cumulative']:\n",
    "    cumulative_group = spain_df.groupby('Date')[column_name]\n",
    "    df[column_name] = cumulative_group.agg(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, (55, 25)), (8, (105, 7)), (3, (54, 5)), (11, (65, 2)), (4, (6, 10)), (13, (20, 2)), (14, (19, 2)), (2, (13, 2)), (25, (8, 2)), (26, (7, 2)), (28, (7, 2)), (15, (6, 2)), (19, (6, 2)), (29, (6, 2)), (20, (5, 2)), (30, (5, 2)), (18, (4, 2)), (21, (4, 2)), (22, (3, 2)), (23, (3, 2)), (17, (2, 2)), (24, (2, 2)), (27, (2, 2)), (0, (1, 2)), (1, (1, 2)), (5, (1, 2)), (6, (1, 2)), (7, (1, 2)), (10, (1, 2)), (12, (1, 2)), (16, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_France'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\france_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_New</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_New</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>2572.0</td>\n",
       "      <td>54097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>1846.0</td>\n",
       "      <td>55943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>2494.0</td>\n",
       "      <td>58437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2986.0</td>\n",
       "      <td>61423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>2804.0</td>\n",
       "      <td>64227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_New  Confirmed_Cumulative  Deaths_New  \\\n",
       "46 2020-04-04         2572.0                 54097         0.0   \n",
       "47 2020-04-05         1846.0                 55943         0.0   \n",
       "48 2020-04-06         2494.0                 58437         0.0   \n",
       "49 2020-04-07         2986.0                 61423         0.0   \n",
       "50 2020-04-08         2804.0                 64227         0.0   \n",
       "\n",
       "    Deaths_Cumulative  \n",
       "46                2.0  \n",
       "47                2.0  \n",
       "48                2.0  \n",
       "49                2.0  \n",
       "50                2.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "france_df = tables_list[9].copy()\n",
    "france_df.columns = ['Date', 'Metropolitan_ARA', 'Metropolitan_BFC', 'Metropolitan_BRE', 'Metropolitan_CVL', 'Metropolitan_COR',\n",
    "                     'Metropolitan_GES', 'Metropolitan_HDF', 'Metropolitan_IDF', 'Metropolitan_NOR', 'Metropolitan_NAQ', 'Metropolitan_OCC',\n",
    "                     'Metropolitan_PDL', 'Metropolitan_PACA', 'Overseas_La_R', 'Overseas_S_M', 'Overseas_S_B',\n",
    "                     'Overseas_Mart', 'Overseas_Guad', 'Overseas_May', 'Overseas_Guy', 'Confirmed_New', 'Confirmed_Cumulative',\n",
    "                     'Deaths_New', 'Deaths_Cumulative']\n",
    "france_df = france_df.iloc[:-4]\n",
    "for column_name in france_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        france_df[column_name] = pd.to_datetime(france_df[column_name])\n",
    "    else:\n",
    "        france_df[column_name] = france_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        france_df[column_name] = pd.to_numeric(france_df[column_name], errors='coerce')\n",
    "france_df.Deaths_Cumulative = france_df.Deaths_Cumulative.ffill()\n",
    "france_df.Deaths_New.fillna(value=0, inplace=True)\n",
    "france_df.Confirmed_New.fillna(value=0, inplace=True)\n",
    "s.store_objects(france_df=france_df)\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "france_df[columns_list].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## United Kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (81, 21)), (1, (65, 2)), (3, (20, 2)), (4, (19, 2)), (15, (8, 2)), (16, (7, 2)), (18, (7, 2)), (5, (6, 2)), (9, (6, 2)), (19, (6, 2)), (10, (5, 2)), (8, (4, 2)), (11, (4, 2)), (12, (3, 2)), (13, (3, 2)), (7, (2, 2)), (14, (2, 2)), (17, (2, 2)), (2, (1, 2)), (6, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_the_United_Kingdom'\n",
    "tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/United_Kingdom_medical_cases'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\uk_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_New</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_Hospital_New</th>\n",
       "      <th>Deaths_Hospital_Cumulative</th>\n",
       "      <th>Deaths_Certificate_New</th>\n",
       "      <th>Deaths_Certificate_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2020-04-12</td>\n",
       "      <td>5288.0</td>\n",
       "      <td>84279</td>\n",
       "      <td>717.0</td>\n",
       "      <td>11329.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>4342.0</td>\n",
       "      <td>88621</td>\n",
       "      <td>778.0</td>\n",
       "      <td>12107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>5252.0</td>\n",
       "      <td>93873</td>\n",
       "      <td>761.0</td>\n",
       "      <td>12868.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>4605.0</td>\n",
       "      <td>98476</td>\n",
       "      <td>861.0</td>\n",
       "      <td>13729.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>4618.0</td>\n",
       "      <td>103093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13729.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6235.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_New  Confirmed_Cumulative  Deaths_Hospital_New  \\\n",
       "72 2020-04-12         5288.0                 84279                717.0   \n",
       "73 2020-04-13         4342.0                 88621                778.0   \n",
       "74 2020-04-14         5252.0                 93873                761.0   \n",
       "75 2020-04-15         4605.0                 98476                861.0   \n",
       "76 2020-04-16         4618.0                103093                  0.0   \n",
       "\n",
       "    Deaths_Hospital_Cumulative  Deaths_Certificate_New  \\\n",
       "72                     11329.0                     0.0   \n",
       "73                     12107.0                     0.0   \n",
       "74                     12868.0                     0.0   \n",
       "75                     13729.0                     0.0   \n",
       "76                     13729.0                     0.0   \n",
       "\n",
       "    Deaths_Certificate_Cumulative  \n",
       "72                         6235.0  \n",
       "73                         6235.0  \n",
       "74                         6235.0  \n",
       "75                         6235.0  \n",
       "76                         6235.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "uk_df = tables_list[0].copy()\n",
    "#['{}_{}'.format(column_tuple[0], column_tuple[1]) for column_tuple in uk_df.columns.tolist()]\n",
    "uk_df.columns = ['Date', 'England_East', 'England_London', 'England_Midlands', 'England_NE_Yorks', 'England_North_West',\n",
    "                 'England_South_East', 'England_South_West', 'England_Unclassified', 'Northern_Ireland', 'Scotland', 'Wales',\n",
    "                 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_Hospital_New', 'Deaths_Hospital_Cumulative',\n",
    "                 'Deaths_Certificate_New', 'Deaths_Certificate_Cumulative', 'Tested_New', 'Tested_Cumulative', 'Sources']\n",
    "uk_df = uk_df.iloc[:-4]\n",
    "date_format = '%d %b, %Y'\n",
    "for column_name in uk_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        uk_df[column_name] = uk_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(re.sub('\\\\xa0', ' ', x)), date_format))\n",
    "        uk_df[column_name] = pd.to_datetime(uk_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        uk_df[column_name] = uk_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        uk_df[column_name] = pd.to_numeric(uk_df[column_name], errors='coerce')\n",
    "for column_name in [cn for cn in uk_df.columns if cn.endswith('_New')]:\n",
    "    uk_df[column_name].fillna(value=0, inplace=True)\n",
    "for column_name in [cn for cn in uk_df.columns if cn.endswith('_Cumulative')]:\n",
    "    uk_df[column_name].ffill(inplace=True)\n",
    "    uk_df[column_name].fillna(value=0, inplace=True)\n",
    "s.store_objects(uk_df=uk_df)\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_Hospital_New', 'Deaths_Hospital_Cumulative',\n",
    "                 'Deaths_Certificate_New', 'Deaths_Certificate_Cumulative']\n",
    "uk_df[columns_list].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Philippines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, (217, 10)), (2, (54, 5)), (6, (20, 10)), (14, (65, 2)), (7, (18, 7)), (8, (15, 7)), (16, (20, 2)), (17, (19, 2)), (36, (12, 3)), (4, (7, 5)), (1, (12, 2)), (11, (12, 2)), (33, (10, 2)), (28, (8, 2)), (10, (5, 3)), (29, (7, 2)), (31, (7, 2)), (18, (6, 2)), (22, (6, 2)), (32, (6, 2)), (23, (5, 2)), (37, (5, 2)), (21, (4, 2)), (24, (4, 2)), (25, (3, 2)), (26, (3, 2)), (20, (2, 2)), (27, (2, 2)), (30, (2, 2)), (34, (2, 2)), (35, (2, 2)), (0, (1, 2)), (5, (1, 2)), (9, (1, 2)), (12, (1, 2)), (13, (1, 2)), (15, (1, 2)), (19, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_the_Philippines'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\philippines_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>4932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>5223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>5453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>5660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>5878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_New\n",
       "48 2020-04-13           4932\n",
       "49 2020-04-14           5223\n",
       "50 2020-04-15           5453\n",
       "51 2020-04-16           5660\n",
       "52 2020-04-17           5878"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "philippines_df = tables_list[2].copy()\n",
    "philippines_df.columns = philippines_df.iloc[0].tolist()\n",
    "columns_list = [1, 3]\n",
    "philippines_df = philippines_df.iloc[:, columns_list]\n",
    "philippines_df.columns = ['Date', 'Confirmed_New']\n",
    "#philippines_df.Date.to_dict()\n",
    "idx_list = [1] + [3] + [5] + [7] + [9] + list(range(11, 53))\n",
    "mask_series = philippines_df.index.isin(idx_list)\n",
    "philippines_df = philippines_df[mask_series]\n",
    "for column_name in philippines_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        philippines_df[column_name] = pd.to_datetime(philippines_df[column_name])\n",
    "    else:\n",
    "        philippines_df[column_name] = philippines_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        philippines_df[column_name] = pd.to_numeric(philippines_df[column_name], errors='coerce')\n",
    "s.store_objects(philippines_df=philippines_df)\n",
    "philippines_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Thailand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, (342, 12)), (3, (65, 5)), (6, (19, 7)), (7, (65, 2)), (9, (22, 2)), (10, (21, 2)), (1, (13, 2)), (19, (10, 2)), (20, (9, 2)), (22, (8, 2)), (23, (7, 2)), (4, (2, 6)), (11, (5, 2)), (12, (5, 2)), (15, (4, 2)), (21, (4, 2)), (13, (3, 2)), (16, (3, 2)), (17, (3, 2)), (18, (2, 2)), (0, (1, 2)), (2, (1, 2)), (8, (1, 2)), (14, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Thailand'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\thailand_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "thailand_df = tables_list[3].copy()\n",
    "thailand_df.columns = thailand_df.iloc[0].tolist()\n",
    "columns_list = [1, 3]\n",
    "thailand_df = thailand_df.iloc[:, columns_list]\n",
    "thailand_df.columns = ['Date', 'Confirmed_New']\n",
    "#thailand_df.Date.to_dict()\n",
    "idx_list = [1, 3,] + list(range(5, 20)) + list(range(21, 26)) + list(range(27, 29)) + list(range(30, 42)) + list(range(43, 64))\n",
    "mask_series = thailand_df.index.isin(idx_list)\n",
    "thailand_df = thailand_df[mask_series]\n",
    "for column_name in thailand_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        thailand_df[column_name] = pd.to_datetime(thailand_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        thailand_df[column_name] = thailand_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        thailand_df[column_name] = pd.to_numeric(thailand_df[column_name], errors='coerce')\n",
    "s.store_objects(thailand_df=thailand_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, (121, 12)), (2, (47, 5)), (5, (65, 2)), (3, (21, 3)), (7, (22, 2)), (8, (21, 2)), (1, (11, 2)), (17, (10, 2)), (18, (9, 2)), (20, (8, 2)), (21, (7, 2)), (9, (5, 2)), (10, (5, 2)), (13, (4, 2)), (19, (4, 2)), (11, (3, 2)), (14, (3, 2)), (15, (3, 2)), (16, (2, 2)), (0, (1, 2)), (6, (1, 2)), (12, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Vietnam'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\vietnam_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vietnam_df = tables_list[2].copy()\n",
    "vietnam_df.columns = vietnam_df.iloc[0].tolist()\n",
    "columns_list = [1, 3]\n",
    "vietnam_df = vietnam_df.iloc[:, columns_list]\n",
    "vietnam_df.columns = ['Date', 'Confirmed_New']\n",
    "#vietnam_df.Date.to_dict()\n",
    "idx_list = [1, 3,] + list(range(5, 19)) + list(range(20, 23)) + [24] + list(range(26, 46))\n",
    "mask_series = vietnam_df.index.isin(idx_list)\n",
    "vietnam_df = vietnam_df[mask_series]\n",
    "for column_name in vietnam_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        vietnam_df[column_name] = pd.to_datetime(vietnam_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        vietnam_df[column_name] = vietnam_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        vietnam_df[column_name] = pd.to_numeric(vietnam_df[column_name], errors='coerce')\n",
    "s.store_objects(vietnam_df=vietnam_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Malaysia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, (120, 8)), (6, (113, 5)), (7, (46, 7)), (3, (56, 5)), (5, (13, 18)), (9, (20, 9)), (12, (65, 2)), (14, (22, 2)), (15, (21, 2)), (2, (16, 2)), (4, (6, 5)), (24, (10, 2)), (25, (9, 2)), (11, (4, 4)), (27, (8, 2)), (10, (3, 5)), (28, (7, 2)), (16, (5, 2)), (17, (5, 2)), (20, (4, 2)), (26, (4, 2)), (18, (3, 2)), (21, (3, 2)), (22, (3, 2)), (23, (2, 2)), (0, (1, 2)), (1, (1, 2)), (13, (1, 2)), (19, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Malaysia'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\malaysia_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "malaysia_df = tables_list[7].copy()\n",
    "malaysia_df.columns = ['Date', 'Selangor', 'Johor', 'Kuala_Lumpur', 'Kedah', 'North_Sembilan', 'Confirmed_New']\n",
    "malaysia_df = malaysia_df.loc[:43]\n",
    "date_format = '%d %B, %Y'\n",
    "for column_name in malaysia_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        malaysia_df[column_name] = malaysia_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        malaysia_df[column_name] = pd.to_datetime(malaysia_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        malaysia_df[column_name] = malaysia_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        malaysia_df[column_name] = pd.to_numeric(malaysia_df[column_name], errors='coerce')\n",
    "s.store_objects(malaysia_df=malaysia_df)\n",
    "#columns_list = ['Date', 'Confirmed_New']\n",
    "#malaysia_df[columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## South Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, (75, 28)), (2, (30, 13)), (3, (61, 5)), (5, (20, 7)), (7, (20, 7)), (9, (62, 2)), (4, (13, 7)), (6, (8, 5)), (11, (19, 2)), (12, (18, 2)), (1, (14, 2)), (21, (10, 2)), (22, (9, 2)), (24, (8, 2)), (25, (7, 2)), (14, (5, 2)), (17, (4, 2)), (23, (4, 2)), (15, (3, 2)), (18, (3, 2)), (19, (3, 2)), (13, (2, 2)), (20, (2, 2)), (0, (1, 2)), (10, (1, 2)), (16, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_South_Korea'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\south_korea_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "south_korea_df = tables_list[8].copy()\n",
    "districts_list = ['Gyeonggi_Incheon', 'Gyeonggi_Seoul', 'Gyeonggi_Gyeonggi', 'Gangwon_Gangwon',\n",
    "                  'Gyeongsang_Gyeongbuk', 'Gyeongsang_Daegu', 'Gyeongsang_Gyeongnam', 'Gyeongsang_Busan',\n",
    "                  'Gyeongsang_Ulsan', 'Chungcheong_Chungbuk', 'Chungcheong_Sejong', 'Chungcheong_Daejeon',\n",
    "                  'Chungcheong_Chungnam', 'Jeolla_Jeonbuk', 'Jeolla_Gwangju', 'Jeolla_Jeonnam',\n",
    "                  'Jeolla_Jeju', 'Airport Screening']\n",
    "summaries_list = ['Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative',\n",
    "                  'Tested_Cumulative', 'Tested_New', 'Discharged_Cumulative']\n",
    "south_korea_df.columns = ['Date', 'Time'] + districts_list + summaries_list + ['Sources']\n",
    "#south_korea_df.Date.to_dict()\n",
    "idx_list = list(range(18)) + list(range(19, 42)) + list(range(43, 69))\n",
    "mask_series = south_korea_df.index.isin(idx_list)\n",
    "south_korea_df = south_korea_df[mask_series]\n",
    "for column_name in south_korea_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        south_korea_df[column_name] = pd.to_datetime(south_korea_df[column_name])\n",
    "        pass\n",
    "    elif column_name == 'Time':\n",
    "        south_korea_df[column_name] = pd.to_timedelta(south_korea_df[column_name]+':00')\n",
    "        pass\n",
    "    else:\n",
    "        south_korea_df[column_name] = south_korea_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        south_korea_df[column_name] = pd.to_numeric(south_korea_df[column_name], errors='coerce')\n",
    "df_columns_list = districts_list + summaries_list\n",
    "df = pd.DataFrame([], columns=df_columns_list)\n",
    "summed_list = districts_list + ['Confirmed_New', 'Deaths_New', 'Tested_New']\n",
    "for column_name in summed_list:\n",
    "    new_group = south_korea_df.groupby('Date')[column_name]\n",
    "    df[column_name] = new_group.agg(np.sum)\n",
    "for column_name in ['Confirmed_Cumulative', 'Deaths_Cumulative', 'Tested_Cumulative', 'Discharged_Cumulative']:\n",
    "    cumulative_group = south_korea_df.groupby('Date')[column_name]\n",
    "    df[column_name] = cumulative_group.agg(max)\n",
    "df.reset_index(level=0, inplace=True)\n",
    "s.store_objects(south_korea_df=df)\n",
    "#columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative']\n",
    "#df[columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, (62, 50)), (3, (55, 7)), (2, (70, 5)), (5, (62, 2)), (7, (19, 2)), (8, (18, 2)), (1, (10, 2)), (17, (10, 2)), (18, (9, 2)), (20, (8, 2)), (21, (7, 2)), (10, (5, 2)), (13, (4, 2)), (19, (4, 2)), (11, (3, 2)), (14, (3, 2)), (15, (3, 2)), (9, (2, 2)), (16, (2, 2)), (0, (1, 2)), (6, (1, 2)), (12, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Japan'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\japan_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "japan_df = tables_list[4].copy()\n",
    "japan_df.columns = ['Date', 'Hokkaidō', 'Honshū_Tōhoku_Aomori', 'Honshū_Tōhoku_Akita',\n",
    "                    'Honshū_Tōhoku_Miyagi', 'Honshū_Tōhoku_Fukushima', 'Honshū_Kantō_Tochigi', 'Honshū_Kantō_Ibaraki',\n",
    "                    'Honshū_Kantō_Chiba', 'Honshū_Kantō_Tōkyō', 'Honshū_Kantō_Kanagawa', 'Honshū_Kantō_Saitama',\n",
    "                    'Honshū_Kantō_Gunma', 'Honshū_Chūbu_Niigata', 'Honshū_Chūbu_Nagano', 'Honshū_Chūbu_Yamanashi',\n",
    "                    'Honshū_Chūbu_Shizuoka', 'Honshū_Chūbu_Aichi', 'Honshū_Chūbu_Ishikawa', 'Honshū_Chūbu_Fukui',\n",
    "                    'Honshū_Chūbu_Gifu', 'Honshū_Kansai_Mie', 'Honshū_Kansai_Shiga', 'Honshū_Kansai_Wakayama', 'Honshū_Kansai_Nara',\n",
    "                    'Honshū_Kansai_Kyōto', 'Honshū_Kansai_Ōsaka', 'Honshū_Kansai_Hyōgo', 'Honshū_Chūgoku_Okayama',\n",
    "                    'Honshū_Chūgoku_Hiroshima', 'Honshū_Chūgoku_Yamaguchi', 'Shikoku_Shikoku_Kagawa',\n",
    "                    'Shikoku_Shikoku_Tokushima', 'Shikoku_Shikoku_Kōchi', 'Shikoku_Shikoku_Ehime', 'Kyūshū_Kyūshū_Ōita',\n",
    "                    'Kyūshū_Kyūshū_Fukuoka', 'Kyūshū_Kyūshū_Saga', 'Kyūshū_Kyūshū_Nagasaki', 'Kyūshū_Kyūshū_Kumamoto',\n",
    "                    'Kyūshū_Kyūshū_Miyazaki', 'Okinawa', 'Airport', 'Diamond_Princess_Workers', 'Repatriated_Total',\n",
    "                    'Abroad_Total', 'Confirmed_New', 'Confirmed_Total', 'Tested_Total', 'Sources']\n",
    "#japan_df.Date.to_dict()\n",
    "idx_list = list(range(14)) + list(range(15, 32)) + list(range(33, 56))\n",
    "mask_series = japan_df.index.isin(idx_list)\n",
    "japan_df = japan_df[mask_series]\n",
    "#date_format = '%Y/%M/%d'\n",
    "for column_name in japan_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        #japan_df[column_name] = japan_df[column_name].map(lambda x: datetime.strptime('{}'.format(x), date_format))\n",
    "        japan_df[column_name] = pd.to_datetime(japan_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        japan_df[column_name] = japan_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        japan_df[column_name] = pd.to_numeric(japan_df[column_name], errors='coerce')\n",
    "s.store_objects(japan_df=japan_df)\n",
    "#columns_list = ['Date', 'Confirmed_Total', 'Confirmed_New']\n",
    "#japan_df[columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Slovakia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\slovakia_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "slovakia_df = s.load_object('slovakia_df')\n",
    "slovakia_df.columns = ['Date', 'Bratislava', 'Žilina', 'Košice', 'Trnava', 'Trenčín', 'Prešov', 'Banská_Bystrica',\n",
    "                       'Nitra', 'Confirmed_New', 'Confirmed_Total', 'Deaths_New', 'Deaths_Total',\n",
    "                       'Recoveries_Total', 'Tested_Total', 'References']\n",
    "slovakia_df = slovakia_df.loc[:17]\n",
    "for column_name in slovakia_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        slovakia_df[column_name] = pd.to_datetime(slovakia_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        slovakia_df[column_name] = slovakia_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        slovakia_df[column_name] = pd.to_numeric(slovakia_df[column_name], errors='coerce')\n",
    "s.store_objects(slovakia_df=slovakia_df)\n",
    "#columns_list = ['Date', 'Confirmed_New', 'Confirmed_Total', 'Deaths_New', 'Deaths_Total']\n",
    "#slovakia_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\singapore_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "singapore_df = s.load_object('singapore_df')\n",
    "singapore_df.columns = singapore_df.iloc[0].tolist()\n",
    "singapore_df = singapore_df.loc[1:61]\n",
    "columns_list = [1, 3, 4]\n",
    "singapore_df = singapore_df.iloc[:, columns_list]\n",
    "singapore_df.columns = ['Date', 'Confirmed_New', 'Deaths_New']\n",
    "idx_list = list(range(11)) + list(range(12, 62))\n",
    "mask_series = singapore_df.index.isin(idx_list)\n",
    "singapore_df = singapore_df[mask_series]\n",
    "for column_name in singapore_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        singapore_df[column_name] = pd.to_datetime(singapore_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        singapore_df[column_name] = singapore_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        singapore_df[column_name] = pd.to_numeric(singapore_df[column_name], errors='coerce')\n",
    "s.store_objects(singapore_df=singapore_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\netherlands_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "netherlands_df = s.load_object('netherlands_df')\n",
    "netherlands_df.columns = netherlands_df.iloc[0].tolist()\n",
    "netherlands_df = netherlands_df.loc[1:27]\n",
    "columns_list = [1, 3, 4]\n",
    "netherlands_df = netherlands_df.iloc[:, columns_list]\n",
    "netherlands_df.columns = ['Date', 'Confirmed_New', 'Deaths_New']\n",
    "for column_name in netherlands_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        netherlands_df[column_name] = pd.to_datetime(netherlands_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        netherlands_df[column_name] = netherlands_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        netherlands_df[column_name] = pd.to_numeric(netherlands_df[column_name], errors='coerce')\n",
    "s.store_objects(netherlands_df=netherlands_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hong_kong_df = s.load_object('hong_kong_df')\n",
    "hong_kong_df.columns = hong_kong_df.iloc[0].tolist()\n",
    "hong_kong_df = hong_kong_df.loc[1:62]\n",
    "columns_list = [1, 4]\n",
    "hong_kong_df = hong_kong_df.iloc[:, columns_list]\n",
    "hong_kong_df.columns = ['Date', 'Confirmed_New']\n",
    "hong_kong_df.Confirmed_New = hong_kong_df.Confirmed_New.map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "hong_kong_df.Confirmed_New = pd.to_numeric(hong_kong_df.Confirmed_New, errors='coerce')\n",
    "hong_kong_df.Date = pd.to_datetime(hong_kong_df.Date)\n",
    "s.store_objects(hong_kong_df=hong_kong_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "germany_df = s.load_object('germany_df')\n",
    "germany_df.columns = ['Date', 'Baden-Württemberg', 'Bavaria', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg', 'Hesse',\n",
    "                      'Mecklenburg-Vorpommern', 'Lower Saxony', 'North Rhine-Westphalia', 'Rhineland-Palatinate', 'Saarland',\n",
    "                      'Saxony', 'Saxony-Anhalt', 'Schleswig-Holstein', 'Thuringia', 'Confirmed_Repatriated', 'Confirmed_Total',\n",
    "                      'Deaths_Total', 'Confirmed_New', 'Deaths_New']\n",
    "date_format = '%d %b, %Y'\n",
    "for column_name in germany_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        germany_df[column_name] = germany_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        germany_df[column_name] = pd.to_datetime(germany_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        germany_df[column_name] = germany_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        germany_df[column_name] = pd.to_numeric(germany_df[column_name], errors='coerce')\n",
    "s.store_objects(germany_df=germany_df)\n",
    "#columns_list = ['Date', 'Confirmed_Repatriated', 'Confirmed_Total', 'Deaths_Total', 'Confirmed_New', 'Deaths_New']\n",
    "#germany_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "czech_df = s.load_object('czech_df')\n",
    "czech_df.columns = ['Date', 'Confirmed_New', 'Confirmed_Total', 'Recovered_New', 'Recovered_Total', 'Deaths_New', 'Deaths_Total',\n",
    "                    'Tested_Total']\n",
    "date_format = '%d %B, %Y'\n",
    "for column_name in czech_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        czech_df[column_name] = czech_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        czech_df[column_name] = pd.to_datetime(czech_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        czech_df[column_name] = czech_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        czech_df[column_name] = pd.to_numeric(czech_df[column_name], errors='coerce')\n",
    "s.store_objects(czech_df=czech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "china_df = s.load_object('china_df')\n",
    "china_df.columns = china_df.iloc[0].tolist()\n",
    "china_df = china_df.loc[1:77]\n",
    "columns_list = [1, 4]\n",
    "china_df = china_df.iloc[:, columns_list]\n",
    "china_df.columns = ['Date', 'Confirmed_New']\n",
    "china_df.Confirmed_New = china_df.Confirmed_New.map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "china_df.Confirmed_New = pd.to_numeric(china_df.Confirmed_New, errors='coerce')\n",
    "china_df.Date = pd.to_datetime(china_df.Date)\n",
    "s.store_objects(china_df=china_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page_title = '2019–20 coronavirus pandemic'\n",
    "page_obj = wikipedia.page(title=page_title)\n",
    "print(['page_obj.{}'.format(fn) for fn in dir(page_obj) if 'link' in fn.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(['page_obj.{}'.format(fn) for fn in dir(page_obj) if 'url' in fn.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "countries_list = ['Italy']\n",
    "locations_list = [link for link in set(page_obj.links) if ' in ' in link]\n",
    "for page_title in locations_list:\n",
    "    if any(map(lambda x: x in page_title, countries_list)):\n",
    "        page_obj = wikipedia.page(title=page_title)\n",
    "        tables_url = page_obj.url\n",
    "        tables_list = get_page_tables(tables_url)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(page_title)\n",
    "tables_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "locations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df = s.load_object('pandemic_df')\n",
    "pandemic_df.loc[12, 'R0_low'] = 2\n",
    "pandemic_df.loc[12, 'R0_high'] = 6\n",
    "s.store_objects(pandemic_df=pandemic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df.set_index('Disease', drop=True, inplace=True)\n",
    "s.store_objects(pandemic_df=pandemic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_name = 'COVID-19'\n",
    "csv_path = os.path.abspath(os.path.join(s.data_csv_folder, '{}.csv'.format(csv_name)))\n",
    "covid19_df = pd.read_csv(csv_path, encoding=s.encoding_type, header=[0, 1])\n",
    "covid19_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_regex = re.compile(r'[^0-9A-Za-z]+')\n",
    "[name_regex.sub('_', '{} {}'.format(cn_tuple[0], cn_tuple[1]).lower()) for cn_tuple in covid19_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covid19_df.columns = ['Disease', 'deaths_per_day_global_rounded', 'total_news_mentions_millions_data_retrieved_3rd_mar_2020',\n",
    "                      'news_mentions_per_death', 'CFR_low', 'R0_low', 'annual_global_fatalities_latest_data_year',\n",
    "                      'new_cases_per_year_latest_data_year', 'days_of_outbreak', 'fatalty_notes', 'source', 'url']\n",
    "covid19_df.set_index('Disease', drop=True, inplace=True)\n",
    "s.store_objects(covid19_df=covid19_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df = s.load_object('pandemic_df')\n",
    "pandemic_df.index = ['Pertussis', 'Diphtheria', 'Measles', 'Smallpox', 'Rubella', 'Mumps', 'SARS', 'MERS', 'Swine flu (A/H1N1)',\n",
    "                     'Seasonal Flu', 'Hong Kong flu (1968 pandemic)', 'Spanish flu (1918 pandemic)', 'COVID-19 (Wuhan Coronavirus)']\n",
    "s.store_objects(pandemic_df=pandemic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s.store_objects(pandemic_df=pandemic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df = s.load_object('pandemic_df')\n",
    "covid19_df = s.load_object('covid19_df')\n",
    "pandemic_df = pd.merge(left=pandemic_df, right=covid19_df, how='outer', left_index=True, right_index=True, suffixes=('_old', '_new'))\n",
    "pandemic_df.columns = ['Transmission', 'R0', 'Case_fatality_ratio', 'Hospitalized_cases_sent_to_ICU', 'Asymptomatic_transmission',\n",
    "                       'Year_vaccine_available', 'R0_high', 'R0_low', 'CFR_high', 'CFR_low', 'deaths_per_day_global_rounded',\n",
    "                       'total_news_mentions_millions_data_retrieved_3rd_mar_2020', 'news_mentions_per_death', 'CFR_new',\n",
    "                       'R0_new', 'annual_global_fatalities_latest_data_year', 'new_cases_per_year_latest_data_year',\n",
    "                       'days_of_outbreak', 'fatalty_notes', 'source', 'url']\n",
    "columns_list = sorted([cn for cn in pandemic_df.columns if ('_low' in cn) or ('_high' in cn) or ('_new' in cn)])\n",
    "num_regex = re.compile(r'[^0-9.]+')\n",
    "for disease_name, row_series in pandemic_df[columns_list].iterrows():\n",
    "    CFR_high = row_series['CFR_high']\n",
    "    CFR_low = row_series['CFR_low']\n",
    "    CFR_new = row_series['CFR_new']\n",
    "    if str(CFR_new) != 'nan':\n",
    "        CFR_new = float(num_regex.sub('', str(CFR_new)))\n",
    "    if str(CFR_high) == 'nan':\n",
    "        pandemic_df.loc[disease_name, 'CFR_high'] = CFR_new\n",
    "    if str(CFR_low) == 'nan':\n",
    "        pandemic_df.loc[disease_name, 'CFR_low'] = CFR_new\n",
    "    if str(CFR_new) != 'nan':\n",
    "        if CFR_new < CFR_low:\n",
    "            pandemic_df.loc[disease_name, 'CFR_low'] = CFR_new\n",
    "        elif CFR_new > CFR_high:\n",
    "            pandemic_df.loc[disease_name, 'CFR_high'] = CFR_new\n",
    "    R0_high = row_series['R0_high']\n",
    "    R0_low = row_series['R0_low']\n",
    "    R0_new = row_series['R0_new']\n",
    "    if str(R0_new) != 'nan':\n",
    "        R0_new = float(num_regex.sub('', str(R0_new)))\n",
    "    if str(R0_high) == 'nan':\n",
    "        pandemic_df.loc[disease_name, 'R0_high'] = R0_new\n",
    "    if str(R0_low) == 'nan':\n",
    "        pandemic_df.loc[disease_name, 'R0_low'] = R0_new\n",
    "    if str(R0_new) != 'nan':\n",
    "        if R0_new < R0_low:\n",
    "            pandemic_df.loc[disease_name, 'R0_low'] = R0_new\n",
    "        elif R0_new > R0_high:\n",
    "            pandemic_df.loc[disease_name, 'R0_high'] = R0_new\n",
    "columns_list = ['Transmission', 'R0', 'Case_fatality_ratio', 'Hospitalized_cases_sent_to_ICU', 'Asymptomatic_transmission',\n",
    "                'Year_vaccine_available', 'R0_high', 'R0_low', 'CFR_high', 'CFR_low', 'deaths_per_day_global_rounded',\n",
    "                'total_news_mentions_millions_data_retrieved_3rd_mar_2020', 'news_mentions_per_death',\n",
    "                'annual_global_fatalities_latest_data_year', 'new_cases_per_year_latest_data_year',\n",
    "                'days_of_outbreak', 'fatalty_notes', 'source', 'url']\n",
    "pandemic_df = pandemic_df[columns_list]\n",
    "pandemic_df.sample(5).T.sample(6).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covid19_df = s.load_object('covid19_df')\n",
    "covid19_df.index = ['Tuberculosis', 'Hepatitis B', 'Pneumonia', 'HIV ', 'Malaria ', 'Shigellosis', 'Rotavirus', 'Seasonal Flu',\n",
    "                    'Norovirus', 'Whooping Cough', 'Typhoid', 'Cholera', 'Meningitis', 'Measles', 'Rabies ', 'Yellow Fever',\n",
    "                    'Leishmaniasis', 'Echinococcosis', 'COVID-19 (Wuhan Coronavirus)', 'Dengue Fever', 'Hepatitis A',\n",
    "                    'Chicken Pox', 'Sleeping Sickness', 'Ebola', 'SARS', 'MERS']\n",
    "s.store_objects(covid19_df=covid19_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df = s.load_object('pandemic_df')\n",
    "covid19_df = s.load_object('covid19_df')\n",
    "check_for_typos(left_list=covid19_df.index.tolist(), right_list=pandemic_df.index.tolist(), verbose=False).sort_values('left_item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_page_soup(page_url):\n",
    "    with urllib.request.urlopen(page_url) as response:\n",
    "        page_html = response.read()\n",
    "    page_soup = bs(page_html, 'html.parser')\n",
    "    \n",
    "    return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables_url = 'https://www.zorinaq.com/pub/ncov-comparison.html'\n",
    "ncov_path = os.path.join(s.data_folder, 'html', 'ncov-comparison.html')\n",
    "#page_soup = get_page_soup(tables_url)\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "pandemic_df = tables_list[0].copy()\n",
    "pandemic_df.columns = ['Disease', 'Transmission', 'R0', 'Case_fatality_ratio', 'Hospitalized_cases_sent_to_ICU',\n",
    "                       'Asymptomatic_transmission', 'Year_vaccine_available']\n",
    "\n",
    "def f(x):\n",
    "    result = re.sub(r' *\\(', ' (', str(x))\n",
    "    \n",
    "    return result\n",
    "\n",
    "pandemic_df.Disease = pandemic_df.Disease.map(f)\n",
    "\n",
    "def f(x):\n",
    "    result = re.sub(r'⁹', '', str(x))\n",
    "    \n",
    "    return result\n",
    "\n",
    "pandemic_df.Transmission = pandemic_df.Transmission.map(f)\n",
    "pandemic_df['R0_high'] = np.nan\n",
    "\n",
    "def f(x):\n",
    "    result = str(x).split('-')[-1]\n",
    "    result = re.sub(r'[^0-9.]+', '', result)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "pandemic_df['R0_high'] = pandemic_df.R0.map(f)\n",
    "pandemic_df['R0_low'] = np.nan\n",
    "\n",
    "def f(x):\n",
    "    result_list = str(x).split('-')\n",
    "    if len(result_list) < 3:\n",
    "        result = result_list[0]\n",
    "    else:\n",
    "        result = result_list[1].split(' ')[-1]\n",
    "    result = re.sub(r'[^0-9.]+', '', result)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "pandemic_df['R0_low'] = pandemic_df.R0.map(f)\n",
    "pandemic_df['CFR_high'] = np.nan\n",
    "\n",
    "def f(x):\n",
    "    result = str(x).split('-')[-1]\n",
    "    result = re.sub(r'[^0-9.]+', '', result)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "pandemic_df['CFR_high'] = pandemic_df.Case_fatality_ratio.map(f)\n",
    "pandemic_df['CFR_low'] = np.nan\n",
    "\n",
    "def f(x):\n",
    "    result_list = str(x).split('-')\n",
    "    result = result_list[0]\n",
    "    result = re.sub(r'[^0-9.]+', '', result)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "pandemic_df['CFR_low'] = pandemic_df.Case_fatality_ratio.map(f)\n",
    "#print(pandemic_df.R0.unique().tolist())\n",
    "#print(pandemic_df.R0.map(f).unique().tolist())\n",
    "s.store_objects(pandemic_df=pandemic_df)\n",
    "pandemic_df.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_format = '%b %d, %Y'\n",
    "usa_df.Date = usa_df.Date.map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "s.store_objects(usa_df=usa_df)\n",
    "s.save_dataframes(usa_df=usa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "usa_df = s.load_object('usa_df')\n",
    "usa_df.set_index('Date', drop=True, inplace=True)\n",
    "italy_df = s.load_object('italy_df')\n",
    "italy_df.set_index('Date', drop=True, inplace=True)\n",
    "columns_list = ['Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "tracking_df = usa_df[columns_list].merge(italy_df[columns_list], how='outer',\n",
    "                                         left_index=True, right_index=True, suffixes=('_usa', '_italy'))\n",
    "s.store_objects(tracking_df=tracking_df)\n",
    "s.save_dataframes(include_index=True, tracking_df=tracking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "czech_df: False\n",
      "germany_df: False\n",
      "hong_kong_df: False\n",
      "iran_df: False\n",
      "italy_df: False\n",
      "japan_df: False\n",
      "netherlands_df: False\n",
      "singapore_df: False\n",
      "slovakia_df: False\n",
      "south_korea_df: False\n",
      "spain_df: False\n",
      "uk_df: False\n",
      "usa_df: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "china_df = s.load_object('china_df')\n",
    "china_df.Date = china_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "china_df.set_index('Date', drop=True, inplace=True)\n",
    "czech_df = s.load_object('czech_df')\n",
    "czech_df.Date = czech_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "czech_df.set_index('Date', drop=True, inplace=True)\n",
    "columns_list = list(set(china_df.columns).intersection(set(czech_df.columns)))\n",
    "merge_df = china_df[columns_list].merge(czech_df[columns_list], how='outer',\n",
    "                                        left_index=True, right_index=True,\n",
    "                                        suffixes=('_china', '_czech'))\n",
    "print('czech_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "germany_df = s.load_object('germany_df')\n",
    "germany_df.Date = germany_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "germany_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(germany_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_germany'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany']\n",
    "print('germany_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "hong_kong_df = s.load_object('hong_kong_df')\n",
    "hong_kong_df.Date = hong_kong_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "hong_kong_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(hong_kong_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_hong_kong'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong']\n",
    "print('hong_kong_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "iran_df = s.load_object('iran_df')\n",
    "iran_df.Date = iran_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "iran_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(iran_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_iran'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong', 'Confirmed_New_iran']\n",
    "print('iran_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "italy_df = s.load_object('italy_df')\n",
    "italy_df.Date = italy_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "italy_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(italy_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_italy'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy']\n",
    "print('italy_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "japan_df = s.load_object('japan_df')\n",
    "japan_df.Date = japan_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "japan_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(japan_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_japan'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan']\n",
    "print('japan_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "netherlands_df = s.load_object('netherlands_df')\n",
    "netherlands_df.Date = netherlands_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "netherlands_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(netherlands_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_netherlands'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands']\n",
    "print('netherlands_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "singapore_df = s.load_object('singapore_df')\n",
    "singapore_df.Date = singapore_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "singapore_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(singapore_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_singapore'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands', 'Confirmed_New_singapore']\n",
    "print('singapore_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "slovakia_df = s.load_object('slovakia_df')\n",
    "slovakia_df.Date = slovakia_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "slovakia_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(slovakia_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_slovakia'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands',\n",
    "                    'Confirmed_New_singapore', 'Confirmed_New_slovakia']\n",
    "print('slovakia_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "south_korea_df = s.load_object('south_korea_df')\n",
    "south_korea_df.Date = south_korea_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "south_korea_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(south_korea_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_south_korea'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands',\n",
    "                    'Confirmed_New_singapore', 'Confirmed_New_slovakia', 'Confirmed_New_south_korea']\n",
    "print('south_korea_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "spain_df = s.load_object('spain_df')\n",
    "spain_df.Date = spain_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "spain_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(spain_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_spain'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands',\n",
    "                    'Confirmed_New_singapore', 'Confirmed_New_slovakia', 'Confirmed_New_south_korea', 'Confirmed_New_spain']\n",
    "print('spain_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "uk_df = s.load_object('uk_df')\n",
    "uk_df.Date = uk_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "uk_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(uk_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_uk'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands',\n",
    "                    'Confirmed_New_singapore', 'Confirmed_New_slovakia', 'Confirmed_New_south_korea',\n",
    "                    'Confirmed_New_spain', 'Confirmed_New_uk']\n",
    "print('uk_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "usa_df = s.load_object('usa_df')\n",
    "usa_df.Date = usa_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "usa_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(usa_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_usa'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong', 'Confirmed_New_iran',\n",
    " 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands', 'Confirmed_New_singapore',\n",
    " 'Confirmed_New_slovakia', 'Confirmed_New_south_korea', 'Confirmed_New_spain', 'Confirmed_New_uk', 'Confirmed_New_usa']\n",
    "print('usa_df:', merge_df.index.has_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "countries_list = ['Thailand', 'China', 'Japan', 'South Korea', 'Laos', 'Cambodia', 'Singapore', 'the Philippines', 'Hong Kong',\n",
    "                  'Vietnam', 'Malaysia']\n",
    "df_set = set(['{}_df'.format('_'.join(country.lower().split(' '))) for country in countries_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'south_korea_df', 'hong_kong_df', 'japan_df', 'the_philippines_df', 'singapore_df', 'malaysia_df', 'china_df', 'vietnam_df', 'thailand_df', 'laos_df', 'cambodia_df'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'south_korea_df', 'hong_kong_df', 'japan_df', 'thailand_df', 'malaysia_df', 'china_df', 'vietnam_df', 'singapore_df'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "created_set = set()\n",
    "df_name_list = []\n",
    "for file_name in os.listdir(s.saves_pickle_folder):\n",
    "    if file_name.endswith('_df.pickle'):\n",
    "        df_name = file_name.split('.')[0]\n",
    "        df_name_list.append(df_name)\n",
    "        if df_name in df_set:\n",
    "            created_set.add(df_name)\n",
    "created_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "usa_df = s.load_object('usa_df')\n",
    "\n",
    "# Reporting Country/Territory/Area Total_confirmed_cases Total_confirmed_new_cases Total_deaths Total_new_deaths\n",
    "# Transmission_classification Days_since_last_reported_case\n",
    "\n",
    "# Timestamp('2020-03-26 00:00:00')\n",
    "# United States of America 63570 11656 884 211 Local transmission 0\n",
    "usa_df.loc[38, 'Confirmed_Cumulative'] = 63570\n",
    "usa_df.loc[38, 'Confirmed_New'] = 11656\n",
    "usa_df.loc[38, 'Deaths_Cumulative'] = 884\n",
    "usa_df.loc[38, 'Deaths_New'] = 211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Timestamp('2020-03-27 00:00:00')\n",
    "# United States of America 68334 4764 991 107 Local transmission 0\n",
    "usa_df.loc[39, 'Confirmed_Cumulative'] = 68334\n",
    "usa_df.loc[39, 'Confirmed_New'] = 4764\n",
    "usa_df.loc[39, 'Deaths_Cumulative'] = 991\n",
    "usa_df.loc[39, 'Deaths_New'] = 107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# United States of America 85228 16894 1243 252 Local transmission 0\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "row_dict = {'Date': pd.Timestamp('2020-03-28 00:00:00'), 'Confirmed_New': 16894,\n",
    "            'Confirmed_Cumulative': 85228, 'Deaths_New': 252, 'Deaths_Cumulative': 1243}\n",
    "usa_df = pd.concat([usa_df, pd.DataFrame([row_dict], columns=columns_list, index=[40])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# United States of America ??? ??? ??? ??? Local transmission 0\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "row_dict = {'Date': pd.Timestamp('2020-03-29 00:00:00'), 'Confirmed_New': xxxxxx,\n",
    "            'Confirmed_Cumulative': xxxxxx, 'Deaths_New': xxxxxx, 'Deaths_Cumulative': xxxxxx}\n",
    "usa_df = pd.concat([usa_df, pd.DataFrame([row_dict], columns=columns_list, index=[41])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\covid19\\saves\\pickle\\usa_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s.store_objects(usa_df=usa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed_New</th>\n",
       "      <th>Confirmed_Cumulative</th>\n",
       "      <th>Deaths_New</th>\n",
       "      <th>Deaths_Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-02-20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>15.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>28.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>26.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>64.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>77.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>101.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>144.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>148.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>291.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>269.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>393.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2163.0</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>662.0</td>\n",
       "      <td>2825.0</td>\n",
       "      <td>7</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>676.0</td>\n",
       "      <td>3501.0</td>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>872.0</td>\n",
       "      <td>4373.0</td>\n",
       "      <td>13</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>1289.0</td>\n",
       "      <td>5662.0</td>\n",
       "      <td>21</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>2412.0</td>\n",
       "      <td>8074.0</td>\n",
       "      <td>26</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>3944.0</td>\n",
       "      <td>12018.0</td>\n",
       "      <td>52</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>5288.0</td>\n",
       "      <td>17306.0</td>\n",
       "      <td>51</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>6271.0</td>\n",
       "      <td>23577.0</td>\n",
       "      <td>67</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>8631.0</td>\n",
       "      <td>32208.0</td>\n",
       "      <td>109</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>10408.0</td>\n",
       "      <td>42749.0</td>\n",
       "      <td>111</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>9923.0</td>\n",
       "      <td>52672.0</td>\n",
       "      <td>159</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>9417.0</td>\n",
       "      <td>62089.0</td>\n",
       "      <td>225</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>11656.0</td>\n",
       "      <td>63570.0</td>\n",
       "      <td>211</td>\n",
       "      <td>884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>4764.0</td>\n",
       "      <td>68334.0</td>\n",
       "      <td>107</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>16894.0</td>\n",
       "      <td>85228.0</td>\n",
       "      <td>252</td>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Confirmed_New  Confirmed_Cumulative  Deaths_New  \\\n",
       "0  2020-01-21            1.0                   1.0           0   \n",
       "1  2020-01-24            1.0                   2.0           0   \n",
       "2  2020-01-25            1.0                   3.0           0   \n",
       "3  2020-01-26            2.0                   5.0           0   \n",
       "4  2020-01-30            1.0                   6.0           0   \n",
       "5  2020-01-31            1.0                   7.0           0   \n",
       "6  2020-02-01            1.0                   8.0           0   \n",
       "7  2020-02-02            3.0                  11.0           0   \n",
       "8  2020-02-05            1.0                  12.0           0   \n",
       "9  2020-02-20            2.0                  14.0           0   \n",
       "10 2020-02-26            1.0                  15.0           0   \n",
       "11 2020-02-28            4.0                  19.0           0   \n",
       "12 2020-02-29            5.0                  24.0           1   \n",
       "13 2020-03-01           18.0                  42.0           1   \n",
       "14 2020-03-02           15.0                  57.0           4   \n",
       "15 2020-03-03           28.0                  85.0           3   \n",
       "16 2020-03-04           26.0                 111.0           2   \n",
       "17 2020-03-05           64.0                 175.0           1   \n",
       "18 2020-03-06           77.0                 252.0           5   \n",
       "19 2020-03-07          101.0                 353.0           2   \n",
       "20 2020-03-08          144.0                 497.0           2   \n",
       "21 2020-03-09          148.0                 645.0           5   \n",
       "22 2020-03-10          291.0                 936.0           5   \n",
       "23 2020-03-11          269.0                1205.0           7   \n",
       "24 2020-03-12          393.0                1598.0           4   \n",
       "25 2020-03-13          565.0                2163.0           7   \n",
       "26 2020-03-14          662.0                2825.0           7   \n",
       "27 2020-03-15          676.0                3501.0           6   \n",
       "28 2020-03-16          872.0                4373.0          13   \n",
       "29 2020-03-17         1289.0                5662.0          21   \n",
       "30 2020-03-18         2412.0                8074.0          26   \n",
       "31 2020-03-19         3944.0               12018.0          52   \n",
       "32 2020-03-20         5288.0               17306.0          51   \n",
       "33 2020-03-21         6271.0               23577.0          67   \n",
       "34 2020-03-22         8631.0               32208.0         109   \n",
       "35 2020-03-23        10408.0               42749.0         111   \n",
       "36 2020-03-24         9923.0               52672.0         159   \n",
       "37 2020-03-25         9417.0               62089.0         225   \n",
       "38 2020-03-26        11656.0               63570.0         211   \n",
       "39 2020-03-27         4764.0               68334.0         107   \n",
       "40 2020-03-28        16894.0               85228.0         252   \n",
       "\n",
       "    Deaths_Cumulative  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "5                   0  \n",
       "6                   0  \n",
       "7                   0  \n",
       "8                   0  \n",
       "9                   0  \n",
       "10                  0  \n",
       "11                  0  \n",
       "12                  1  \n",
       "13                  2  \n",
       "14                  6  \n",
       "15                  9  \n",
       "16                 11  \n",
       "17                 12  \n",
       "18                 17  \n",
       "19                 19  \n",
       "20                 21  \n",
       "21                 26  \n",
       "22                 31  \n",
       "23                 38  \n",
       "24                 42  \n",
       "25                 49  \n",
       "26                 56  \n",
       "27                 62  \n",
       "28                 75  \n",
       "29                 96  \n",
       "30                122  \n",
       "31                174  \n",
       "32                225  \n",
       "33                292  \n",
       "34                401  \n",
       "35                512  \n",
       "36                671  \n",
       "37                906  \n",
       "38                884  \n",
       "39                991  \n",
       "40               1243  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "usa_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\slovakia_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\iran_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\usa_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\italy_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\germany_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\thailand_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\hong_kong_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\czech_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\singapore_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\china_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\philippines_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\malaysia_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\japan_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\spain_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\uk_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\vietnam_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\netherlands_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\south_korea_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "asian_list = ['south_korea_df', 'hong_kong_df', 'japan_df', 'thailand_df', 'malaysia_df', 'china_df', 'vietnam_df',\n",
    "              'singapore_df', 'philippines_df']\n",
    "cumulative_list = ['china_df', 'czech_df', 'germany_df', 'hong_kong_df', 'iran_df', 'italy_df', 'japan_df', 'netherlands_df',\n",
    "                   'singapore_df', 'slovakia_df', 'south_korea_df', 'spain_df', 'uk_df', 'usa_df']\n",
    "for df_name in set(asian_list + cumulative_list):\n",
    "    df = s.load_object(df_name)\n",
    "    df.columns = [re.sub('Recoveries_Cumulative', 'Recovered_Cumulative', cn) for cn in df.columns]\n",
    "    kwargs = {df_name: df}\n",
    "    s.store_objects(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Arkansas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-65a682068f85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmatch_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates_stats_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mabbrev_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates_stats_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmatch_series\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mState_Abbreviation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mabbrev_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mabbrev_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstate_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mabbrev_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-65a682068f85>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmatch_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates_stats_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mabbrev_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates_stats_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmatch_series\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mState_Abbreviation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mabbrev_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mabbrev_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstate_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mabbrev_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Arkansas'"
     ]
    }
   ],
   "source": [
    "\n",
    "states_stats_df = s.load_object('states_stats_df')\n",
    "state_list = ['Arkansas', 'Iowa', 'Nebraska', 'North Dakota', 'South Dakota']\n",
    "mask_series = states_stats_df.index.isin(state_list)\n",
    "abbrev_dict = states_stats_df[mask_series].State_Abbreviation.to_dict()\n",
    "abbrev_list = [abbrev_dict[state_name] for state_name in state_list]\n",
    "abbrev_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Alabama', 'Alaska', 'Arizona', 'California', 'Colorado', 'Connecticut',\n",
       "       'Delaware', 'Florida', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
       "       'Kansas', 'Louisiana', 'Maine', 'Maryland', 'Michigan', 'Minnesota',\n",
       "       'Mississippi', 'Montana', 'Nevada', 'New Jersey', 'New Mexico',\n",
       "       'New York', 'North Carolina', 'Ohio', 'Oregon', 'Pennsylvania',\n",
       "       'Rhode Island', 'South Carolina', 'Tennessee', 'Texas', 'Vermont',\n",
       "       'Virginia', 'Washington', 'West Virginia', 'Wisconsin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "states_stats_df.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
