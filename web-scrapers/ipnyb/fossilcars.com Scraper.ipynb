{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "siteUrl = \"http://www.fossilcars.com\"\n",
    "tagPageUrlArray = [\"http://www.fossilcars.com/classic-cars\",\n",
    "                   \"http://www.fossilcars.com/muscle-cars\",\n",
    "                   \"http://www.fossilcars.com/classic-trucks\",\n",
    "                   \"http://www.fossilcars.com/hot-rods\",\n",
    "                   \"http://www.fossilcars.com/antique-cars\",\n",
    "                   \"http://www.fossilcars.com/fastback\",\n",
    "                   \"http://www.fossilcars.com/luxury-classics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.fossilcars.com/classic-trucks\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tagPageUrl = tagPageUrlArray[randint(0,len(tagPageUrlArray)-1)]\n",
    "print(tagPageUrl)\n",
    "\n",
    "# Retrieve the page with tag results and set it up to be scraped\n",
    "tagPage = requests.get(url=tagPageUrl)\n",
    "tagPageSoup = BeautifulSoup(tagPage.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.fossilcars.com/classic-trucks/page157\n"
     ]
    }
   ],
   "source": [
    "# Get the last page number\n",
    "links = tagPageSoup.select('#c_pagination > div.c_pagination > a')\n",
    "if len(links):\n",
    "    max_page = int(links[-2].text)\n",
    "else:\n",
    "    max_page = 1\n",
    "\n",
    "# Get the random page\n",
    "i = randint(1,max_page)\n",
    "linkPageUrl = tagPageUrl+\"/page\"+str(i)\n",
    "print(linkPageUrl)\n",
    "\n",
    "# Retrieve the page with tag results and set it up to be scraped\n",
    "linkPage = requests.get(url=linkPageUrl)\n",
    "linkPageSoup = BeautifulSoup(linkPage.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "# Get the random link\n",
    "links = linkPageSoup.find_all(\"a\", class_=\"c_row_link\")\n",
    "link = links[randint(0,len(links)-1)]\n",
    "\n",
    "# find the private sellers\n",
    "dealer_divs = link.find_all(\"div\", class_=\"c_dealer\", onclick=True)\n",
    "print(len(dealer_divs))\n",
    "\n",
    "# Get the soup\n",
    "url = urllib.parse.urljoin(siteUrl, re.sub(' ', '%20', link['href']))\n",
    "page = requests.get(url=url)\n",
    "soup = BeautifulSoup(page.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972 Chevrolet C10\n",
      "$35,995\n",
      "Call For Info:231-468-2809\n"
     ]
    }
   ],
   "source": [
    "model = soup.find_all(\"h1\", itemprop=\"name\")\n",
    "if len(model):\n",
    "    model = model[0].get_text()\n",
    "else:\n",
    "    model = \"Unknown\"\n",
    "print(model)\n",
    "\n",
    "price = soup.find_all(\"div\", class_=\"da_price\")\n",
    "if len(price):\n",
    "    price = price[0].get_text()\n",
    "else:\n",
    "    price = \"Unknown\"\n",
    "print(price)\n",
    "\n",
    "try:\n",
    "    phone = soup.find_all(\"div\", class_=\"da_phone\")\n",
    "    if len(phone):\n",
    "        phone = phone[0].get_text()\n",
    "    else:\n",
    "        phone = \"\"\n",
    "except:\n",
    "    phone = \"\"\n",
    "print(phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tagPageUrl in tagPageUrlArray:\n",
    "    \n",
    "    # Retrieve the page with tag results and set it up to be scraped\n",
    "    tagPage = requests.get(url=tagPageUrl)\n",
    "    tagPageSoup = BeautifulSoup(tagPage.content, 'lxml')\n",
    "    \n",
    "    # Get the last page number\n",
    "    links = tagPageSoup.select('#c_pagination > div.c_pagination > a')\n",
    "    if len(links):\n",
    "        max_page = int(links[-2].text)\n",
    "    else:\n",
    "        max_page = 1\n",
    "    \n",
    "    for i in range(1, max_page+1):\n",
    "        linkPageUrl = tagPageUrl+\"/page\"+str(i)\n",
    "\n",
    "        # Retrieve the page with tag results and set it up to be scraped\n",
    "        linkPage = requests.get(url=linkPageUrl)\n",
    "        linkPageSoup = BeautifulSoup(linkPage.content, 'lxml')\n",
    "\n",
    "        links = linkPageSoup.find_all(\"a\", class_=\"c_row_link\")\n",
    "        for link in links:\n",
    "            \n",
    "            # find the private sellers\n",
    "            dealer_divs = link.find_all(\"div\", class_=\"c_dealer\", onclick=True)\n",
    "            if not len(dealer_divs):\n",
    "                url = urllib.parse.urljoin(siteUrl, re.sub(' ', '%20', link['href']))\n",
    "                page = requests.get(url=url)\n",
    "                soup = BeautifulSoup(page.content, 'lxml')\n",
    "                \n",
    "                model = soup.find_all(\"h1\", itemprop=\"name\")\n",
    "                if len(model):\n",
    "                    model = model[0].get_text()\n",
    "                else:\n",
    "                    model = \"Unknown\"\n",
    "                price = soup.find_all(\"div\", class_=\"da_price\")\n",
    "                if len(price):\n",
    "                    price = price[0].get_text()\n",
    "                else:\n",
    "                    price = \"Unknown\"\n",
    "                \n",
    "                try:\n",
    "                    phone = soup.find_all(\"div\", class_=\"da_phone\")\n",
    "                    if len(phone):\n",
    "                        phone = phone[0].get_text()\n",
    "                    if len(phone):\n",
    "                        bigFile = open('fossilcars.txt', 'a', encoding='utf-8')\n",
    "                        bigFile.write(model + '\\t' + price + '\\t' + phone + '\\n')\n",
    "                        bigFile.close()\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
