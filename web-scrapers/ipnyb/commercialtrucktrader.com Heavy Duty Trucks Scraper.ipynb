{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.commercialtrucktrader.com/Heavy-Duty-Trucks-For-Sale/search-results?type=heavy\n"
     ]
    }
   ],
   "source": [
    "siteUrl = \"http://www.commercialtrucktrader.com/Heavy-Duty-Trucks-For-Sale/search-results\"\n",
    "siteUrl = \"?\".join([siteUrl, \"type=heavy\"])\n",
    "print(siteUrl)\n",
    "\n",
    "proxies = {'http': 'http://<<USERID>>:<<PASSWORD>>@23.19.51.245:40791/'}\n",
    "max_page = 0\n",
    "seen_urls_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719\n"
     ]
    }
   ],
   "source": [
    "#<a href=\"/Heavy-Duty-Trucks-For-Sale/search-results?type=heavy&amp;page=1799\" class=\"listings-pag-default\">1799</a>\n",
    "\n",
    "# Retrieve the page with tag results and set it up to be scraped\n",
    "sitePage = requests.get(url=siteUrl, proxies=proxies)\n",
    "sitePageSoup = BeautifulSoup(sitePage.content, 'lxml')\n",
    "\n",
    "css = 'div.listings-pag-bottom > div > a.listings-pag-default'\n",
    "pageLinks = sitePageSoup.select(css)\n",
    "if len(pageLinks):\n",
    "    max_page = int(pageLinks[-1][\"href\"].split('=')[2])\n",
    "print(max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42935\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, max_page+1):\n",
    "    tagUrl = \"&\".join([siteUrl, \"page=\"+str(i)])\n",
    "    page = requests.get(url=tagUrl, proxies=proxies)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "    # Get the title of the page to prove we are progressing\n",
    "    titleTag = soup.select(\"head > title\")\n",
    "    titleString = titleTag[0].get_text().strip()\n",
    "    #print(titleString)\n",
    "\n",
    "    # Get the links to the individual pages\n",
    "    pageLinks = soup.select('div.listing-header > a > h2')\n",
    "    for pageLink in pageLinks:\n",
    "        pageUrl = urllib.parse.urljoin(siteUrl, re.sub(' ', '%20', pageLink.parent['href']))\n",
    "        #print(pageUrl)\n",
    "        if pageUrl not in seen_urls_array:\n",
    "            seen_urls_array.append(pageUrl)\n",
    "    \n",
    "    # Destroy the tree when you're done working with it\n",
    "    soup.decompose()\n",
    "\n",
    "# Show the number of individual page links you have\n",
    "print(len(seen_urls_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('commercialtrucktraderHeavyDutyTrucks.pickle', 'wb') as handle:\n",
    "    pickle.dump(seen_urls_array, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('commercialtrucktraderHeavyDutyTrucks.pickle', 'rb') as handle:\n",
    "    seen_urls_array = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.commercialtrucktrader.com/listing/2011-Freightliner-Columbia-120--120787998\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "pageUrl = seen_urls_array[randint(0,len(seen_urls_array)-1)]\n",
    "print(pageUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from base64 import b64encode\n",
    "\n",
    "proxy = {'host': '23.19.51.245', 'port': '40791', 'usr': '<<USERID>>', 'pwd': '<<PASSWORD>>'}\n",
    "# Retrieve the page with tag results and set it up to be scraped\n",
    "fp = webdriver.FirefoxProfile()\n",
    "fp.set_preference('network.proxy.type', 1)\n",
    "fp.set_preference('network.proxy.http', proxy['host'])\n",
    "fp.set_preference('network.proxy.http_port', int(proxy['port']))\n",
    "fp.set_preference('network.proxy.no_proxies_on', 'localhost, 127.0.0.1')\n",
    "fp.add_extension('closeproxy.xpi')\n",
    "credentials = '{usr}:{pwd}'.format(**proxy)\n",
    "credentials = b64encode(credentials.encode('ascii')).decode('utf-8')\n",
    "fp.set_preference('extensions.closeproxyauth.authtoken', credentials)\n",
    "\n",
    "#set some privacy settings\n",
    "fp.set_preference( \"places.history.enabled\", False )\n",
    "fp.set_preference( \"privacy.clearOnShutdown.offlineApps\", True )\n",
    "fp.set_preference( \"privacy.clearOnShutdown.passwords\", True )\n",
    "fp.set_preference( \"privacy.clearOnShutdown.siteSettings\", True )\n",
    "fp.set_preference( \"privacy.sanitize.sanitizeOnShutdown\", True )\n",
    "fp.set_preference( \"signon.rememberSignons\", False )\n",
    "fp.set_preference( \"network.cookie.lifetimePolicy\", 2 )\n",
    "fp.set_preference( \"network.dns.disablePrefetch\", True )\n",
    "fp.set_preference( \"network.http.sendRefererHeader\", 0 )\n",
    "\n",
    "#if you're really hardcore about your security\n",
    "#js can be used to reveal your true i.p.\n",
    "fp.set_preference( \"javascript.enabled\", False )\n",
    "\n",
    "#get a huge speed increase by not downloading images\n",
    "fp.set_preference( \"permissions.default.image\", 2 )\n",
    "\n",
    "driver = webdriver.Firefox(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# set timeout information\n",
    "driver.set_page_load_timeout(5)\n",
    "\n",
    "finished = 0\n",
    "while finished == 0:\n",
    "    try:\n",
    "        driver.get(pageUrl)\n",
    "        finished = 1\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G  and  J Truck Sales\n",
      "2011 FREIGHTLINER COLUMBIA 120 CONVENTIONAL - SLEEPER TRUCK in Groom, TX\n",
      "Price: $16,900\n",
      ")866( 565-0320\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "seller = soup.select('div.details-right > strong')\n",
    "if len(seller):\n",
    "    seller = seller[0].get_text().strip()\n",
    "else:\n",
    "    seller = \"Unknown\"\n",
    "print(seller)\n",
    "\n",
    "model = soup.select(\"div.details > h1\")\n",
    "if len(model):\n",
    "    model = model[0].get_text().strip()\n",
    "else:\n",
    "    model = \"Unknown\"\n",
    "print(model)\n",
    "\n",
    "price = soup.select(\"div.details-right > h2.lfloat\")\n",
    "if len(price):\n",
    "    price = price[0].get_text().strip()\n",
    "else:\n",
    "    price = \"Unknown\"\n",
    "print(price)\n",
    "\n",
    "phone = soup.select(\"span.flipphone.bold.font1-1\")\n",
    "if len(phone):\n",
    "    phone = phone[0].get_text().strip()[::-1]\n",
    "else:\n",
    "    phone = \"\"\n",
    "print(phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pageUrl in seen_urls_array:\n",
    "    try:\n",
    "        finished = 0\n",
    "        while finished == 0:\n",
    "            try:\n",
    "                driver.get(pageUrl)\n",
    "                finished = 1\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                time.sleep(5)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        seller = soup.select('div.details-right > strong')\n",
    "        if len(seller):\n",
    "            seller = seller[0].get_text().strip()\n",
    "        else:\n",
    "            seller = \"Unknown\"\n",
    "        #print(seller)\n",
    "\n",
    "        model = soup.select(\"div.details > h1\")\n",
    "        if len(model):\n",
    "            model = model[0].get_text().strip()\n",
    "        else:\n",
    "            model = \"Unknown\"\n",
    "        #print(model)\n",
    "\n",
    "        price = soup.select(\"div.details-right > h2.lfloat\")\n",
    "        if len(price):\n",
    "            price = price[0].get_text().strip()\n",
    "        else:\n",
    "            price = \"Unknown\"\n",
    "        #print(price)\n",
    "\n",
    "        phone = soup.select(\"span.flipphone.bold.font1-1\")\n",
    "        if len(phone):\n",
    "            phone = phone[0].get_text().strip()[::-1]\n",
    "        else:\n",
    "            phone = \"\"\n",
    "        #print(phone)\n",
    "        \n",
    "        if len(phone):\n",
    "            bigFile = open('commercialtrucktraderHeavyDutyTrucks.txt', 'a', encoding='utf-8')\n",
    "            bigFile.write(seller + ': ' + model + '\\t' + price + '\\t' + phone + '\\n')\n",
    "            bigFile.close()\n",
    "    \n",
    "        # Destroy the tree when you're done working with it\n",
    "        soup.decompose()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
