{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The R0 is between 2 and 6, meaning each infected person infects 2–6 others, which can cause the number of infected to double every few days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://informationisbeautiful.net/visualizations/covid-19-coronavirus-infographic-datapack/\n",
    "# https://docs.google.com/spreadsheets/d/1g_YxmDfQx7aOU2DKzNZo9b-NTk62Bju6X3z6OuCa6gw/edit#gid=515684451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "command_str = '{sys.executable} -m pip install --upgrade statsmodels'.format(sys=sys)\n",
    "print(command_str)\n",
    "!{command_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\ipynb\\Corona Virus DataFrame Creation.ipynb\n",
      "['s.attempt_to_pickle', 's.data_csv_folder', 's.data_folder', 's.encoding_type', 's.load_csv', 's.load_dataframes', 's.load_object', 's.save_dataframes', 's.saves_csv_folder', 's.saves_folder', 's.saves_pickle_folder', 's.store_objects']\n",
      "Pretty printing has been turned OFF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Config', 'In', 'Out', 'RandomForestClassifier', 'SequenceMatcher', 'Storage', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__nonzero__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'bs', 'check_4_doubles', 'check_for_typos', 'conjunctify_list', 'copyfile', 'csv', 'encoding', 'example_iterrows', 'exit', 'filepath_regex', 'get_classifier', 'get_column_descriptions', 'get_data_structs_dataframe', 'get_datastructure_prediction', 'get_dir_tree', 'get_git_lfs_track_commands', 'get_importances', 'get_input_sample', 'get_ipython', 'get_max_rsquared_adj', 'get_module_version', 'get_notebook_path', 'get_page_tables', 'get_specific_gitignore_files', 'get_struct_name', 'humanize_bytes', 'io', 'ipykernel', 'json', 'jupyter_config_dir', 'math', 'notebook_path', 'notebookapp', 'nx', 'os', 'pd', 'pickle', 'plt', 'preprocess_data', 'print_all_files_ending_starting_with', 'print_all_files_ending_with', 'print_all_files_starting_with', 'quit', 're', 'remove_empty_folders', 's', 'scraping_utils', 'similar', 'sm', 'sns', 'stats', 'subprocess', 'sys', 'time', 'url_regex', 'urllib', 'wikipedia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%run ../../load_magic/storage.py\n",
    "%run ../../load_magic/paths.py\n",
    "%run ../../load_magic/lists.py\n",
    "%run ../../load_magic/environment.py\n",
    "%run ../../load_magic/dataframes.py\n",
    "\n",
    "import scraping_utils\n",
    "wikipedia = scraping_utils.wikipedia\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import networkx as nx\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "notebook_path = get_notebook_path()\n",
    "print(notebook_path)\n",
    "s = Storage()\n",
    "#pandemic_df = s.load_object('pandemic_df')\n",
    "print(['s.{}'.format(fn) for fn in dir(s) if not fn.startswith('_')])\n",
    "\n",
    "%pprint\n",
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\slovakia_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\iran_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\usa_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\italy_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\germany_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\thailand_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\hong_kong_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\czech_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\singapore_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\china_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\philippines_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\malaysia_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\japan_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\spain_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\uk_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\vietnam_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\netherlands_df.pickle\n",
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\south_korea_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "asian_list = ['south_korea_df', 'hong_kong_df', 'japan_df', 'thailand_df', 'malaysia_df', 'china_df', 'vietnam_df',\n",
    "              'singapore_df', 'philippines_df']\n",
    "cumulative_list = ['china_df', 'czech_df', 'germany_df', 'hong_kong_df', 'iran_df', 'italy_df', 'japan_df', 'netherlands_df',\n",
    "                   'singapore_df', 'slovakia_df', 'south_korea_df', 'spain_df', 'uk_df', 'usa_df']\n",
    "for df_name in set(asian_list + cumulative_list):\n",
    "    df = s.load_object(df_name)\n",
    "    df.columns = [re.sub('Recoveries_Cumulative', 'Recovered_Cumulative', cn) for cn in df.columns]\n",
    "    kwargs = {df_name: df}\n",
    "    s.store_objects(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!start %windir%\\explorer.exe \"{os.path.abspath(os.path.dirname(notebook_path))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "countries_list = ['Thailand', 'China', 'Japan', 'South Korea', 'Laos', 'Cambodia', 'Singapore', 'the Philippines', 'Hong Kong',\n",
    "                  'Vietnam', 'Malaysia']\n",
    "df_set = set(['{}_df'.format('_'.join(country.lower().split(' '))) for country in countries_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'south_korea_df', 'hong_kong_df', 'japan_df', 'the_philippines_df', 'singapore_df', 'malaysia_df', 'china_df', 'vietnam_df', 'thailand_df', 'laos_df', 'cambodia_df'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'south_korea_df', 'hong_kong_df', 'japan_df', 'thailand_df', 'malaysia_df', 'china_df', 'vietnam_df', 'singapore_df'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "created_set = set()\n",
    "df_name_list = []\n",
    "for file_name in os.listdir(s.saves_pickle_folder):\n",
    "    if file_name.endswith('_df.pickle'):\n",
    "        df_name = file_name.split('.')[0]\n",
    "        df_name_list.append(df_name)\n",
    "        if df_name in df_set:\n",
    "            created_set.add(df_name)\n",
    "created_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df_name in list(df_set - created_set):\n",
    "    country_name = df_name[:-3].title()\n",
    "    country_name = re.sub('The_', 'the_', country_name)\n",
    "    rm_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_{}'.format(country_name)\n",
    "    !\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" {rm_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Create the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (242, 6)), (1, (12, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\country_populations_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "country_populations_df = tables_list[0].copy()\n",
    "country_populations_df.columns = ['Rank', 'Country', 'Population', 'world_population_percent', 'Date', 'Source']\n",
    "country_populations_df = country_populations_df.loc[:240]\n",
    "for column_name in country_populations_df.columns:\n",
    "    if column_name == 'Country':\n",
    "        country_populations_df[column_name] = country_populations_df[column_name].map(lambda x: str(x).split('[')[0])\n",
    "    elif column_name in ['Population', 'Rank']:\n",
    "        country_populations_df[column_name] = country_populations_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        country_populations_df[column_name] = pd.to_numeric(country_populations_df[column_name], errors='coerce')\n",
    "country_populations_df.set_index('Country', drop=True, inplace=True)\n",
    "columns_list = ['Rank', 'Population', 'Date', 'Source']\n",
    "country_populations_df = country_populations_df[columns_list]\n",
    "s.store_objects(country_populations_df=country_populations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (43, 61)), (1, (30, 56)), (2, (66, 2)), (4, (22, 2)), (5, (21, 2)), (14, (11, 2)), (15, (10, 2)), (17, (8, 2)), (18, (7, 2)), (6, (5, 2)), (7, (5, 2)), (16, (5, 2)), (10, (4, 2)), (8, (3, 2)), (11, (3, 2)), (12, (3, 2)), (13, (2, 2)), (3, (1, 2)), (9, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/United_States_medical_cases'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\usa_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "usa_df = tables_list[0].copy()\n",
    "usa_df.columns = ['Date', 'West_AK', 'West_AZ', 'West_CA', 'West_CO', 'West_HI', 'West_ID', 'West_MT', 'West_NM', 'West_NV', 'West_OR',\n",
    "                  'West_UT', 'West_WA', 'West_WY', 'Midwest_IA', 'Midwest_IL', 'Midwest_IN', 'Midwest_KS', 'Midwest_MI', 'Midwest_MN',\n",
    "                  'Midwest_MO', 'Midwest_ND', 'Midwest_NE', 'Midwest_OH', 'Midwest_OK', 'Midwest_SD', 'Midwest_WI', 'South_AL',\n",
    "                  'South_AR', 'South_FL', 'South_GA', 'South_KY', 'South_LA', 'South_MS', 'South_NC', 'South_SC', 'South_TN', 'South_TX',\n",
    "                  'South_VA', 'South_WV', 'Northeast_CT', 'Northeast_DC', 'Northeast_DE', 'Northeast_MA', 'Northeast_MD',\n",
    "                  'Northeast_ME', 'Northeast_NH', 'Northeast_NJ', 'Northeast_NY', 'Northeast_PA', 'Northeast_RI', 'Northeast_VT',\n",
    "                  'Territories_GU', 'Territories_PR', 'Territories_VI', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New',\n",
    "                  'Deaths_Cumulative', 'Recovered_New', 'Recovered_Cumulative']\n",
    "usa_df = usa_df.loc[:37]\n",
    "date_format = '%b %d, %Y'\n",
    "for column_name in usa_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        usa_df[column_name] = usa_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        usa_df[column_name] = pd.to_datetime(usa_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        usa_df[column_name] = usa_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        usa_df[column_name] = pd.to_numeric(usa_df[column_name], errors='coerce')\n",
    "s.store_objects(usa_df=usa_df)\n",
    "#columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "#usa_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, (636, 10)), (10, (69, 7)), (6, (29, 5)), (12, (65, 2)), (5, (13, 8)), (3, (25, 4)), (4, (15, 4)), (14, (22, 2)), (15, (21, 2)), (8, (13, 3)), (29, (12, 3)), (1, (12, 2)), (24, (10, 2)), (25, (9, 2)), (27, (8, 2)), (28, (7, 2)), (7, (5, 2)), (16, (5, 2)), (17, (5, 2)), (30, (5, 2)), (20, (4, 2)), (26, (4, 2)), (18, (3, 2)), (21, (3, 2)), (22, (3, 2)), (23, (2, 2)), (0, (1, 2)), (9, (1, 2)), (11, (1, 2)), (13, (1, 2)), (19, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_the_Philippines'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\philippines_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "philippines_df = tables_list[6].copy()\n",
    "philippines_df.columns = philippines_df.iloc[0].tolist()\n",
    "columns_list = [1, 3]\n",
    "philippines_df = philippines_df.iloc[:, columns_list]\n",
    "philippines_df.columns = ['Date', 'Confirmed_New']\n",
    "#philippines_df.Date.to_dict()\n",
    "idx_list = [1] + list(range(3, 6)) + [7] + list(range(9, 28))\n",
    "match_series = philippines_df.index.isin(idx_list)\n",
    "philippines_df = philippines_df[match_series]\n",
    "for column_name in philippines_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        philippines_df[column_name] = pd.to_datetime(philippines_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        philippines_df[column_name] = philippines_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        philippines_df[column_name] = pd.to_numeric(philippines_df[column_name], errors='coerce')\n",
    "s.store_objects(philippines_df=philippines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, (342, 12)), (3, (65, 5)), (6, (19, 7)), (7, (65, 2)), (9, (22, 2)), (10, (21, 2)), (1, (13, 2)), (19, (10, 2)), (20, (9, 2)), (22, (8, 2)), (23, (7, 2)), (4, (2, 6)), (11, (5, 2)), (12, (5, 2)), (15, (4, 2)), (21, (4, 2)), (13, (3, 2)), (16, (3, 2)), (17, (3, 2)), (18, (2, 2)), (0, (1, 2)), (2, (1, 2)), (8, (1, 2)), (14, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Thailand'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\thailand_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "thailand_df = tables_list[3].copy()\n",
    "thailand_df.columns = thailand_df.iloc[0].tolist()\n",
    "columns_list = [1, 3]\n",
    "thailand_df = thailand_df.iloc[:, columns_list]\n",
    "thailand_df.columns = ['Date', 'Confirmed_New']\n",
    "#thailand_df.Date.to_dict()\n",
    "idx_list = [1, 3,] + list(range(5, 20)) + list(range(21, 26)) + list(range(27, 29)) + list(range(30, 42)) + list(range(43, 64))\n",
    "match_series = thailand_df.index.isin(idx_list)\n",
    "thailand_df = thailand_df[match_series]\n",
    "for column_name in thailand_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        thailand_df[column_name] = pd.to_datetime(thailand_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        thailand_df[column_name] = thailand_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        thailand_df[column_name] = pd.to_numeric(thailand_df[column_name], errors='coerce')\n",
    "s.store_objects(thailand_df=thailand_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, (121, 12)), (2, (47, 5)), (5, (65, 2)), (3, (21, 3)), (7, (22, 2)), (8, (21, 2)), (1, (11, 2)), (17, (10, 2)), (18, (9, 2)), (20, (8, 2)), (21, (7, 2)), (9, (5, 2)), (10, (5, 2)), (13, (4, 2)), (19, (4, 2)), (11, (3, 2)), (14, (3, 2)), (15, (3, 2)), (16, (2, 2)), (0, (1, 2)), (6, (1, 2)), (12, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Vietnam'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\vietnam_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vietnam_df = tables_list[2].copy()\n",
    "vietnam_df.columns = vietnam_df.iloc[0].tolist()\n",
    "columns_list = [1, 3]\n",
    "vietnam_df = vietnam_df.iloc[:, columns_list]\n",
    "vietnam_df.columns = ['Date', 'Confirmed_New']\n",
    "#vietnam_df.Date.to_dict()\n",
    "idx_list = [1, 3,] + list(range(5, 19)) + list(range(20, 23)) + [24] + list(range(26, 46))\n",
    "match_series = vietnam_df.index.isin(idx_list)\n",
    "vietnam_df = vietnam_df[match_series]\n",
    "for column_name in vietnam_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        vietnam_df[column_name] = pd.to_datetime(vietnam_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        vietnam_df[column_name] = vietnam_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        vietnam_df[column_name] = pd.to_numeric(vietnam_df[column_name], errors='coerce')\n",
    "s.store_objects(vietnam_df=vietnam_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, (120, 8)), (6, (113, 5)), (7, (46, 7)), (3, (56, 5)), (5, (13, 18)), (9, (20, 9)), (12, (65, 2)), (14, (22, 2)), (15, (21, 2)), (2, (16, 2)), (4, (6, 5)), (24, (10, 2)), (25, (9, 2)), (11, (4, 4)), (27, (8, 2)), (10, (3, 5)), (28, (7, 2)), (16, (5, 2)), (17, (5, 2)), (20, (4, 2)), (26, (4, 2)), (18, (3, 2)), (21, (3, 2)), (22, (3, 2)), (23, (2, 2)), (0, (1, 2)), (1, (1, 2)), (13, (1, 2)), (19, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Malaysia'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\malaysia_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "malaysia_df = tables_list[7].copy()\n",
    "malaysia_df.columns = ['Date', 'Selangor', 'Johor', 'Kuala_Lumpur', 'Kedah', 'North_Sembilan', 'Confirmed_New']\n",
    "malaysia_df = malaysia_df.loc[:43]\n",
    "date_format = '%d %B, %Y'\n",
    "for column_name in malaysia_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        malaysia_df[column_name] = malaysia_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        malaysia_df[column_name] = pd.to_datetime(malaysia_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        malaysia_df[column_name] = malaysia_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        malaysia_df[column_name] = pd.to_numeric(malaysia_df[column_name], errors='coerce')\n",
    "s.store_objects(malaysia_df=malaysia_df)\n",
    "#columns_list = ['Date', 'Confirmed_New']\n",
    "#malaysia_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (34, 19)), (1, (65, 2)), (3, (22, 2)), (4, (21, 2)), (13, (10, 2)), (14, (9, 2)), (16, (8, 2)), (17, (7, 2)), (5, (5, 2)), (6, (5, 2)), (9, (4, 2)), (15, (4, 2)), (7, (3, 2)), (10, (3, 2)), (11, (3, 2)), (12, (2, 2)), (2, (1, 2)), (8, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/United_Kingdom_medical_cases'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\uk_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "uk_df = tables_list[0].copy()\n",
    "uk_df.columns = ['Date', 'England_East', 'England_London', 'England_Midlands', 'England_NE_Yorks', 'England_North_West',\n",
    "                 'England_South_East', 'England_South_West', 'England_Unclassified', 'Northern_Ireland', 'Scotland', 'Wales',\n",
    "                 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative', 'Tested_New', 'Tested_Cumulative',\n",
    "                 'Sources']\n",
    "uk_df = uk_df.loc[:31]\n",
    "#date_format = '%Y/%M/%d'\n",
    "for column_name in uk_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        #uk_df[column_name] = uk_df[column_name].map(lambda x: datetime.strptime('{}'.format(x), date_format))\n",
    "        uk_df[column_name] = pd.to_datetime(uk_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        uk_df[column_name] = uk_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        uk_df[column_name] = pd.to_numeric(uk_df[column_name], errors='coerce')\n",
    "s.store_objects(uk_df=uk_df)\n",
    "#columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "#uk_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, (37, 30)), (0, (36, 11)), (3, (29, 10)), (2, (12, 11)), (8, (62, 2)), (1, (20, 6)), (4, (12, 10)), (5, (12, 10)), (10, (19, 2)), (11, (18, 2)), (20, (10, 2)), (21, (9, 2)), (23, (8, 2)), (24, (7, 2)), (13, (5, 2)), (16, (4, 2)), (22, (4, 2)), (14, (3, 2)), (17, (3, 2)), (18, (3, 2)), (12, (2, 2)), (19, (2, 2)), (7, (1, 3)), (9, (1, 2)), (15, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/Spain_medical_cases'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\spain_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spain_df = tables_list[6].copy()\n",
    "districts_list = ['AN', 'AR', 'AS', 'IB', 'CN', 'CB', 'CM', 'CL', 'CT', 'CE', 'VC', 'EX', 'GA', 'MD', 'ML', 'MU', 'NA',\n",
    "                  'PV', 'RI']\n",
    "summaries_list = ['Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative', 'ICU_Cumulative',\n",
    "                  'Recovered_Cumulative', 'Tested_Cumulative']\n",
    "spain_df.columns = ['Date', 'Time'] + districts_list + summaries_list + ['Refs', 'Notes']\n",
    "spain_df = spain_df.loc[:30]\n",
    "for column_name in spain_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        spain_df[column_name] = pd.to_datetime(spain_df[column_name])\n",
    "        pass\n",
    "    elif column_name == 'Time':\n",
    "        spain_df[column_name] = pd.to_timedelta(spain_df[column_name]+':00')\n",
    "        pass\n",
    "    else:\n",
    "        spain_df[column_name] = spain_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        spain_df[column_name] = pd.to_numeric(spain_df[column_name], errors='coerce')\n",
    "df_columns_list = districts_list + summaries_list\n",
    "df = pd.DataFrame([], columns=df_columns_list)\n",
    "summed_list = districts_list + ['Confirmed_New', 'Deaths_New']\n",
    "for column_name in summed_list:\n",
    "    new_group = spain_df.groupby('Date')[column_name]\n",
    "    df[column_name] = new_group.agg(np.sum)\n",
    "for column_name in ['Confirmed_Cumulative', 'Deaths_Cumulative', 'ICU_Cumulative', 'Recovered_Cumulative', 'Tested_Cumulative']:\n",
    "    cumulative_group = spain_df.groupby('Date')[column_name]\n",
    "    df[column_name] = cumulative_group.agg(max)\n",
    "df.reset_index(level=0, inplace=True)\n",
    "s.store_objects(spain_df=df)\n",
    "#columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "#df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, (75, 28)), (2, (30, 13)), (3, (61, 5)), (5, (20, 7)), (7, (20, 7)), (9, (62, 2)), (4, (13, 7)), (6, (8, 5)), (11, (19, 2)), (12, (18, 2)), (1, (14, 2)), (21, (10, 2)), (22, (9, 2)), (24, (8, 2)), (25, (7, 2)), (14, (5, 2)), (17, (4, 2)), (23, (4, 2)), (15, (3, 2)), (18, (3, 2)), (19, (3, 2)), (13, (2, 2)), (20, (2, 2)), (0, (1, 2)), (10, (1, 2)), (16, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_South_Korea'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\south_korea_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "south_korea_df = tables_list[8].copy()\n",
    "districts_list = ['Gyeonggi_Incheon', 'Gyeonggi_Seoul', 'Gyeonggi_Gyeonggi', 'Gangwon_Gangwon',\n",
    "                  'Gyeongsang_Gyeongbuk', 'Gyeongsang_Daegu', 'Gyeongsang_Gyeongnam', 'Gyeongsang_Busan',\n",
    "                  'Gyeongsang_Ulsan', 'Chungcheong_Chungbuk', 'Chungcheong_Sejong', 'Chungcheong_Daejeon',\n",
    "                  'Chungcheong_Chungnam', 'Jeolla_Jeonbuk', 'Jeolla_Gwangju', 'Jeolla_Jeonnam',\n",
    "                  'Jeolla_Jeju', 'Airport Screening']\n",
    "summaries_list = ['Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative',\n",
    "                  'Tested_Cumulative', 'Tested_New', 'Discharged_Cumulative']\n",
    "south_korea_df.columns = ['Date', 'Time'] + districts_list + summaries_list + ['Sources']\n",
    "#south_korea_df.Date.to_dict()\n",
    "idx_list = list(range(18)) + list(range(19, 42)) + list(range(43, 69))\n",
    "match_series = south_korea_df.index.isin(idx_list)\n",
    "south_korea_df = south_korea_df[match_series]\n",
    "for column_name in south_korea_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        south_korea_df[column_name] = pd.to_datetime(south_korea_df[column_name])\n",
    "        pass\n",
    "    elif column_name == 'Time':\n",
    "        south_korea_df[column_name] = pd.to_timedelta(south_korea_df[column_name]+':00')\n",
    "        pass\n",
    "    else:\n",
    "        south_korea_df[column_name] = south_korea_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        south_korea_df[column_name] = pd.to_numeric(south_korea_df[column_name], errors='coerce')\n",
    "df_columns_list = districts_list + summaries_list\n",
    "df = pd.DataFrame([], columns=df_columns_list)\n",
    "summed_list = districts_list + ['Confirmed_New', 'Deaths_New', 'Tested_New']\n",
    "for column_name in summed_list:\n",
    "    new_group = south_korea_df.groupby('Date')[column_name]\n",
    "    df[column_name] = new_group.agg(np.sum)\n",
    "for column_name in ['Confirmed_Cumulative', 'Deaths_Cumulative', 'Tested_Cumulative', 'Discharged_Cumulative']:\n",
    "    cumulative_group = south_korea_df.groupby('Date')[column_name]\n",
    "    df[column_name] = cumulative_group.agg(max)\n",
    "df.reset_index(level=0, inplace=True)\n",
    "s.store_objects(south_korea_df=df)\n",
    "#columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative']\n",
    "#df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, (62, 50)), (3, (55, 7)), (2, (70, 5)), (5, (62, 2)), (7, (19, 2)), (8, (18, 2)), (1, (10, 2)), (17, (10, 2)), (18, (9, 2)), (20, (8, 2)), (21, (7, 2)), (10, (5, 2)), (13, (4, 2)), (19, (4, 2)), (11, (3, 2)), (14, (3, 2)), (15, (3, 2)), (9, (2, 2)), (16, (2, 2)), (0, (1, 2)), (6, (1, 2)), (12, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Japan'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\japan_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "japan_df = tables_list[4].copy()\n",
    "japan_df.columns = ['Date', 'Hokkaidō', 'Honshū_Tōhoku_Aomori', 'Honshū_Tōhoku_Akita',\n",
    "                    'Honshū_Tōhoku_Miyagi', 'Honshū_Tōhoku_Fukushima', 'Honshū_Kantō_Tochigi', 'Honshū_Kantō_Ibaraki',\n",
    "                    'Honshū_Kantō_Chiba', 'Honshū_Kantō_Tōkyō', 'Honshū_Kantō_Kanagawa', 'Honshū_Kantō_Saitama',\n",
    "                    'Honshū_Kantō_Gunma', 'Honshū_Chūbu_Niigata', 'Honshū_Chūbu_Nagano', 'Honshū_Chūbu_Yamanashi',\n",
    "                    'Honshū_Chūbu_Shizuoka', 'Honshū_Chūbu_Aichi', 'Honshū_Chūbu_Ishikawa', 'Honshū_Chūbu_Fukui',\n",
    "                    'Honshū_Chūbu_Gifu', 'Honshū_Kansai_Mie', 'Honshū_Kansai_Shiga', 'Honshū_Kansai_Wakayama', 'Honshū_Kansai_Nara',\n",
    "                    'Honshū_Kansai_Kyōto', 'Honshū_Kansai_Ōsaka', 'Honshū_Kansai_Hyōgo', 'Honshū_Chūgoku_Okayama',\n",
    "                    'Honshū_Chūgoku_Hiroshima', 'Honshū_Chūgoku_Yamaguchi', 'Shikoku_Shikoku_Kagawa',\n",
    "                    'Shikoku_Shikoku_Tokushima', 'Shikoku_Shikoku_Kōchi', 'Shikoku_Shikoku_Ehime', 'Kyūshū_Kyūshū_Ōita',\n",
    "                    'Kyūshū_Kyūshū_Fukuoka', 'Kyūshū_Kyūshū_Saga', 'Kyūshū_Kyūshū_Nagasaki', 'Kyūshū_Kyūshū_Kumamoto',\n",
    "                    'Kyūshū_Kyūshū_Miyazaki', 'Okinawa', 'Airport', 'Diamond_Princess_Workers', 'Repatriated_Total',\n",
    "                    'Abroad_Total', 'Confirmed_New', 'Confirmed_Total', 'Tested_Total', 'Sources']\n",
    "#japan_df.Date.to_dict()\n",
    "idx_list = list(range(14)) + list(range(15, 32)) + list(range(33, 56))\n",
    "match_series = japan_df.index.isin(idx_list)\n",
    "japan_df = japan_df[match_series]\n",
    "#date_format = '%Y/%M/%d'\n",
    "for column_name in japan_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        #japan_df[column_name] = japan_df[column_name].map(lambda x: datetime.strptime('{}'.format(x), date_format))\n",
    "        japan_df[column_name] = pd.to_datetime(japan_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        japan_df[column_name] = japan_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        japan_df[column_name] = pd.to_numeric(japan_df[column_name], errors='coerce')\n",
    "s.store_objects(japan_df=japan_df)\n",
    "#columns_list = ['Date', 'Confirmed_Total', 'Confirmed_New']\n",
    "#japan_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\italy_df.pickle\n",
      "Saving to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\csv\\italy_df.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/Italy_medical_cases'\n",
    "xlsx_dir = os.path.join(s.saves_folder, 'xlsx')\n",
    "file_path = os.path.join(xlsx_dir, 'Daily_COVID-19_cases_in_Italy_by_region.xlsx')\n",
    "italy_df = pd.read_excel(file_path)\n",
    "italy_df.columns = ['Date', 'North_West_VDA', 'North_West_LIG', 'North_West_PIE', 'North_West_LOM', 'North_East_VEN',\n",
    "                    'North_East_TN', 'North_East_BZ', 'North_East_FVG', 'North_East_EMR', 'Centre_MAR', 'Centre_TOS', 'Centre_UMB',\n",
    "                    'Centre_LAZ', 'South_ABR', 'South_MOL', 'South_CAM', 'South_BAS', 'South_PUG', 'South_CAL', 'Islands_SIC',\n",
    "                    'Islands_SAR', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative', 'Active_ICU', 'Active_Cumulative',\n",
    "                    'Recovered_Cumulative', 'Tested_Cumulative', 'References', 'Notes']\n",
    "italy_df = italy_df.iloc[1:36]\n",
    "#date_format = '%Y/%M/%d'\n",
    "for column_name in italy_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        #italy_df[column_name] = italy_df[column_name].map(lambda x: datetime.strptime('{}'.format(x), date_format))\n",
    "        italy_df[column_name] = pd.to_datetime(italy_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        italy_df[column_name] = italy_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        italy_df[column_name] = pd.to_numeric(italy_df[column_name], errors='coerce')\n",
    "s.store_objects(italy_df=italy_df)\n",
    "s.save_dataframes(italy_df=italy_df)\n",
    "#columns_list = ['Date', 'Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative', 'Active_ICU', 'Active_Cumulative']\n",
    "#italy_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, (35, 37)), (4, (36, 6)), (3, (36, 5)), (8, (62, 2)), (10, (19, 2)), (11, (18, 2)), (1, (14, 2)), (20, (10, 2)), (21, (9, 2)), (23, (8, 2)), (24, (7, 2)), (13, (5, 2)), (16, (4, 2)), (22, (4, 2)), (14, (3, 2)), (17, (3, 2)), (18, (3, 2)), (5, (2, 2)), (12, (2, 2)), (19, (2, 2)), (0, (1, 2)), (2, (1, 2)), (7, (1, 2)), (9, (1, 2)), (15, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "page_tables_list = get_page_tables('https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Iran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\iran_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iran_df = page_tables_list[6].copy()\n",
    "iran_df.columns = ['Date', 'Region_1_Qom', 'Region_1_Teh', 'Region_1_Maz', 'Region_1_Alb', 'Region_1_Sem', 'Region_1_Gol',\n",
    "                   'Region_1_Qaz', 'Region_2_Esf', 'Region_2_Frs', 'Region_2_Hor', 'Region_2_Koh', 'Region_2_Cha', 'Region_2_Bus',\n",
    "                   'Region_3_Gil', 'Region_3_Ard', 'Region_3_Azs', 'Region_3_Azg', 'Region_3_Kur', 'Region_3_Zan', 'Region_4_Mar',\n",
    "                   'Region_4_Ham', 'Region_4_Khz', 'Region_4_Krs', 'Region_4_Lor', 'Region_4_Ilm', 'Region_5_Khr', 'Region_5_Sis',\n",
    "                   'Region_5_Yaz', 'Region_5_Khs', 'Region_5_Ker', 'Region_5_Khn', 'Confirmed_New', 'Confirmed_Total',\n",
    "                   'Deaths_New', 'Deaths_Total', 'Sources']\n",
    "iran_df = iran_df.loc[:32]\n",
    "#date_format = '%Y/%M/%d'\n",
    "for column_name in iran_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        #iran_df[column_name] = iran_df[column_name].map(lambda x: datetime.strptime('{}'.format(x), date_format))\n",
    "        iran_df[column_name] = pd.to_datetime(iran_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        iran_df[column_name] = iran_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        iran_df[column_name] = pd.to_numeric(iran_df[column_name], errors='coerce')\n",
    "s.store_objects(iran_df=iran_df)\n",
    "#columns_list = ['Date', 'Confirmed_Total', 'Deaths_Total', 'Confirmed_New', 'Deaths_New']\n",
    "#iran_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\slovakia_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "slovakia_df = s.load_object('slovakia_df')\n",
    "slovakia_df.columns = ['Date', 'Bratislava', 'Žilina', 'Košice', 'Trnava', 'Trenčín', 'Prešov', 'Banská_Bystrica',\n",
    "                       'Nitra', 'Confirmed_New', 'Confirmed_Total', 'Deaths_New', 'Deaths_Total',\n",
    "                       'Recoveries_Total', 'Tested_Total', 'References']\n",
    "slovakia_df = slovakia_df.loc[:17]\n",
    "for column_name in slovakia_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        slovakia_df[column_name] = pd.to_datetime(slovakia_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        slovakia_df[column_name] = slovakia_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        slovakia_df[column_name] = pd.to_numeric(slovakia_df[column_name], errors='coerce')\n",
    "s.store_objects(slovakia_df=slovakia_df)\n",
    "#columns_list = ['Date', 'Confirmed_New', 'Confirmed_Total', 'Deaths_New', 'Deaths_Total']\n",
    "#slovakia_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\singapore_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "singapore_df = s.load_object('singapore_df')\n",
    "singapore_df.columns = singapore_df.iloc[0].tolist()\n",
    "singapore_df = singapore_df.loc[1:61]\n",
    "columns_list = [1, 3, 4]\n",
    "singapore_df = singapore_df.iloc[:, columns_list]\n",
    "singapore_df.columns = ['Date', 'Confirmed_New', 'Deaths_New']\n",
    "idx_list = list(range(11)) + list(range(12, 62))\n",
    "match_series = singapore_df.index.isin(idx_list)\n",
    "singapore_df = singapore_df[match_series]\n",
    "for column_name in singapore_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        singapore_df[column_name] = pd.to_datetime(singapore_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        singapore_df[column_name] = singapore_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        singapore_df[column_name] = pd.to_numeric(singapore_df[column_name], errors='coerce')\n",
    "s.store_objects(singapore_df=singapore_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\saves\\pickle\\netherlands_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "netherlands_df = s.load_object('netherlands_df')\n",
    "netherlands_df.columns = netherlands_df.iloc[0].tolist()\n",
    "netherlands_df = netherlands_df.loc[1:27]\n",
    "columns_list = [1, 3, 4]\n",
    "netherlands_df = netherlands_df.iloc[:, columns_list]\n",
    "netherlands_df.columns = ['Date', 'Confirmed_New', 'Deaths_New']\n",
    "for column_name in netherlands_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        netherlands_df[column_name] = pd.to_datetime(netherlands_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        netherlands_df[column_name] = netherlands_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "        netherlands_df[column_name] = pd.to_numeric(netherlands_df[column_name], errors='coerce')\n",
    "s.store_objects(netherlands_df=netherlands_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hong_kong_df = s.load_object('hong_kong_df')\n",
    "hong_kong_df.columns = hong_kong_df.iloc[0].tolist()\n",
    "hong_kong_df = hong_kong_df.loc[1:62]\n",
    "columns_list = [1, 4]\n",
    "hong_kong_df = hong_kong_df.iloc[:, columns_list]\n",
    "hong_kong_df.columns = ['Date', 'Confirmed_New']\n",
    "hong_kong_df.Confirmed_New = hong_kong_df.Confirmed_New.map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "hong_kong_df.Confirmed_New = pd.to_numeric(hong_kong_df.Confirmed_New, errors='coerce')\n",
    "hong_kong_df.Date = pd.to_datetime(hong_kong_df.Date)\n",
    "s.store_objects(hong_kong_df=hong_kong_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "germany_df = s.load_object('germany_df')\n",
    "germany_df.columns = ['Date', 'Baden-Württemberg', 'Bavaria', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg', 'Hesse',\n",
    "                      'Mecklenburg-Vorpommern', 'Lower Saxony', 'North Rhine-Westphalia', 'Rhineland-Palatinate', 'Saarland',\n",
    "                      'Saxony', 'Saxony-Anhalt', 'Schleswig-Holstein', 'Thuringia', 'Confirmed_Repatriated', 'Confirmed_Total',\n",
    "                      'Deaths_Total', 'Confirmed_New', 'Deaths_New']\n",
    "date_format = '%d %b, %Y'\n",
    "for column_name in germany_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        germany_df[column_name] = germany_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        germany_df[column_name] = pd.to_datetime(germany_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        germany_df[column_name] = germany_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        germany_df[column_name] = pd.to_numeric(germany_df[column_name], errors='coerce')\n",
    "s.store_objects(germany_df=germany_df)\n",
    "#columns_list = ['Date', 'Confirmed_Repatriated', 'Confirmed_Total', 'Deaths_Total', 'Confirmed_New', 'Deaths_New']\n",
    "#germany_df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "czech_df = s.load_object('czech_df')\n",
    "czech_df.columns = ['Date', 'Confirmed_New', 'Confirmed_Total', 'Recovered_New', 'Recovered_Total', 'Deaths_New', 'Deaths_Total',\n",
    "                    'Tested_Total']\n",
    "date_format = '%d %B, %Y'\n",
    "for column_name in czech_df.columns:\n",
    "    if column_name == 'Date':\n",
    "        czech_df[column_name] = czech_df[column_name].map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "        czech_df[column_name] = pd.to_datetime(czech_df[column_name])\n",
    "        pass\n",
    "    else:\n",
    "        czech_df[column_name] = czech_df[column_name].map(lambda x: re.sub('[^\\d]+', '', str(x)))\n",
    "        czech_df[column_name] = pd.to_numeric(czech_df[column_name], errors='coerce')\n",
    "s.store_objects(czech_df=czech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "china_df = s.load_object('china_df')\n",
    "china_df.columns = china_df.iloc[0].tolist()\n",
    "china_df = china_df.loc[1:77]\n",
    "columns_list = [1, 4]\n",
    "china_df = china_df.iloc[:, columns_list]\n",
    "china_df.columns = ['Date', 'Confirmed_New']\n",
    "china_df.Confirmed_New = china_df.Confirmed_New.map(lambda x: re.sub('[^\\d]+', '', str(x).split('(')[0]))\n",
    "china_df.Confirmed_New = pd.to_numeric(china_df.Confirmed_New, errors='coerce')\n",
    "china_df.Date = pd.to_datetime(china_df.Date)\n",
    "s.store_objects(china_df=china_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page_title = '2019–20 coronavirus pandemic'\n",
    "page_obj = wikipedia.page(title=page_title)\n",
    "print(['page_obj.{}'.format(fn) for fn in dir(page_obj) if 'link' in fn.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(['page_obj.{}'.format(fn) for fn in dir(page_obj) if 'url' in fn.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "countries_list = ['Italy']\n",
    "locations_list = [link for link in set(page_obj.links) if ' in ' in link]\n",
    "for page_title in locations_list:\n",
    "    if any(map(lambda x: x in page_title, countries_list)):\n",
    "        page_obj = wikipedia.page(title=page_title)\n",
    "        tables_url = page_obj.url\n",
    "        tables_list = get_page_tables(tables_url)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(page_title)\n",
    "tables_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "locations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df = s.load_object('pandemic_df')\n",
    "pandemic_df.loc[12, 'R0_low'] = 2\n",
    "pandemic_df.loc[12, 'R0_high'] = 6\n",
    "s.store_objects(pandemic_df=pandemic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df.set_index('Disease', drop=True, inplace=True)\n",
    "s.store_objects(pandemic_df=pandemic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_name = 'COVID-19'\n",
    "csv_path = os.path.abspath(os.path.join(s.data_csv_folder, '{}.csv'.format(csv_name)))\n",
    "covid19_df = pd.read_csv(csv_path, encoding=s.encoding_type, header=[0, 1])\n",
    "covid19_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_regex = re.compile(r'[^0-9A-Za-z]+')\n",
    "[name_regex.sub('_', '{} {}'.format(cn_tuple[0], cn_tuple[1]).lower()) for cn_tuple in covid19_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covid19_df.columns = ['Disease', 'deaths_per_day_global_rounded', 'total_news_mentions_millions_data_retrieved_3rd_mar_2020',\n",
    "                      'news_mentions_per_death', 'CFR_low', 'R0_low', 'annual_global_fatalities_latest_data_year',\n",
    "                      'new_cases_per_year_latest_data_year', 'days_of_outbreak', 'fatalty_notes', 'source', 'url']\n",
    "covid19_df.set_index('Disease', drop=True, inplace=True)\n",
    "s.store_objects(covid19_df=covid19_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df = s.load_object('pandemic_df')\n",
    "pandemic_df.index = ['Pertussis', 'Diphtheria', 'Measles', 'Smallpox', 'Rubella', 'Mumps', 'SARS', 'MERS', 'Swine flu (A/H1N1)',\n",
    "                     'Seasonal Flu', 'Hong Kong flu (1968 pandemic)', 'Spanish flu (1918 pandemic)', 'COVID-19 (Wuhan Coronavirus)']\n",
    "s.store_objects(pandemic_df=pandemic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s.store_objects(pandemic_df=pandemic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df = s.load_object('pandemic_df')\n",
    "covid19_df = s.load_object('covid19_df')\n",
    "pandemic_df = pd.merge(left=pandemic_df, right=covid19_df, how='outer', left_index=True, right_index=True, suffixes=('_old', '_new'))\n",
    "pandemic_df.columns = ['Transmission', 'R0', 'Case_fatality_ratio', 'Hospitalized_cases_sent_to_ICU', 'Asymptomatic_transmission',\n",
    "                       'Year_vaccine_available', 'R0_high', 'R0_low', 'CFR_high', 'CFR_low', 'deaths_per_day_global_rounded',\n",
    "                       'total_news_mentions_millions_data_retrieved_3rd_mar_2020', 'news_mentions_per_death', 'CFR_new',\n",
    "                       'R0_new', 'annual_global_fatalities_latest_data_year', 'new_cases_per_year_latest_data_year',\n",
    "                       'days_of_outbreak', 'fatalty_notes', 'source', 'url']\n",
    "columns_list = sorted([cn for cn in pandemic_df.columns if ('_low' in cn) or ('_high' in cn) or ('_new' in cn)])\n",
    "num_regex = re.compile(r'[^0-9.]+')\n",
    "for disease_name, row_series in pandemic_df[columns_list].iterrows():\n",
    "    CFR_high = row_series['CFR_high']\n",
    "    CFR_low = row_series['CFR_low']\n",
    "    CFR_new = row_series['CFR_new']\n",
    "    if str(CFR_new) != 'nan':\n",
    "        CFR_new = float(num_regex.sub('', str(CFR_new)))\n",
    "    if str(CFR_high) == 'nan':\n",
    "        pandemic_df.loc[disease_name, 'CFR_high'] = CFR_new\n",
    "    if str(CFR_low) == 'nan':\n",
    "        pandemic_df.loc[disease_name, 'CFR_low'] = CFR_new\n",
    "    if str(CFR_new) != 'nan':\n",
    "        if CFR_new < CFR_low:\n",
    "            pandemic_df.loc[disease_name, 'CFR_low'] = CFR_new\n",
    "        elif CFR_new > CFR_high:\n",
    "            pandemic_df.loc[disease_name, 'CFR_high'] = CFR_new\n",
    "    R0_high = row_series['R0_high']\n",
    "    R0_low = row_series['R0_low']\n",
    "    R0_new = row_series['R0_new']\n",
    "    if str(R0_new) != 'nan':\n",
    "        R0_new = float(num_regex.sub('', str(R0_new)))\n",
    "    if str(R0_high) == 'nan':\n",
    "        pandemic_df.loc[disease_name, 'R0_high'] = R0_new\n",
    "    if str(R0_low) == 'nan':\n",
    "        pandemic_df.loc[disease_name, 'R0_low'] = R0_new\n",
    "    if str(R0_new) != 'nan':\n",
    "        if R0_new < R0_low:\n",
    "            pandemic_df.loc[disease_name, 'R0_low'] = R0_new\n",
    "        elif R0_new > R0_high:\n",
    "            pandemic_df.loc[disease_name, 'R0_high'] = R0_new\n",
    "columns_list = ['Transmission', 'R0', 'Case_fatality_ratio', 'Hospitalized_cases_sent_to_ICU', 'Asymptomatic_transmission',\n",
    "                'Year_vaccine_available', 'R0_high', 'R0_low', 'CFR_high', 'CFR_low', 'deaths_per_day_global_rounded',\n",
    "                'total_news_mentions_millions_data_retrieved_3rd_mar_2020', 'news_mentions_per_death',\n",
    "                'annual_global_fatalities_latest_data_year', 'new_cases_per_year_latest_data_year',\n",
    "                'days_of_outbreak', 'fatalty_notes', 'source', 'url']\n",
    "pandemic_df = pandemic_df[columns_list]\n",
    "pandemic_df.sample(5).T.sample(6).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covid19_df = s.load_object('covid19_df')\n",
    "covid19_df.index = ['Tuberculosis', 'Hepatitis B', 'Pneumonia', 'HIV ', 'Malaria ', 'Shigellosis', 'Rotavirus', 'Seasonal Flu',\n",
    "                    'Norovirus', 'Whooping Cough', 'Typhoid', 'Cholera', 'Meningitis', 'Measles', 'Rabies ', 'Yellow Fever',\n",
    "                    'Leishmaniasis', 'Echinococcosis', 'COVID-19 (Wuhan Coronavirus)', 'Dengue Fever', 'Hepatitis A',\n",
    "                    'Chicken Pox', 'Sleeping Sickness', 'Ebola', 'SARS', 'MERS']\n",
    "s.store_objects(covid19_df=covid19_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandemic_df = s.load_object('pandemic_df')\n",
    "covid19_df = s.load_object('covid19_df')\n",
    "check_for_typos(left_list=covid19_df.index.tolist(), right_list=pandemic_df.index.tolist(), verbose=False).sort_values('left_item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_page_soup(page_url):\n",
    "    with urllib.request.urlopen(page_url) as response:\n",
    "        page_html = response.read()\n",
    "    page_soup = bs(page_html, 'html.parser')\n",
    "    \n",
    "    return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables_url = 'https://www.zorinaq.com/pub/ncov-comparison.html'\n",
    "ncov_path = os.path.join(s.data_folder, 'html', 'ncov-comparison.html')\n",
    "#page_soup = get_page_soup(tables_url)\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "pandemic_df = tables_list[0].copy()\n",
    "pandemic_df.columns = ['Disease', 'Transmission', 'R0', 'Case_fatality_ratio', 'Hospitalized_cases_sent_to_ICU',\n",
    "                       'Asymptomatic_transmission', 'Year_vaccine_available']\n",
    "\n",
    "def f(x):\n",
    "    result = re.sub(r' *\\(', ' (', str(x))\n",
    "    \n",
    "    return result\n",
    "\n",
    "pandemic_df.Disease = pandemic_df.Disease.map(f)\n",
    "\n",
    "def f(x):\n",
    "    result = re.sub(r'⁹', '', str(x))\n",
    "    \n",
    "    return result\n",
    "\n",
    "pandemic_df.Transmission = pandemic_df.Transmission.map(f)\n",
    "pandemic_df['R0_high'] = np.nan\n",
    "\n",
    "def f(x):\n",
    "    result = str(x).split('-')[-1]\n",
    "    result = re.sub(r'[^0-9.]+', '', result)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "pandemic_df['R0_high'] = pandemic_df.R0.map(f)\n",
    "pandemic_df['R0_low'] = np.nan\n",
    "\n",
    "def f(x):\n",
    "    result_list = str(x).split('-')\n",
    "    if len(result_list) < 3:\n",
    "        result = result_list[0]\n",
    "    else:\n",
    "        result = result_list[1].split(' ')[-1]\n",
    "    result = re.sub(r'[^0-9.]+', '', result)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "pandemic_df['R0_low'] = pandemic_df.R0.map(f)\n",
    "pandemic_df['CFR_high'] = np.nan\n",
    "\n",
    "def f(x):\n",
    "    result = str(x).split('-')[-1]\n",
    "    result = re.sub(r'[^0-9.]+', '', result)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "pandemic_df['CFR_high'] = pandemic_df.Case_fatality_ratio.map(f)\n",
    "pandemic_df['CFR_low'] = np.nan\n",
    "\n",
    "def f(x):\n",
    "    result_list = str(x).split('-')\n",
    "    result = result_list[0]\n",
    "    result = re.sub(r'[^0-9.]+', '', result)\n",
    "    \n",
    "    return float(result)\n",
    "\n",
    "pandemic_df['CFR_low'] = pandemic_df.Case_fatality_ratio.map(f)\n",
    "#print(pandemic_df.R0.unique().tolist())\n",
    "#print(pandemic_df.R0.map(f).unique().tolist())\n",
    "s.store_objects(pandemic_df=pandemic_df)\n",
    "pandemic_df.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (43, 61)), (1, (30, 56)), (2, (66, 2)), (4, (22, 2)), (5, (21, 2)), (14, (11, 2)), (15, (10, 2)), (17, (8, 2)), (18, (7, 2)), (6, (5, 2)), (7, (5, 2)), (16, (5, 2)), (10, (4, 2)), (8, (3, 2)), (11, (3, 2)), (12, (3, 2)), (13, (2, 2)), (3, (1, 2)), (9, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_url = 'https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/United_States_medical_cases'\n",
    "tables_list = get_page_tables(tables_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "us_deaths_df = tables_list[1].copy()\n",
    "us_deaths_df.columns = ['Date', 'West_AK', 'West_AZ', 'West_CA', 'West_CO', 'West_HI', 'West_ID', 'West_MT', 'West_NM', 'West_NV',\n",
    "                        'West_OR', 'West_UT', 'West_WA', 'West_WY', 'Midwest_IA', 'Midwest_IL', 'Midwest_IN', 'Midwest_KS',\n",
    "                        'Midwest_MI', 'Midwest_MN', 'Midwest_MO', 'Midwest_ND', 'Midwest_NE', 'Midwest_OH', 'Midwest_OK', 'Midwest_SD',\n",
    "                        'Midwest_WI', 'South_AL', 'South_AR', 'South_FL', 'South_GA', 'South_KY', 'South_LA', 'South_MS', 'South_NC',\n",
    "                        'South_SC', 'South_TN', 'South_TX', 'South_VA', 'Northeast_CT', 'Northeast_DC', 'Northeast_DE',\n",
    "                        'Northeast_MA', 'Northeast_MD', 'Northeast_ME', 'Northeast_NH', 'Northeast_NJ', 'Northeast_NY',\n",
    "                        'Northeast_PA', 'Northeast_RI', 'Northeast_VT', 'Deaths_New', 'Deaths_Cumulative']\n",
    "columns_list = ['Date', 'Deaths_New', 'Deaths_Cumulative']\n",
    "us_deaths_df = us_deaths_df[columns_list].loc[:21]\n",
    "date_format = '%b %d, %Y'\n",
    "us_deaths_df.Date = us_deaths_df.Date.map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "us_deaths_df.set_index('Date', drop=True, inplace=True)\n",
    "italy_df = s.load_object('italy_df')\n",
    "italy_df.set_index('Date', drop=True, inplace=True)\n",
    "columns_list = ['Deaths_New', 'Deaths_Cumulative']\n",
    "tracking_df = us_deaths_df[columns_list].merge(italy_df[columns_list], how='outer',\n",
    "                                               left_index=True, right_index=True, suffixes=('_usa', '_italy'))\n",
    "for column_name in tracking_df.columns:\n",
    "    tracking_df[column_name] = pd.to_numeric(tracking_df[column_name], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_format = '%b %d, %Y'\n",
    "usa_df.Date = usa_df.Date.map(lambda x: datetime.strptime('{}, 2020'.format(x), date_format))\n",
    "s.store_objects(usa_df=usa_df)\n",
    "s.save_dataframes(usa_df=usa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "usa_df = s.load_object('usa_df')\n",
    "usa_df.set_index('Date', drop=True, inplace=True)\n",
    "italy_df = s.load_object('italy_df')\n",
    "italy_df.set_index('Date', drop=True, inplace=True)\n",
    "columns_list = ['Confirmed_New', 'Confirmed_Cumulative', 'Deaths_New', 'Deaths_Cumulative']\n",
    "tracking_df = usa_df[columns_list].merge(italy_df[columns_list], how='outer',\n",
    "                                         left_index=True, right_index=True, suffixes=('_usa', '_italy'))\n",
    "s.store_objects(tracking_df=tracking_df)\n",
    "s.save_dataframes(include_index=True, tracking_df=tracking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "czech_df: False\n",
      "germany_df: False\n",
      "hong_kong_df: False\n",
      "iran_df: False\n",
      "italy_df: False\n",
      "japan_df: False\n",
      "netherlands_df: False\n",
      "singapore_df: False\n",
      "slovakia_df: False\n",
      "south_korea_df: False\n",
      "spain_df: False\n",
      "uk_df: False\n",
      "usa_df: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "china_df = s.load_object('china_df')\n",
    "china_df.Date = china_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "china_df.set_index('Date', drop=True, inplace=True)\n",
    "czech_df = s.load_object('czech_df')\n",
    "czech_df.Date = czech_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "czech_df.set_index('Date', drop=True, inplace=True)\n",
    "columns_list = list(set(china_df.columns).intersection(set(czech_df.columns)))\n",
    "merge_df = china_df[columns_list].merge(czech_df[columns_list], how='outer',\n",
    "                                        left_index=True, right_index=True,\n",
    "                                        suffixes=('_china', '_czech'))\n",
    "print('czech_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "germany_df = s.load_object('germany_df')\n",
    "germany_df.Date = germany_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "germany_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(germany_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_germany'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany']\n",
    "print('germany_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "hong_kong_df = s.load_object('hong_kong_df')\n",
    "hong_kong_df.Date = hong_kong_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "hong_kong_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(hong_kong_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_hong_kong'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong']\n",
    "print('hong_kong_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "iran_df = s.load_object('iran_df')\n",
    "iran_df.Date = iran_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "iran_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(iran_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_iran'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong', 'Confirmed_New_iran']\n",
    "print('iran_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "italy_df = s.load_object('italy_df')\n",
    "italy_df.Date = italy_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "italy_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(italy_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_italy'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy']\n",
    "print('italy_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "japan_df = s.load_object('japan_df')\n",
    "japan_df.Date = japan_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "japan_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(japan_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_japan'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan']\n",
    "print('japan_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "netherlands_df = s.load_object('netherlands_df')\n",
    "netherlands_df.Date = netherlands_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "netherlands_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(netherlands_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_netherlands'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands']\n",
    "print('netherlands_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "singapore_df = s.load_object('singapore_df')\n",
    "singapore_df.Date = singapore_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "singapore_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(singapore_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_singapore'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands', 'Confirmed_New_singapore']\n",
    "print('singapore_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "slovakia_df = s.load_object('slovakia_df')\n",
    "slovakia_df.Date = slovakia_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "slovakia_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(slovakia_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_slovakia'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands',\n",
    "                    'Confirmed_New_singapore', 'Confirmed_New_slovakia']\n",
    "print('slovakia_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "south_korea_df = s.load_object('south_korea_df')\n",
    "south_korea_df.Date = south_korea_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "south_korea_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(south_korea_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_south_korea'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands',\n",
    "                    'Confirmed_New_singapore', 'Confirmed_New_slovakia', 'Confirmed_New_south_korea']\n",
    "print('south_korea_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "spain_df = s.load_object('spain_df')\n",
    "spain_df.Date = spain_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "spain_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(spain_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_spain'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands',\n",
    "                    'Confirmed_New_singapore', 'Confirmed_New_slovakia', 'Confirmed_New_south_korea', 'Confirmed_New_spain']\n",
    "print('spain_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "uk_df = s.load_object('uk_df')\n",
    "uk_df.Date = uk_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "uk_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(uk_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_uk'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong',\n",
    "                    'Confirmed_New_iran', 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands',\n",
    "                    'Confirmed_New_singapore', 'Confirmed_New_slovakia', 'Confirmed_New_south_korea',\n",
    "                    'Confirmed_New_spain', 'Confirmed_New_uk']\n",
    "print('uk_df:', merge_df.index.has_duplicates)\n",
    "\n",
    "\n",
    "usa_df = s.load_object('usa_df')\n",
    "usa_df.Date = usa_df.Date.map(lambda ts: date(ts.year, ts.month, ts.day))\n",
    "usa_df.set_index('Date', drop=True, inplace=True)\n",
    "merge_df = merge_df.merge(usa_df[columns_list], how='outer', left_index=True,\n",
    "                          right_index=True, suffixes=('_merge', '_usa'))\n",
    "merge_df.columns = ['Confirmed_New_china', 'Confirmed_New_czech', 'Confirmed_New_germany', 'Confirmed_New_hong_kong', 'Confirmed_New_iran',\n",
    " 'Confirmed_New_italy', 'Confirmed_New_japan', 'Confirmed_New_netherlands', 'Confirmed_New_singapore',\n",
    " 'Confirmed_New_slovakia', 'Confirmed_New_south_korea', 'Confirmed_New_spain', 'Confirmed_New_uk', 'Confirmed_New_usa']\n",
    "print('usa_df:', merge_df.index.has_duplicates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
