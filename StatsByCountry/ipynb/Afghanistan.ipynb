{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\577342\\AppData\\Local\\Continuum\\anaconda3\\python.exe -m pip install --upgrade wikipedia\n",
      "Processing c:\\users\\577342\\appdata\\local\\pip\\cache\\wheels\\87\\2a\\18\\4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\\wikipedia-1.4.0-cp37-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in c:\\users\\577342\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wikipedia) (4.8.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.0.0 in c:\\users\\577342\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wikipedia) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>=1.2 in c:\\users\\577342\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.5)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\577342\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\577342\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\577342\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\577342\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "command_str = '{sys.executable} -m pip install --upgrade wikipedia'.format(sys=sys)\n",
    "print(command_str)\n",
    "!{command_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "D:\\Documents\\Repositories\\notebooks\\StatsByCountry\\ipynb\\Afghanistan.ipynb\n",
      "['c.axes_str', 'c.clean_up_state_common_dict', 'c.clean_up_state_merge_dataframe', 'c.clean_up_state_unique_dict', 'c.clean_up_suggestion_list_dict', 'c.column_description_dict', 'c.conjunctify_list', 'c.copy_file_path', 'c.create_label_line_file', 'c.create_state_first_dict', 'c.create_us_colored_labeled_map', 'c.create_us_colored_map', 'c.create_us_google_suggest_labeled_map', 'c.create_us_labeled_map', 'c.figure_str', 'c.fill_style_prefix', 'c.fill_style_str', 'c.get_colorbar_xml', 'c.get_column_description', 'c.get_google_suggestion_list', 'c.get_legend_xml', 'c.get_style_list', 'c.get_tfidf_lists', 'c.get_tspan_list', 'c.gradient_file_path', 'c.gradient_str', 'c.html_style_str', 'c.hyphen_dict', 'c.hyphenate_words', 'c.l_str', 'c.label_line_file_path', 'c.label_line_style_dict', 'c.label_line_style_list', 'c.label_str', 'c.label_text_style_list', 'c.label_tspan_style_list', 'c.line_height', 'c.matplotlib_axis_str', 'c.namedview_attributes_list', 'c.regex_sub_str', 'c.scrape_suggestion_list_dictionary', 'c.show_colorbar', 'c.state_common_dict', 'c.state_first_dict', 'c.state_merge_df', 'c.state_path_str', 'c.state_unique_dict', 'c.suggestion_list_dict', 'c.svg_attributes_list', 'c.svg_dir', 'c.svg_prefix_str', 'c.svg_regex', 'c.svg_suffix', 'c.t_str', 'c.text_style_dict', 'c.text_style_list', 'c.tick_text_str', 'c.tick_text_style_list', 'c.tick_tspan_style_list', 'c.ts_str', 'c.ytick_str']\n",
      "['u.add_an_episode_note', 'u.audio_xpath_list', 'u.bar_xpath', 'u.button_xpath', 'u.cancel_upload_window', 'u.click_another_upload_media_button', 'u.click_external_file_subtab', 'u.click_the_add_new_speaker_button', 'u.click_the_audio_apply_button', 'u.click_the_browse_button', 'u.click_the_close_signin_window_link', 'u.click_the_close_upload_window_link', 'u.click_the_copyright_checkbox', 'u.click_the_edit_button', 'u.click_the_edit_media_button', 'u.click_the_edit_overlay', 'u.click_the_first_link', 'u.click_the_login_button', 'u.click_the_member_signin_button', 'u.click_the_new_speaker_checkbox', 'u.click_the_podcasting_tab', 'u.click_the_save_post_button', 'u.click_the_search_button', 'u.click_the_signin_button', 'u.click_the_speaker_selector_button', 'u.click_the_upload_file_subtab', 'u.click_the_upload_media_button', 'u.click_web_element', 'u.create_keywords_file', 'u.driver_get_url', 'u.fill_in_field', 'u.fill_in_textarea', 'u.fill_in_the_password_field', 'u.get_contains_list', 'u.get_div', 'u.get_driver', 'u.get_element_contents', 'u.get_last_sunday', 'u.get_mp3_filepath', 'u.get_page_soup', 'u.get_speaker_str', 'u.get_the_file_preview', 'u.get_web_element', 'u.input_xpath', 'u.key_in_search', 'u.log_into_squarespace', 'u.login_css', 'u.login_xpath', 'u.move_to', 'u.move_to_title', 'u.obscure_regex', 'u.preview_xpath', 'u.s', 'u.similar', 'u.spinner_xpath', 'u.title_xpath', 'u.unobscure_element', 'u.upload_xpath', 'u.url_regex', 'u.wait_for']\n",
      "['s.attempt_to_pickle', 's.data_csv_folder', 's.data_folder', 's.encoding_type', 's.load_csv', 's.load_dataframes', 's.load_object', 's.save_dataframes', 's.saves_csv_folder', 's.saves_folder', 's.saves_pickle_folder', 's.store_objects']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ActionChains', 'AudioSegment', 'By', 'Config', 'EC', 'Element', 'Font', 'In', 'Keys', 'Out', 'Path', 'RandomForestClassifier', 'Select', 'SequenceMatcher', 'Storage', 'TfidfVectorizer', 'WebDriverWait', 'Workbook', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__nonzero__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'bs', 'c', 'choropleth_utils', 'cm', 'copyfile', 'csv', 'datetime', 'et', 'exit', 'get_classifier', 'get_data_structs_df', 'get_datastructure_prediction', 'get_dir_tree', 'get_importances', 'get_input_sample', 'get_ipython', 'get_module_version', 'get_notebook_path', 'get_struct_name', 'ipykernel', 'json', 'jupyter_config_dir', 'load_workbook', 'math', 'mpl', 'notebook_path', 'notebookapp', 'np', 'os', 'pd', 'pickle', 'plt', 'preprocess_data', 'pylab', 'pyphen', 'quit', 'random', 're', 'relativedelta', 's', 'scraping_utils', 'storage', 'sys', 'text_editor_path', 'textwrap', 'time', 'timedelta', 'u', 'urllib', 'urlretrieve', 'webdriver', 'xml']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import choropleth_utils\n",
    "import scraping_utils\n",
    "import storage\n",
    "import numpy as np\n",
    "%run ../../load_magic/environment.py\n",
    "%run ../../load_magic/storage.py\n",
    "%pprint\n",
    "\n",
    "notebook_path = get_notebook_path()\n",
    "print(notebook_path)\n",
    "ActionChains = scraping_utils.ActionChains\n",
    "AudioSegment = scraping_utils.AudioSegment\n",
    "By = scraping_utils.By\n",
    "EC = scraping_utils.EC\n",
    "Element = choropleth_utils.Element\n",
    "Font = scraping_utils.Font\n",
    "Keys = scraping_utils.Keys\n",
    "Path = scraping_utils.Path\n",
    "Select = scraping_utils.Select\n",
    "SequenceMatcher = scraping_utils.SequenceMatcher\n",
    "TfidfVectorizer = choropleth_utils.TfidfVectorizer\n",
    "WebDriverWait = scraping_utils.WebDriverWait\n",
    "Workbook = scraping_utils.Workbook\n",
    "bs = scraping_utils.bs\n",
    "c = choropleth_utils.ChoroplethUtilities()\n",
    "cm = choropleth_utils.cm\n",
    "copyfile = choropleth_utils.copyfile\n",
    "csv = storage.csv\n",
    "datetime = scraping_utils.datetime\n",
    "et = choropleth_utils.et\n",
    "load_workbook = scraping_utils.load_workbook\n",
    "math = scraping_utils.math\n",
    "mpl = choropleth_utils.mpl\n",
    "np = scraping_utils.np\n",
    "os = storage.os\n",
    "pd = storage.pd\n",
    "pickle = storage.pickle\n",
    "plt = choropleth_utils.plt\n",
    "pylab = choropleth_utils.pylab\n",
    "pyphen = choropleth_utils.pyphen\n",
    "random = scraping_utils.random\n",
    "re = scraping_utils.re\n",
    "relativedelta = scraping_utils.relativedelta\n",
    "s = storage.Storage()\n",
    "textwrap = choropleth_utils.textwrap\n",
    "time = scraping_utils.time\n",
    "timedelta = scraping_utils.timedelta\n",
    "u = scraping_utils.ScrapingUtilities()\n",
    "urllib = scraping_utils.urllib\n",
    "urlretrieve = scraping_utils.urlretrieve\n",
    "webdriver = scraping_utils.webdriver\n",
    "xml = choropleth_utils.xml\n",
    "print(['c.{}'.format(fn) for fn in dir(c) if not fn.startswith('_')])\n",
    "print(['u.{}'.format(fn) for fn in dir(u) if not fn.startswith('_')])\n",
    "print(['s.{}'.format(fn) for fn in dir(s) if not fn.startswith('_')])\n",
    "text_editor_path = r'C:\\Program Files\\Notepad++\\notepad++.exe'\n",
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wiki_soup.ASCII_SPACES', 'wiki_soup.DEFAULT_BUILDER_FEATURES', 'wiki_soup.NO_PARSER_SPECIFIED_WARNING', 'wiki_soup.ROOT_TAG_NAME', 'wiki_soup.append', 'wiki_soup.attrs', 'wiki_soup.builder', 'wiki_soup.can_be_empty_element', 'wiki_soup.cdata_list_attributes', 'wiki_soup.childGenerator', 'wiki_soup.children', 'wiki_soup.clear', 'wiki_soup.contains_replacement_characters', 'wiki_soup.contents', 'wiki_soup.currentTag', 'wiki_soup.current_data', 'wiki_soup.declared_html_encoding', 'wiki_soup.decode', 'wiki_soup.decode_contents', 'wiki_soup.decompose', 'wiki_soup.descendants', 'wiki_soup.element_classes', 'wiki_soup.encode', 'wiki_soup.encode_contents', 'wiki_soup.endData', 'wiki_soup.extend', 'wiki_soup.extract', 'wiki_soup.fetchNextSiblings', 'wiki_soup.fetchParents', 'wiki_soup.fetchPrevious', 'wiki_soup.fetchPreviousSiblings', 'wiki_soup.find', 'wiki_soup.findAll', 'wiki_soup.findAllNext', 'wiki_soup.findAllPrevious', 'wiki_soup.findChild', 'wiki_soup.findChildren', 'wiki_soup.findNext', 'wiki_soup.findNextSibling', 'wiki_soup.findNextSiblings', 'wiki_soup.findParent', 'wiki_soup.findParents', 'wiki_soup.findPrevious', 'wiki_soup.findPreviousSibling', 'wiki_soup.findPreviousSiblings', 'wiki_soup.find_all', 'wiki_soup.find_all_next', 'wiki_soup.find_all_previous', 'wiki_soup.find_next', 'wiki_soup.find_next_sibling', 'wiki_soup.find_next_siblings', 'wiki_soup.find_parent', 'wiki_soup.find_parents', 'wiki_soup.find_previous', 'wiki_soup.find_previous_sibling', 'wiki_soup.find_previous_siblings', 'wiki_soup.format_string', 'wiki_soup.formatter_for_name', 'wiki_soup.get', 'wiki_soup.getText', 'wiki_soup.get_attribute_list', 'wiki_soup.get_text', 'wiki_soup.handle_data', 'wiki_soup.handle_endtag', 'wiki_soup.handle_starttag', 'wiki_soup.has_attr', 'wiki_soup.has_key', 'wiki_soup.hidden', 'wiki_soup.index', 'wiki_soup.insert', 'wiki_soup.insert_after', 'wiki_soup.insert_before', 'wiki_soup.isSelfClosing', 'wiki_soup.is_empty_element', 'wiki_soup.is_xml', 'wiki_soup.known_xml', 'wiki_soup.markup', 'wiki_soup.name', 'wiki_soup.namespace', 'wiki_soup.new_string', 'wiki_soup.new_tag', 'wiki_soup.next', 'wiki_soup.nextGenerator', 'wiki_soup.nextSibling', 'wiki_soup.nextSiblingGenerator', 'wiki_soup.next_element', 'wiki_soup.next_elements', 'wiki_soup.next_sibling', 'wiki_soup.next_siblings', 'wiki_soup.object_was_parsed', 'wiki_soup.original_encoding', 'wiki_soup.parent', 'wiki_soup.parentGenerator', 'wiki_soup.parents', 'wiki_soup.parse_only', 'wiki_soup.parserClass', 'wiki_soup.parser_class', 'wiki_soup.popTag', 'wiki_soup.prefix', 'wiki_soup.preserve_whitespace_tag_stack', 'wiki_soup.preserve_whitespace_tags', 'wiki_soup.prettify', 'wiki_soup.previous', 'wiki_soup.previousGenerator', 'wiki_soup.previousSibling', 'wiki_soup.previousSiblingGenerator', 'wiki_soup.previous_element', 'wiki_soup.previous_elements', 'wiki_soup.previous_sibling', 'wiki_soup.previous_siblings', 'wiki_soup.pushTag', 'wiki_soup.recursiveChildGenerator', 'wiki_soup.renderContents', 'wiki_soup.replaceWith', 'wiki_soup.replaceWithChildren', 'wiki_soup.replace_with', 'wiki_soup.replace_with_children', 'wiki_soup.reset', 'wiki_soup.select', 'wiki_soup.select_one', 'wiki_soup.setup', 'wiki_soup.smooth', 'wiki_soup.string', 'wiki_soup.strings', 'wiki_soup.stripped_strings', 'wiki_soup.tagStack', 'wiki_soup.text', 'wiki_soup.unwrap', 'wiki_soup.wrap']\n",
      "['page_obj.categories', 'page_obj.content', 'page_obj.coordinates', 'page_obj.html', 'page_obj.images', 'page_obj.links', 'page_obj.original_title', 'page_obj.pageid', 'page_obj.parent_id', 'page_obj.references', 'page_obj.revision_id', 'page_obj.section', 'page_obj.sections', 'page_obj.summary', 'page_obj.title', 'page_obj.url']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "province_urls_list = ['https://en.wikipedia.org/wiki/Badakhshan_Province', 'https://en.wikipedia.org/wiki/Badghis_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Baghlan_Province', 'https://en.wikipedia.org/wiki/Balkh_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Bamyan_Province', 'https://en.wikipedia.org/wiki/Daykundi_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Farah_Province', 'https://en.wikipedia.org/wiki/Faryab_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Ghazni_Province', 'https://en.wikipedia.org/wiki/Ghor_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Helmand_Province', 'https://en.wikipedia.org/wiki/Herat_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Jowzjan_Province', 'https://en.wikipedia.org/wiki/Kabul_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Kandahar_Province', 'https://en.wikipedia.org/wiki/Kapisa_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Khost_Province', 'https://en.wikipedia.org/wiki/Kunar_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Kunduz_Province', 'https://en.wikipedia.org/wiki/Laghman_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Logar_Province', 'https://en.wikipedia.org/wiki/Nangarhar_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Nimruz_Province', 'https://en.wikipedia.org/wiki/Nuristan_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Oruzgan_Province', 'https://en.wikipedia.org/wiki/Paktia_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Paktika_Province', 'https://en.wikipedia.org/wiki/Panjshir_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Parwan_Province', 'https://en.wikipedia.org/wiki/Samangan_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Sar-e_Pol_Province', 'https://en.wikipedia.org/wiki/Takhar_Province',\n",
    "                      'https://en.wikipedia.org/wiki/Wardak_Province', 'https://en.wikipedia.org/wiki/Zabul_Province']\n",
    "for province_url in province_urls_list:\n",
    "    wiki_soup = u.get_page_soup(province_url)\n",
    "    print(['wiki_soup.{}'.format(fn) for fn in dir(wiki_soup) if not fn.startswith('_')])\n",
    "    page_obj = wikipedia.page(title=province_url.split('/')[-1], pageid=None, auto_suggest=True, redirect=True, preload=False)\n",
    "    print(['page_obj.{}'.format(fn) for fn in dir(page_obj) if not fn.startswith('_')])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, (58, 2)), (5, (28, 5)), (3, (19, 2)), (2, (18, 2)), (0, (16, 2)), (4, (13, 2)), (10, (6, 3)), (7, (4, 3)), (8, (3, 3)), (6, (1, 2)), (9, (1, 3)), (11, (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import io\n",
    "\n",
    "f = io.StringIO(page_obj.html())\n",
    "tables_df_list = pd.read_html(f)\n",
    "print(sorted([(i, df.shape) for (i, df) in enumerate(tables_df_list)],\n",
    "             key=lambda x: x[1][0], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wikipedia.API_URL', 'wikipedia.BeautifulSoup', 'wikipedia.Decimal', 'wikipedia.DisambiguationError', 'wikipedia.HTTPTimeoutError', 'wikipedia.ODD_ERROR_MESSAGE', 'wikipedia.PageError', 'wikipedia.RATE_LIMIT', 'wikipedia.RATE_LIMIT_LAST_CALL', 'wikipedia.RATE_LIMIT_MIN_WAIT', 'wikipedia.RedirectError', 'wikipedia.USER_AGENT', 'wikipedia.WikipediaException', 'wikipedia.WikipediaPage', 'wikipedia.cache', 'wikipedia.datetime', 'wikipedia.debug', 'wikipedia.donate', 'wikipedia.exceptions', 'wikipedia.geosearch', 'wikipedia.languages', 'wikipedia.page', 'wikipedia.random', 'wikipedia.re', 'wikipedia.requests', 'wikipedia.search', 'wikipedia.set_lang', 'wikipedia.set_rate_limiting', 'wikipedia.set_user_agent', 'wikipedia.stdout_encode', 'wikipedia.suggest', 'wikipedia.summary', 'wikipedia.sys', 'wikipedia.time', 'wikipedia.timedelta', 'wikipedia.unicode_literals', 'wikipedia.util', 'wikipedia.wikipedia']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import wikipedia\n",
    "\n",
    "print(['wikipedia.{}'.format(fn) for fn in dir(wikipedia) if not fn.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wikipedia.WikipediaPage.categories', 'wikipedia.WikipediaPage.content', 'wikipedia.WikipediaPage.coordinates', 'wikipedia.WikipediaPage.html', 'wikipedia.WikipediaPage.images', 'wikipedia.WikipediaPage.links', 'wikipedia.WikipediaPage.parent_id', 'wikipedia.WikipediaPage.references', 'wikipedia.WikipediaPage.revision_id', 'wikipedia.WikipediaPage.section', 'wikipedia.WikipediaPage.sections', 'wikipedia.WikipediaPage.summary']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(['wikipedia.WikipediaPage.{}'.format(fn) for fn in dir(wikipedia.WikipediaPage) if not fn.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mauto_suggest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Get a WikipediaPage object for the page with title `title` or the pageid\n",
       "`pageid` (mutually exclusive).\n",
       "\n",
       "Keyword arguments:\n",
       "\n",
       "* title - the title of the page to load\n",
       "* pageid - the numeric pageid of the page to load\n",
       "* auto_suggest - let Wikipedia find a valid page title for the query\n",
       "* redirect - allow redirection without raising RedirectError\n",
       "* preload - load content, summary, images, references, and links during initialization\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\577342\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\t\\x0c\\r'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wiki_soup.ASCII_SPACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xpath = '/html/body/div/div[1]/div[1]/div[3]/div[4]/div/div[49]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "svg_dir = os.path.abspath(os.path.join(s.data_folder, 'svg'))\n",
    "file_path = os.path.join(svg_dir, 'Afghanistan.svg')\n",
    "root = et.parse(file_path).getroot()\n",
    "for root_elem in root:\n",
    "    id = root_elem.attrib['id']\n",
    "    tag = root_elem.tag\n",
    "    #print(tag, id)\n",
    "    if (tag.split('}')[-1] == 'g') and (id in ['afghan-provinces', 'afghan-lakes', 'afghan-rivers']):\n",
    "        prefix_str = id.split('-')[1][:-1]\n",
    "        for path_elem in root_elem:\n",
    "            tag = path_elem.tag\n",
    "            if (tag.split('}')[-1] == 'path'):\n",
    "                attrib_dict = path_elem.attrib\n",
    "                id = attrib_dict['id']\n",
    "                #province_name = attrib_dict['{http://www.inkscape.org/namespaces/inkscape}label']\n",
    "                key = 'id'\n",
    "                #value = 'path-{}'.format('-'.join(province_name.lower().split(' ')))\n",
    "                value = '{}-{}'.format(prefix_str, id[5:])\n",
    "                path_elem.attrib[key] = value\n",
    "                #print(id, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xml_str = et.tostring(root, encoding='unicode')\n",
    "with open(file_path, 'w') as f:\n",
    "    print(xml_str.strip(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from svgpathtools import Line, Path\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Convert paths to polygons\n",
    "def path_to_poly(inpath):\n",
    "    points = []\n",
    "    for path in inpath:\n",
    "        if isinstance(path, Line):\n",
    "            points.append([path.end.real, path.end.imag])\n",
    "        else:\n",
    "            num_segments = ceil(path.length() / minimum_stitch)\n",
    "            for seg_i in range(int(num_segments + 1)):\n",
    "                points.append([path.point(seg_i / num_segments).real,\n",
    "                                path.point(seg_i / num_segments).imag])\n",
    "    \n",
    "    return Polygon(points)\n",
    "print(['Path.{}'.format(fn) for fn in dir(Path) if not fn.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from svg.path import parse_path\n",
    "import svgpathtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
