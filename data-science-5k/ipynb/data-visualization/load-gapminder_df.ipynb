{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'platform' from 'C:\\\\Users\\\\Dave\\\\Anaconda3\\\\lib\\\\platform.py'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Handy list of the different types of encodings\n",
    "encoding = ['latin1', 'iso8859-1', 'utf-8'][1]\n",
    "\n",
    "# Change this to your data and saves folders\n",
    "data_folder = r'../../data/'\n",
    "saves_folder = r'../../saves/'\n",
    "\n",
    "def load_object(obj_name, download_url=None):\n",
    "    pickle_path = saves_folder + 'pickle/' + obj_name + '.pickle'\n",
    "    if not os.path.isfile(pickle_path):\n",
    "        csv_path = saves_folder + 'csv/' + obj_name + '.csv'\n",
    "        if not os.path.isfile(csv_path):\n",
    "            object = pd.read_csv(download_url, low_memory=False,\n",
    "                                 encoding=encoding)\n",
    "        else:\n",
    "            object = pd.read_csv(csv_path, low_memory=False,\n",
    "                                 encoding=encoding)\n",
    "        try:\n",
    "            if isinstance(object, pd.DataFrame):\n",
    "                object.to_pickle(pickle_path)\n",
    "            else:\n",
    "                with open(pickle_path, 'wb') as handle:\n",
    "                    pickle.dump(object, handle, pickle.HIGHEST_PROTOCOL)\n",
    "        except:\n",
    "            with open(pickle_path, 'wb') as handle:\n",
    "                pickle.dump(object, handle, pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        try:\n",
    "            object = pd.read_pickle(pickle_path)\n",
    "        except:\n",
    "            with open(pickle_path, 'rb') as handle:\n",
    "                object = pickle.load(handle)\n",
    "    \n",
    "    return(object)\n",
    "\n",
    "# Classes, functions, and methods cannot be pickled\n",
    "def store_objects(**kwargs):\n",
    "    for obj_name in kwargs:\n",
    "        if hasattr(kwargs[obj_name], '__call__'):\n",
    "            raise RuntimeError('Functions cannot be pickled.')\n",
    "        obj_path = saves_folder + 'pickle/' + str(obj_name)\n",
    "        pickle_path = obj_path + '.pickle'\n",
    "        if isinstance(kwargs[obj_name], pd.DataFrame):\n",
    "            kwargs[obj_name].to_pickle(pickle_path)\n",
    "        else:\n",
    "            with open(pickle_path, 'wb') as handle:\n",
    "                pickle.dump(kwargs[obj_name], handle, pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "def attempt_to_pickle(df, pickle_path, raise_exception=False):\n",
    "    try:\n",
    "        print('Pickling to ' + pickle_path)\n",
    "        df.to_pickle(pickle_path)\n",
    "    except Exception as e:\n",
    "        os.remove(pickle_path)\n",
    "        print(e, ': Couldn\\'t save ' + str(df.shape[0]*df.shape[1]) + ' cells as a pickle.')\n",
    "        if raise_exception:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "download_url = 'https://raw.githubusercontent.com/jdantonio/ratistics/master/examples/gapminder.csv'\n",
    "gapminder_df = load_object('gapminder_df', download_url=download_url)\n",
    "original_columns = ['country_name', 'income_per_person', 'alcohol_consumption',\n",
    "                    'armed_forces_rate', 'breast_cancer_per_100th', 'co2_emissions',\n",
    "                    'female_employment_rate', 'hiv_rate', 'internet_use_rate',\n",
    "                    'life_expectancy', 'oil_per_person', 'polity_score',\n",
    "                    'residential_electricity_per_person', 'suicide_per_100th',\n",
    "                    'employment_rate', 'urban_rate']\n",
    "if len(gapminder_df.columns) == len(original_columns):\n",
    "    gapminder_df.columns = original_columns\n",
    "number_column_list = list(set(original_columns) - set(['country_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "obj_path = saves_folder + 'pickle/formal_name_dict.pickle'\n",
    "if not os.path.isfile(obj_path):\n",
    "    formal_name_dict = {}\n",
    "    formal_name_dict['alcohol_consumption'] = '2008 alcohol consumption per adult (age 15+) in litres'\n",
    "    formal_name_dict['armed_forces_rate'] = 'Armed forces personnel as a % of total labor force'\n",
    "    formal_name_dict['breast_cancer_per_100th'] = '2002 breast cancer new cases per hundred thousand females'\n",
    "    formal_name_dict['co2_emissions'] = '2006 cumulative CO2 emission in metric tons'\n",
    "    formal_name_dict['employment_rate'] = '2007 total employees age 15+ as a % of population'\n",
    "    formal_name_dict['female_employment_rate'] = '2007 female employees age 15+ as a % of population'\n",
    "    formal_name_dict['hiv_rate'] = '2009 estimated HIV Prevalence % for Ages 15-49'\n",
    "    formal_name_dict['income_per_person'] = '2010 Gross Domestic Product per capita in constant 2000 USD'\n",
    "    formal_name_dict['internet_use_rate'] = '2010 Internet users per 100 people'\n",
    "    formal_name_dict['life_expectancy'] = '2011 life expectancy at birth in years'\n",
    "    formal_name_dict['oil_per_person'] = '2010 oil Consumption per capita in tonnes per year and person'\n",
    "    formal_name_dict['polity_score'] = '2009 Democracy score as measured by Polity'\n",
    "    formal_name_dict['residential_electricity_per_person'] = '2008 residential electricity consumption per person in kWh'\n",
    "    formal_name_dict['suicide_per_100th'] = '2005 Suicide age adjusted per hundred thousand'\n",
    "    formal_name_dict['urban_rate'] = '2008 urban population as a % of total'\n",
    "    store_objects(formal_name_dict=formal_name_dict)\n",
    "else:\n",
    "    formal_name_dict = load_object('formal_name_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "obj_path = saves_folder + 'pickle/informal_name_dict.pickle'\n",
    "if not os.path.isfile(obj_path):\n",
    "    informal_name_dict = {}\n",
    "    informal_name_dict['alcohol_consumption'] = 'alcohol consumption'\n",
    "    informal_name_dict['armed_forces_rate'] = 'armed forces rate'\n",
    "    informal_name_dict['breast_cancer_per_100th'] = 'breast cancer'\n",
    "    informal_name_dict['co2_emissions'] = 'CO2 emissions'\n",
    "    informal_name_dict['employment_rate'] = 'employment rate'\n",
    "    informal_name_dict['female_employment_rate'] = 'female employment rate'\n",
    "    informal_name_dict['hiv_rate'] = 'HIV rate'\n",
    "    informal_name_dict['income_per_person'] = 'income per person'\n",
    "    informal_name_dict['internet_use_rate'] = 'internet use rate'\n",
    "    informal_name_dict['life_expectancy'] = 'life expectancy'\n",
    "    informal_name_dict['oil_per_person'] = 'oil per person'\n",
    "    informal_name_dict['polity_score'] = 'polity score'\n",
    "    informal_name_dict['residential_electricity_per_person'] = 'residential electricity'\n",
    "    informal_name_dict['suicide_per_100th'] = 'suicide rate'\n",
    "    informal_name_dict['urban_rate'] = 'urban rate'\n",
    "    store_objects(informal_name_dict=informal_name_dict)\n",
    "else:\n",
    "    informal_name_dict = load_object('informal_name_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "obj_path = saves_folder + 'pickle/data_provider_dict.pickle'\n",
    "if not os.path.isfile(obj_path):\n",
    "    data_provider_dict = {}\n",
    "    data_provider_dict['income_per_person'] = 'provided by World Bank'\n",
    "    data_provider_dict['alcohol_consumption'] = 'provided by WHO with additions'\n",
    "    data_provider_dict['armed_forces_rate'] = 'provided by WDI'\n",
    "    data_provider_dict['breast_cancer_per_100th'] = 'based on IARC data'\n",
    "    data_provider_dict['co2_emissions'] = 'provided by CDIAC (Carbon Dioxide Information Analysis Center)'\n",
    "    data_provider_dict['female_employment_rate'] = 'provided by International Labour Organization'\n",
    "    data_provider_dict['hiv_rate'] = 'based on UNAIDS'\n",
    "    data_provider_dict['internet_use_rate'] = 'provided by World Bank'\n",
    "    data_provider_dict['life_expectancy'] = 'based on various sources'\n",
    "    data_provider_dict['oil_per_person'] = 'provided by BP'\n",
    "    data_provider_dict['polity_score'] = 'provided by Polity IV project'\n",
    "    data_provider_dict['residential_electricity_per_person'] = 'provided by IEA (International Energy Agency)'\n",
    "    data_provider_dict['suicide_per_100th'] = 'provided by WHO'\n",
    "    data_provider_dict['employment_rate'] = 'provided by International Labour Organization'\n",
    "    data_provider_dict['urban_rate'] = 'provided by World Bank'\n",
    "    store_objects(data_provider_dict=data_provider_dict)\n",
    "else:\n",
    "    data_provider_dict = load_object('data_provider_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "obj_path = saves_folder + 'pickle/data_procedures_dict.pickle'\n",
    "if not os.path.isfile(obj_path):\n",
    "    data_procedures_dict = {}\n",
    "    data_procedures_dict['income_per_person'] = ('World Bank generally relies on official sources collected at the ' +\n",
    "                                                 'national level. In calculating income per person, the World Bank ' +\n",
    "                                                 'uses the Atlas conversion factor.')\n",
    "    data_procedures_dict['alcohol_consumption'] = ('WHO has been collecting data on alcohol consumption and alcohol control ' +\n",
    "                                                 'policies from its Member States since 1996. The current survey ' +\n",
    "                                                 'instrument entitled \"Global Survey on Alcohol and Health\" includes three ' +\n",
    "                                                 'sections, namely alcohol policy, alcohol consumption, and surveillance. ' +\n",
    "                                                 'The information provided is essential for the preparation of the Global ' +\n",
    "                                                 'Status Report on Alcohol and Health and regional publications, as well ' +\n",
    "                                                 'as for updating the Global Information Systems on Alcohol and Health ' +\n",
    "                                                 '(GISAH) and regional information systems, as requested by the World ' +\n",
    "                                                 'Health Assembly Resolution on Public health problems caused by harmful ' +\n",
    "                                                 'use of alcohol.')\n",
    "    data_procedures_dict['armed_forces_rate'] = ('WDI estimates of military spending are based on both the official ' +\n",
    "                                                 'defence budget and data and estimates for a number of items outside the ' +\n",
    "                                                 'budget.')\n",
    "    data_procedures_dict['breast_cancer_per_100th'] = ('The International Association of Cancer Registries (IACR) was formed in ' +\n",
    "                                                       '1966 to develop and standardize collection methods across registries to ' +\n",
    "                                                       'make their data as comparable as possible.')\n",
    "    data_procedures_dict['co2_emissions'] = ('The CDIAC data have been collected over 30 years of operation. The data ' +\n",
    "                                             'from which these carbon-emissions estimates were derived are values of ' +\n",
    "                                             'fuel consumed: in billions of cubic feet, for natural gas; in millions ' +\n",
    "                                             'of barrels, for petroleum products; and in thousands of short tons, for ' +\n",
    "                                             'coal.')\n",
    "    data_procedures_dict['female_employment_rate'] = ('The International Labour Organization (ILO) data for female employment ' +\n",
    "                                                      'rate contains labour force participation rate estimates and projections ' +\n",
    "                                                      'by sex, for the standardized age group of 15+, and for the years 1990 to ' +\n",
    "                                                      '2030. The participation rates are harmonized to account for differences ' +\n",
    "                                                      'in national data collection and tabulation methodologies as well as for ' +\n",
    "                                                      'other country-specific factors such as military service requirements. ' +\n",
    "                                                      'The series includes both nationally reported and imputed data and only ' +\n",
    "                                                      'estimates that are national, meaning there are no geographical ' +\n",
    "                                                      'limitations on coverage. There are systematic differences in the type of ' +\n",
    "                                                      'data source related to the methodology of collection, definitions, scope ' +\n",
    "                                                      'of coverage and reference period that impact the interpretation of ' +\n",
    "                                                      'female employment rate from one country to another. An effort has been ' +\n",
    "                                                      'made in the examination of country-level data to remove non-comparable ' +\n",
    "                                                      'data.')\n",
    "    data_procedures_dict['hiv_rate'] = ('UNAIDS hiv rate data are based on modelled HIV estimates. Modelled HIV ' +\n",
    "                                        'estimates are created by country teams using UNAIDS-supported software. ' +\n",
    "                                        'The country teams are comprised primarily of epidemiologists, ' +\n",
    "                                        'demographers, monitoring and evaluation specialists and technical ' +\n",
    "                                        'partners. Country-submitted files are reviewed at UNAIDS, and selected ' +\n",
    "                                        'HIV service data contained in the files are reviewed and validated in ' +\n",
    "                                        'partnership with WHO and UNICEF. UNAIDS review aims to ensure ' +\n",
    "                                        'comparability of results across regions, countries and over time.')\n",
    "    data_procedures_dict['internet_use_rate'] = ('World Bank generally relies on official sources collected at the ' +\n",
    "                                                 'national level.')\n",
    "    data_procedures_dict['life_expectancy'] = ('Life expectancy data mostly come from survey, census, and death registration data.')\n",
    "    data_procedures_dict['oil_per_person'] = ('The data series for oil per person provided by BP does not necessarily ' +\n",
    "                                              'meet the definitions, guidelines and practices used for determining ' +\n",
    "                                              'proved reserves at company level, for instance, as published by the US ' +\n",
    "                                              'Securities and Exchange Commission, nor does it necessarily represent ' +\n",
    "                                              'BP’s view of proved reserves by country. Rather, the data series has ' +\n",
    "                                              'been compiled using a combination of primary official sources and ' +\n",
    "                                              'third-party data.')\n",
    "    data_procedures_dict['polity_score'] = ('The Polity IV dataset covers all major, independent states in the global ' +\n",
    "                                            'system over the period 1800-2015 (i.e., states with a total population ' +\n",
    "                                            'of 500,000 or more in the most recent year; currently 167 countries). ' +\n",
    "                                            'With the support of the Political Instability Task Force, the Polity IV ' +\n",
    "                                            'Project has been transformed into a living data collection effort, ' +\n",
    "                                            'meaning that it constantly monitors regime changes in all major ' +\n",
    "                                            'countries and provides annual assessments of regime authority ' +\n",
    "                                            'characteristics, changes and data updates.')\n",
    "    data_procedures_dict['residential_electricity_per_person'] = ('The data provided by IEA (International Energy Agency) are based on ' +\n",
    "                                                                  'years, plant efficiency, and capital costs. Years refer to time of plant ' +\n",
    "                                                                  'order. Costs include owner’s costs but exclude interest during ' +\n",
    "                                                                  'construction. Plant efficiency is gross, LHV (lower heating value). The ' +\n",
    "                                                                  'difference between lower and higher heating value, based on IEA ' +\n",
    "                                                                  'conventions, is 5% for coal and 10% for gas. Capital costs presented are ' +\n",
    "                                                                  'a weighted average based on deployment for the given scenario. Capital ' +\n",
    "                                                                  'costs for renewable energy technologies and CCS-equipped power plants ' +\n",
    "                                                                  'are projected based on the levels of regional and global deployment, ' +\n",
    "                                                                  'applying an assumed learning rates for each doubling of capacity. ' +\n",
    "                                                                  'Capital costs for nuclear power and unabated coal- and gas-fired power ' +\n",
    "                                                                  'plants are assumed throughout the projection period.')\n",
    "    data_procedures_dict['suicide_per_100th'] = ('WHO global estimates of the number of suicides in a country in the year ' +\n",
    "                                                 '2005, divided by the population and multiplied with 100,000, represent ' +\n",
    "                                                 'the best estimates of WHO, computed using standard categories, ' +\n",
    "                                                 'definitions and methods to ensure cross-country comparability, and may ' +\n",
    "                                                 'not be the same as official national estimates. The estimates are ' +\n",
    "                                                 'rounded to the appropriate number of significant figures and ' +\n",
    "                                                 'standardized to the WHO World Standard Population.')\n",
    "    data_procedures_dict['employment_rate'] = ('The International Labour Organization data for employment rate contains ' +\n",
    "                                               'labour force participation rate estimates and projections for the ' +\n",
    "                                               'standardized age group of 15+ and for the years 1990 to 2030. The ' +\n",
    "                                               'participation rates are harmonized to account for differences in ' +\n",
    "                                               'national data collection and tabulation methodologies as well as for ' +\n",
    "                                               'other country-specific factors such as military service requirements. ' +\n",
    "                                               'The series includes both nationally reported and imputed data and only ' +\n",
    "                                               'estimates that are national, meaning there are no geographical ' +\n",
    "                                               'limitations on coverage.')\n",
    "    data_procedures_dict['urban_rate'] = ('Urban population refers to people living in urban areas as defined by ' +\n",
    "                                          'national statistical offices. The indicator is calculated using World ' +\n",
    "                                          'Bank population estimates and urban ratios from the United Nations World ' +\n",
    "                                          'Urbanization Prospects. To estimate urban populations, UN ratios of ' +\n",
    "                                          'urban to total population were applied to the World Bank’s estimates of ' +\n",
    "                                          'total population. Countries differ in the way they classify population ' +\n",
    "                                          'as \"urban\" or \"rural\". The population of a city or metropolitan area ' +\n",
    "                                          'depends on the boundaries chosen.')\n",
    "    store_objects(data_procedures_dict=data_procedures_dict)\n",
    "else:\n",
    "    data_procedures_dict = load_object('data_procedures_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "obj_path = saves_folder + 'pickle/data_measures_dict.pickle'\n",
    "if not os.path.isfile(obj_path):\n",
    "    data_measures_dict = {}\n",
    "    data_measures_dict['income_per_person'] = ('gross national income (GNI, formerly referred to as GNP) and GNI per capita in U.S. dollars')\n",
    "    data_measures_dict['alcohol_consumption'] = ('Alcohol Timeline Followback (TLFB), Form 90, Drinking Self–Monitoring ' +\n",
    "                                                 'Log (DSML), Lifetime Drinking (LDH), and various Quantity–Frequency (QF) ' +\n",
    "                                                 'measures')\n",
    "    data_measures_dict['armed_forces_rate'] = ('WDI estimates of military spending are based on both the official ' +\n",
    "                                                 'defence budget and data and estimates for a number of items outside the ' +\n",
    "                                                 'budget.')\n",
    "    data_measures_dict['breast_cancer_per_100th'] = ('The International Association of Cancer Registries (IACR) was formed in ' +\n",
    "                                                       '1966 to develop and standardize collection methods across registries to ' +\n",
    "                                                       'make their data as comparable as possile.')\n",
    "    data_measures_dict['co2_emissions'] = ('The CDIAC data have been collected over 30 years of operation. The data ' +\n",
    "                                             'from which these carbon-emissions estimates were derived are values of ' +\n",
    "                                             'fuel consumed: in billions of cubic feet, for natural gas; in millions ' +\n",
    "                                             'of barrels, for petroleum products; and in thousands of short tons, for ' +\n",
    "                                             'coal.')\n",
    "    data_measures_dict['female_employment_rate'] = ('The International Labour Organization (ILO) data for female employment ' +\n",
    "                                                      'rate contains labour force participation rate estimates and projections ' +\n",
    "                                                      'by sex, for the standardized age group of 15+, and for the years 1990 to ' +\n",
    "                                                      '2030. The participation rates are harmonized to account for differences ' +\n",
    "                                                      'in national data collection and tabulation methodologies as well as for ' +\n",
    "                                                      'other country-specific factors such as military service requirements. ' +\n",
    "                                                      'The series includes both nationally reported and imputed data and only ' +\n",
    "                                                      'estimates that are national, meaning there are no geographical ' +\n",
    "                                                      'limitations on coverage. There are systematic differences in the type of ' +\n",
    "                                                      'data source related to the methodology of collection, definitions, scope ' +\n",
    "                                                      'of coverage and reference period that impact the interpretation of ' +\n",
    "                                                      'female employment rate from one country to another. An effort has been ' +\n",
    "                                                      'made in the examination of country-level data to remove non-comparable ' +\n",
    "                                                      'data.')\n",
    "    data_measures_dict['hiv_rate'] = ('UNAIDS hiv rate data are based on modelled HIV estimates. Modelled HIV ' +\n",
    "                                        'estimates are created by country teams using UNAIDS-supported software. ' +\n",
    "                                        'The country teams are comprised primarily of epidemiologists, ' +\n",
    "                                        'demographers, monitoring and evaluation specialists and technical ' +\n",
    "                                        'partners. Country-submitted files are reviewed at UNAIDS, and selected ' +\n",
    "                                        'HIV service data contained in the files are reviewed and validated in ' +\n",
    "                                        'partnership with WHO and UNICEF. UNAIDS review aims to ensure ' +\n",
    "                                        'comparability of results across regions, countries and over time.')\n",
    "    data_measures_dict['internet_use_rate'] = ('World Bank generally relies on official sources collected at the ' +\n",
    "                                                 'national level.')\n",
    "    data_measures_dict['life_expectancy'] = ('Life expectancy data mostly come from survey, census, and death registration data.')\n",
    "    data_measures_dict['oil_per_person'] = ('The data series for oil per person provided by BP does not necessarily ' +\n",
    "                                              'meet the definitions, guidelines and practices used for determining ' +\n",
    "                                              'proved reserves at company level, for instance, as published by the US ' +\n",
    "                                              'Securities and Exchange Commission, nor does it necessarily represent ' +\n",
    "                                              'BP’s view of proved reserves by country. Rather, the data series has ' +\n",
    "                                              'been compiled using a combination of primary official sources and ' +\n",
    "                                              'third-party data.')\n",
    "    data_measures_dict['polity_score'] = ('The Polity IV dataset covers all major, independent states in the global ' +\n",
    "                                            'system over the period 1800-2015 (i.e., states with a total population ' +\n",
    "                                            'of 500,000 or more in the most recent year; currently 167 countries). ' +\n",
    "                                            'With the support of the Political Instability Task Force, the Polity IV ' +\n",
    "                                            'Project has been transformed into a living data collection effort, ' +\n",
    "                                            'meaning that it constantly monitors regime changes in all major ' +\n",
    "                                            'countries and provides annual assessments of regime authority ' +\n",
    "                                            'characteristics, changes and data updates.')\n",
    "    data_measures_dict['residential_electricity_per_person'] = ('The data provided by IEA (International Energy Agency) are based on ' +\n",
    "                                                                  'years, plant efficiency, and capital costs. Years refer to time of plant ' +\n",
    "                                                                  'order. Costs include owner’s costs but exclude interest during ' +\n",
    "                                                                  'construction. Plant efficiency is gross, LHV (lower heating value). The ' +\n",
    "                                                                  'difference between lower and higher heating value, based on IEA ' +\n",
    "                                                                  'conventions, is 5% for coal and 10% for gas. Capital costs presented are ' +\n",
    "                                                                  'a weighted average based on deployment for the given scenario. Capital ' +\n",
    "                                                                  'costs for renewable energy technologies and CCS-equipped power plants ' +\n",
    "                                                                  'are projected based on the levels of regional and global deployment, ' +\n",
    "                                                                  'applying an assumed learning rates for each doubling of capacity. ' +\n",
    "                                                                  'Capital costs for nuclear power and unabated coal- and gas-fired power ' +\n",
    "                                                                  'plants are assumed throughout the projection period.')\n",
    "    data_measures_dict['suicide_per_100th'] = ('WHO global estimates of the number of suicides in a country in the year ' +\n",
    "                                                 '2005, divided by the population and multiplied with 100,000, represent ' +\n",
    "                                                 'the best estimates of WHO, computed using standard categories, ' +\n",
    "                                                 'definitions and methods to ensure cross-country comparability, and may ' +\n",
    "                                                 'not be the same as official national estimates. The estimates are ' +\n",
    "                                                 'rounded to the appropriate number of significant figures and ' +\n",
    "                                                 'standardized to the WHO World Standard Population.')\n",
    "    data_measures_dict['employment_rate'] = ('The International Labour Organization data for employment rate contains ' +\n",
    "                                               'labour force participation rate estimates and projections for the ' +\n",
    "                                               'standardized age group of 15+ and for the years 1990 to 2030. The ' +\n",
    "                                               'participation rates are harmonized to account for differences in ' +\n",
    "                                               'national data collection and tabulation methodologies as well as for ' +\n",
    "                                               'other country-specific factors such as military service requirements. ' +\n",
    "                                               'The series includes both nationally reported and imputed data and only ' +\n",
    "                                               'estimates that are national, meaning there are no geographical ' +\n",
    "                                               'limitations on coverage.')\n",
    "    data_measures_dict['urban_rate'] = ('Urban population refers to people living in urban areas as defined by ' +\n",
    "                                          'national statistical offices. The indicator is calculated using World ' +\n",
    "                                          'Bank population estimates and urban ratios from the United Nations World ' +\n",
    "                                          'Urbanization Prospects. To estimate urban populations, UN ratios of ' +\n",
    "                                          'urban to total population were applied to the World Bank’s estimates of ' +\n",
    "                                          'total population. Countries differ in the way they classify population ' +\n",
    "                                          'as \"urban\" or \"rural\". The population of a city or metropolitan area ' +\n",
    "                                          'depends on the boundaries chosen.')\n",
    "    store_objects(data_measures_dict=data_measures_dict)\n",
    "else:\n",
    "    data_measures_dict = load_object('data_measures_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_binned_categories(df, number_of_categories, column_name, prefix):\n",
    "    \n",
    "    # Get the percentiles\n",
    "    out_categorical, percentiles_list = pd.cut([0, 1], number_of_categories, retbins=True)\n",
    "    describe_series = df[column_name].describe(percentiles=percentiles_list[1:-1]).copy()\n",
    "\n",
    "    # Get the bin list and group names\n",
    "    bad_list = ['count', 'mean', 'std']\n",
    "    if (number_of_categories % 2) == 1:\n",
    "        bad_list += ['50%']\n",
    "    \n",
    "    # array of indexes, e.g. ['min', '50%', 'max']\n",
    "    index_list = [x for x in describe_series.index.tolist() if x not in bad_list]\n",
    "    bin_list = describe_series.loc[index_list].tolist()\n",
    "    if len(set(bin_list)) == len(bin_list):\n",
    "        \n",
    "        # Create the extra column\n",
    "        df[prefix+'_categories'] = pd.cut(x=df[column_name],\n",
    "                                          bins=bin_list).map(lambda x: (x.left + x.right)/2.)\n",
    "    else:\n",
    "\n",
    "        # array of quantiles, e.g. [0, .25, .5, .75, 1.]\n",
    "        quantiles_list = []\n",
    "        for index in index_list:\n",
    "            if index == 'min':\n",
    "                quantiles_list.append(0)\n",
    "            elif index == 'max':\n",
    "                quantiles_list.append(1.)\n",
    "            else:\n",
    "                quantiles_list.append(float(index.split('%')[0])/100.)\n",
    "        \n",
    "        # Create the extra column\n",
    "        df[prefix+'_categories'] = pd.qcut(x=df[column_name], q=quantiles_list,\n",
    "                                           duplicates='drop').map(lambda x: (x.left + x.right)/2.)\n",
    "\n",
    "    # Fix the bottom row\n",
    "    null_series = df[prefix+'_categories'].isnull()\n",
    "    df.loc[null_series, prefix+'_categories'] = df[~null_series][prefix+'_categories'].min()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import squareform, pdist, euclidean\n",
    "import numpy as np\n",
    "\n",
    "# From https://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "def round_down(num, divisor):\n",
    "    \n",
    "    return num - (num%divisor)\n",
    "\n",
    "def round_up(num, divisor):\n",
    "    \n",
    "    return num - (num%divisor) + divisor\n",
    "\n",
    "def get_min_max(df, column_name, circle_min=5, circle_max=500):\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(circle_min, circle_max))\n",
    "    min_max = min_max_scaler.fit_transform(df[column_name].values.reshape(-1, 1))\n",
    "    \n",
    "    return min_max\n",
    "\n",
    "def conjunctify_list(noun_list):\n",
    "    if len(noun_list) > 2:\n",
    "        list_str = ', and '.join([', '.join(noun_list[:-1])] + [noun_list[-1]])\n",
    "    elif len(noun_list) == 2:\n",
    "        list_str = ' and '.join(noun_list)\n",
    "    elif len(noun_list) == 1:\n",
    "        list_str = noun_list[0]\n",
    "    else:\n",
    "        list_str = ''\n",
    "    \n",
    "    return list_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def set_up_a_minimal_joint_grid(sts):\n",
    "    scatter_kws = dict(s=0, edgecolors='w')\n",
    "    joint_kws = dict(scatter_kws=scatter_kws)\n",
    "    xlim_multiple = round_up((sts.df[sts.qe_column_name].max() - sts.df[sts.qe_column_name].min()) / 10, 10)\n",
    "    xlim = (round_down(sts.df[sts.qe_column_name].min(), xlim_multiple),\n",
    "            round_up(sts.df[sts.qe_column_name].max(), xlim_multiple))\n",
    "    ylim_multiple = round_up((sts.df[sts.qr_column_name].max() - sts.df[sts.qr_column_name].min()) / 10, 10)\n",
    "    ylim = (round_down(sts.df[sts.qr_column_name].min(), ylim_multiple),\n",
    "            round_up(sts.df[sts.qr_column_name].max(), ylim_multiple))\n",
    "    sts.joint_grid = sns.jointplot(x=sts.qe_column_name, y=sts.qr_column_name, data=sts.df, size=13, space=0,\n",
    "                                   stat_func=None, kind='reg',\n",
    "                                   joint_kws=joint_kws, marginal_kws=dict(bins=15, rug=True))\n",
    "    \n",
    "    return sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_correct_scale(sts):\n",
    "    sts.joint_grid.ax_joint.set_autoscale_on(b=True)\n",
    "    sts.joint_grid.ax_marg_x.set_autoscale_on(b=True)\n",
    "    sts.joint_grid.ax_marg_y.set_autoscale_on(b=True)\n",
    "    \n",
    "    sts.ax_joint_xlim = sts.joint_grid.ax_joint.get_xlim()\n",
    "    sts.ax_joint_ylim = sts.joint_grid.ax_joint.get_ylim()\n",
    "    \n",
    "    sts.ax_marg_x_xlim = sts.joint_grid.ax_marg_x.get_xlim()\n",
    "    sts.ax_marg_x_ylim = sts.joint_grid.ax_marg_x.get_ylim()\n",
    "    \n",
    "    sts.ax_marg_y_xlim = sts.joint_grid.ax_marg_y.get_xlim()\n",
    "    sts.ax_marg_y_ylim = sts.joint_grid.ax_marg_y.get_ylim()\n",
    "    \n",
    "    return sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def set_the_axes_and_title_text(sts):\n",
    "    xlabel = sts.qe_formal_name + ' (Explanatory Variable)'\n",
    "    ylabel = sts.qr_formal_name + ' (Response Variable)'\n",
    "    sts.joint_grid.set_axis_labels(xlabel=xlabel, ylabel=ylabel)\n",
    "    plot_title_text = ('Scatterplot for the association between ' +\n",
    "                       sts.qe_informal_name + ' and ' +\n",
    "                       sts.qr_informal_name + ', colored and sized with ' +\n",
    "                       sts.md_informal_name + ' (' +\n",
    "                       sts.low_high + ')')\n",
    "    sts.joint_grid.fig.suptitle(plot_title_text)\n",
    "    \n",
    "    return sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def re_color_and_resize_the_plot_points(sts):\n",
    "    cmap = plt.get_cmap('viridis_r')\n",
    "    scatter_kws = dict(s=get_min_max(sts.df, sts.md_column_name), edgecolors=(0, 0, 0),\n",
    "                       cmap=cmap, color=None, c=sts.df[sts.md_column_name])\n",
    "    sts.joint_grid = sts.joint_grid.plot_joint(sns.regplot, fit_reg=False, scatter_kws=scatter_kws)\n",
    "    \n",
    "    return sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def force_correct_scale(sts):\n",
    "    sts.joint_grid.ax_joint.set_autoscale_on(b=True)\n",
    "    sts.joint_grid.ax_marg_x.set_autoscale_on(b=True)\n",
    "    sts.joint_grid.ax_marg_y.set_autoscale_on(b=True)\n",
    "    \n",
    "    sts.joint_grid.ax_joint.set_xlim(sts.ax_joint_xlim)\n",
    "    sts.joint_grid.ax_marg_x.set_xlim(sts.ax_marg_x_xlim)\n",
    "    sts.joint_grid.ax_marg_y.set_xlim(sts.ax_marg_y_xlim)\n",
    "    \n",
    "    sts.joint_grid.ax_joint.set_ylim(sts.ax_joint_ylim)\n",
    "    sts.joint_grid.ax_marg_x.set_ylim(sts.ax_marg_x_ylim)\n",
    "    sts.joint_grid.ax_marg_y.set_ylim(sts.ax_marg_y_ylim)\n",
    "    \n",
    "    return sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def set_the_upper_left_math(sts):\n",
    "    r_squared = lambda a, b: pearsonr(a, b)[0] ** 2\n",
    "    sts.joint_grid = sts.joint_grid.annotate(r_squared, template='{stat}: {val:.2f}', stat='$R^2$',\n",
    "                                     loc='upper left', fontsize=18, bbox_to_anchor=(-0.07, 1.0),\n",
    "                                     frameon=False)\n",
    "    \n",
    "    return sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def set_the_country_name_labels(sts):\n",
    "    kwargs = dict(textcoords='offset points', ha='left', va='bottom',\n",
    "                  bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.25),\n",
    "                  arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # Get the center of the plot points\n",
    "    column_pair = [sts.qe_column_name, sts.qr_column_name]\n",
    "    center_series = sts.df[column_pair].apply(lambda col: col.mean(), axis=0)\n",
    "    center_matrix = center_series.as_matrix()\n",
    "    center_x = center_matrix[0]\n",
    "    center_y = center_matrix[1]\n",
    "    sts.df['dist_from_center'] = sts.df.apply(lambda row: euclidean(center_matrix, row[column_pair].as_matrix()), axis=1)\n",
    "    sts.df['angle_from_center'] = sts.df.apply(lambda row: angle_between(center_matrix,\n",
    "                                                                         row[column_pair].as_matrix()), axis=1)\n",
    "    \n",
    "    # Compute DBSCAN\n",
    "    sts.df_ndarray = PCA(n_components=2).fit_transform(sts.df[column_pair])\n",
    "    sts.df_dbscan = DBSCAN(eps=sts.df['dist_from_center'].median(), min_samples=3).fit(sts.df_ndarray)\n",
    "    sts.df['dbscan_group'] = sts.df_dbscan.labels_\n",
    "    #print(sts.df['dbscan_group'])\n",
    "    \n",
    "    # Annotate those countries far from the center\n",
    "    angle_list = []\n",
    "    dist_list = []\n",
    "    precision_level = 3\n",
    "    max_x = sts.df[sts.qe_column_name].max()\n",
    "    min_x = sts.df[sts.qe_column_name].min()\n",
    "    max_y = sts.df[sts.qr_column_name].max()\n",
    "    min_y = sts.df[sts.qr_column_name].min()\n",
    "    max_s = sts.df[sts.md_column_name].max()\n",
    "    min_s = sts.df[sts.md_column_name].min()\n",
    "    for row_index, row_series in sts.df.sort_values('dist_from_center', ascending=False).iterrows():\n",
    "        row_x = row_series[sts.qe_column_name]\n",
    "        row_y = row_series[sts.qr_column_name]\n",
    "        row_s = row_series[sts.md_column_name]\n",
    "        row_group = row_series['dbscan_group']\n",
    "        row_dist = row_series['dist_from_center']\n",
    "        #row_angle = row_series['angle_from_center']\n",
    "        #row_dist = row_series['dist_from_center']\n",
    "        #if round(row_angle, precision_level) not in angle_list:\n",
    "        #angle_list.append(round(row_angle, precision_level))\n",
    "        annotation_boolean = (row_index < 2) or (row_group == -1) or (row_x == max_x) or (row_x == min_x)\n",
    "        annotation_boolean = annotation_boolean or (row_y == max_y) or (row_y == min_y) or (row_s == max_s)\n",
    "        annotation_boolean = annotation_boolean or (row_s == min_s)\n",
    "        if annotation_boolean:\n",
    "            annotation = sts.joint_grid.ax_joint.annotate(row_series['country_name'], xy=(row_x, row_y),\n",
    "                                                          xytext=(20*n for n in np.sign([row_x-center_x,\n",
    "                                                                                         row_y-center_y])), **kwargs)\n",
    "    \n",
    "    return sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "\n",
    "def add_joint_plot(sts):\n",
    "\n",
    "    # Turn interactive plotting off\n",
    "    plt.ioff()\n",
    "    \n",
    "    # Set up a minimal joint grid\n",
    "    print()\n",
    "    sts = set_up_a_minimal_joint_grid(sts)\n",
    "    \n",
    "    # Get correct scale\n",
    "    sts = get_correct_scale(sts)\n",
    "\n",
    "    # Set the axes and title text\n",
    "    sts = set_the_axes_and_title_text(sts)\n",
    "\n",
    "    # Re-color and resize the plot points\n",
    "    sts = re_color_and_resize_the_plot_points(sts)\n",
    "    \n",
    "    # Force correct scale\n",
    "    sts = force_correct_scale(sts)\n",
    "\n",
    "    # Set the upper left math\n",
    "    sts = set_the_upper_left_math(sts)\n",
    "    \n",
    "    # Force correct scale\n",
    "    sts = force_correct_scale(sts)\n",
    "    \n",
    "    # Set the country name labels\n",
    "    sts = set_the_country_name_labels(sts)\n",
    "    \n",
    "    # Save the new figure then close it so it never gets displayed\n",
    "    file_name = ('../../saves/png/plot_' + sts.qe_column_name + '_' + sts.qr_column_name +\n",
    "                 '_' + sts.md_column_name + '_' + sts.low_high + '.png')\n",
    "    sts.joint_grid.savefig(file_name)\n",
    "    plt.close(sts.joint_grid.fig)\n",
    "    \n",
    "    #sts.verbose_html += '<p><image src=\"' + file_name + '\" /></p>'\n",
    "    \n",
    "    # Display all \"open\" (non-closed) figures\n",
    "    plt.show()\n",
    "\n",
    "    return Image(filename=file_name, width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "\n",
    "def add_facet_grid(sts):\n",
    "\n",
    "    # Turn interactive plotting off\n",
    "    plt.ioff()\n",
    "    \n",
    "    # Categorical explanatory variable\n",
    "    sts.df[sts.ce_column_name] = sts.df[sts.ce_column_name].astype('float64')\n",
    "    \n",
    "    # Categorical response variable\n",
    "    sts.df[sts.cr_column_name] = sts.df[sts.cr_column_name].astype('float64')\n",
    "    \n",
    "    # Set up a minimal facet grid\n",
    "    sts.facet_grid = sns.factorplot(x=sts.ce_column_name, y=sts.cr_column_name, data=sts.df, kind='bar', ci=None, size=10)\n",
    "\n",
    "    # Set the axes and title text\n",
    "    title_text = ('Graph percent with ' +\n",
    "                  sts.cr_informal_name + ' within each ' +\n",
    "                  sts.ce_informal_name + ' group, moderated by ' +\n",
    "                  sts.md_informal_name + ' (' +\n",
    "                  sts.low_high + ')')\n",
    "    sts.facet_grid = sts.facet_grid.set(xlabel=sts.ce_formal_name + ', group averages (Explanatory Variable)',\n",
    "                                        ylabel=sts.cr_formal_name + ' (Response Variable)',\n",
    "                                        title=title_text)\n",
    "    \n",
    "    # Save the new figure then close it so it never gets displayed\n",
    "    file_name = ('../../saves/png/plot_' +\n",
    "                 sts.ce_column_name + '_' +\n",
    "                 sts.cr_column_name + '.png')\n",
    "    sts.facet_grid.savefig(file_name)\n",
    "    plt.close(sts.facet_grid.fig)\n",
    "    \n",
    "    #sts.verbose_html += '<p><image src=\"' + file_name + '\" /></p>'\n",
    "    \n",
    "    # Display all \"open\" (non-closed) figures\n",
    "    plt.show()\n",
    "\n",
    "    return Image(filename=file_name, width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import chi2_contingency\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "class Statements(object):\n",
    "\n",
    "    def __init__(self, df, **kwargs):\n",
    "        prop_defaults = {\n",
    "            'number_of_categories': 5,\n",
    "            'category_labels_list': ['Very Low', 'Low', 'Moderate', 'High', 'Very High'],\n",
    "            'low_high': 'both',\n",
    "            'explanation_list': ['Pearson’s correlation coefficient', '2-tailed p-value'],\n",
    "            'verbose_html': '',\n",
    "            'test_type': 'pearsonr',\n",
    "            'test_type_html': '',\n",
    "            'qe_column_name': None,\n",
    "            'qr_column_name': None,\n",
    "            'md_column_name': None,\n",
    "            'ce_column_name': None,\n",
    "            'cr_column_name': None,\n",
    "            'qe_formal_name': None,\n",
    "            'qr_formal_name': None,\n",
    "            'md_formal_name': None,\n",
    "            'ce_formal_name': None,\n",
    "            'cr_formal_name': None,\n",
    "            'qe_informal_name': None,\n",
    "            'qr_informal_name': None,\n",
    "            'md_informal_name': None,\n",
    "            'ce_informal_name': None,\n",
    "            'cr_informal_name': None,\n",
    "        }\n",
    "\n",
    "        for (prop, default) in prop_defaults.items():\n",
    "            setattr(self, prop, kwargs.get(prop, default))\n",
    "        \n",
    "        # Initialize informal names\n",
    "        if self.md_column_name is not None:\n",
    "            if self.md_formal_name is None:\n",
    "                self.md_formal_name = formal_name_dict[self.md_column_name]\n",
    "            if self.md_informal_name is None:\n",
    "                self.md_informal_name = informal_name_dict[self.md_column_name]\n",
    "        \n",
    "        if self.test_type is 'pearsonr':\n",
    "            if self.qe_formal_name is None:\n",
    "                self.qe_formal_name = formal_name_dict[self.qe_column_name]\n",
    "            if self.qe_informal_name is None:\n",
    "                self.qe_informal_name = informal_name_dict[self.qe_column_name]\n",
    "            \n",
    "            if self.qr_formal_name is None:\n",
    "                self.qr_formal_name = formal_name_dict[self.qr_column_name]\n",
    "            if self.qr_informal_name is None:\n",
    "                self.qr_informal_name = informal_name_dict[self.qr_column_name]\n",
    "            \n",
    "            if self.md_column_name is None:\n",
    "                self.subset_columns = [self.qe_column_name, self.qr_column_name]\n",
    "            else:\n",
    "                self.subset_columns = [self.qe_column_name, self.qr_column_name, self.md_column_name]\n",
    "        \n",
    "        elif self.test_type is 'chi2':\n",
    "            if self.ce_formal_name is None:\n",
    "                self.ce_formal_name = formal_name_dict[self.ce_column_name]\n",
    "            if self.ce_informal_name is None:\n",
    "                self.ce_informal_name = informal_name_dict[self.ce_column_name]\n",
    "            \n",
    "            if self.cr_formal_name is None:\n",
    "                self.cr_formal_name = formal_name_dict[self.cr_column_name]\n",
    "            if self.cr_informal_name is None:\n",
    "                self.cr_informal_name = informal_name_dict[self.cr_column_name]\n",
    "            \n",
    "            if self.md_column_name is None:\n",
    "                self.subset_columns = [self.ce_column_name, self.cr_column_name]\n",
    "            else:\n",
    "                self.subset_columns = [self.ce_column_name, self.cr_column_name, self.md_column_name]\n",
    "        \n",
    "        elif self.test_type is 'ols':\n",
    "            if self.ce_formal_name is None:\n",
    "                self.ce_formal_name = formal_name_dict[self.ce_column_name]\n",
    "            if self.ce_informal_name is None:\n",
    "                self.ce_informal_name = informal_name_dict[self.ce_column_name]\n",
    "            \n",
    "            if self.qr_formal_name is None:\n",
    "                self.qr_formal_name = formal_name_dict[self.qr_column_name]\n",
    "            if self.qr_informal_name is None:\n",
    "                self.qr_informal_name = informal_name_dict[self.qr_column_name]\n",
    "            \n",
    "            if self.md_column_name is None:\n",
    "                self.subset_columns = [self.ce_column_name, self.qr_column_name]\n",
    "            else:\n",
    "                self.subset_columns = [self.ce_column_name, self.qr_column_name, self.md_column_name]\n",
    "        \n",
    "        self.df = df.dropna(how='any', subset=self.subset_columns).copy()\n",
    "        self.row_count = self.df.shape[0]\n",
    "        \n",
    "        if 'ce_categories' in self.df.columns:\n",
    "            self.category_list = sorted(self.df['ce_categories'].astype('float64').unique().tolist())\n",
    "            self.category_labels_dict = dict(zip(self.category_list, self.category_labels_list))\n",
    "            self.number_of_categories = len(self.category_list)\n",
    "\n",
    "        if 'md_categories' in self.df.columns:\n",
    "            md_category_list = self.df['md_categories'].astype('float64').unique().tolist()\n",
    "            if self.low_high == 'both':\n",
    "                self.moderator_statement = ('categories ' +\n",
    "                                            ' and '.join([str('%.2f' % x) for x in md_category_list]))\n",
    "            elif self.low_high == 'low':\n",
    "                self.moderator_statement = 'category ' + str('%.2f' % md_category_list[0])\n",
    "            else:\n",
    "                self.moderator_statement = 'category ' + str('%.2f' % md_category_list[-1])\n",
    "        \n",
    "        if self.test_type is 'pearsonr':\n",
    "            self.pearsonr_tuple = pearsonr(self.df[self.qe_column_name], self.df[self.qr_column_name])\n",
    "            self.pearson_r = self.pearsonr_tuple[0]\n",
    "            self.coefficient_of_determination = self.pearson_r**2\n",
    "            self.percent_predictable = self.coefficient_of_determination*100\n",
    "            self.p_value = self.pearsonr_tuple[1]\n",
    "            \n",
    "            # Figure out the adverbs\n",
    "            if self.pearson_r > 0:\n",
    "                self.adjective_positive = 'positive'\n",
    "            else:\n",
    "                self.adjective_positive = 'negative'\n",
    "            self.pearsonr_statement = str('%.2f' % self.pearson_r)\n",
    "            if self.coefficient_of_determination >= 0.25:\n",
    "                self.adverb_strong = 'strongly'\n",
    "            else:\n",
    "                self.adverb_strong = 'weakly'\n",
    "            self.cod_statement = str('%.2f' % self.coefficient_of_determination)\n",
    "            self.percent_statement = str('%.1f' % self.percent_predictable)\n",
    "            if self.p_value < 0.0001:\n",
    "                self.pvalue_statement = '<0.0001'\n",
    "            else:\n",
    "                self.pvalue_statement = '=' + str('%.4f' % self.p_value)\n",
    "            if self.p_value < 0.05:\n",
    "                self.adverb_significant = 'significantly'\n",
    "            else:\n",
    "                self.adverb_significant = 'insignificantly'\n",
    "        \n",
    "        elif self.test_type is 'chi2':\n",
    "            \n",
    "            # Get the cross tab\n",
    "            self.crosstab_df = pd.crosstab(self.df[self.cr_column_name], self.df[self.ce_column_name])\n",
    "            self.crosstab_df = self.crosstab_df.loc[:, (self.crosstab_df != 0).any(axis=0)]\n",
    "\n",
    "            if self.crosstab_df.shape[0] > 0:\n",
    "\n",
    "                # Clean up the labels of the cross tab table for visualization\n",
    "                if len(self.category_labels_list) == len(self.crosstab_df.columns.values):\n",
    "                    label_zip = zip(self.category_labels_list, self.crosstab_df.columns.values)\n",
    "                    column_labels = ['{0:} ({1:.2f})'.format(label, value) for label, value in label_zip]\n",
    "                    self.crosstab_df.columns = pd.CategoricalIndex(column_labels, categories=column_labels, ordered=True,\n",
    "                                                                  name=self.ce_informal_name, dtype='category')\n",
    "                if len(self.crosstab_df.index.values) == 2:\n",
    "                    label_zip = zip(['Low', 'High'], self.crosstab_df.index.values)\n",
    "                    index_labels = ['{0:} ({1:.2f})'.format(label, value) for label, value in label_zip]\n",
    "                    self.crosstab_df.index = pd.CategoricalIndex(index_labels, categories=index_labels, ordered=True,\n",
    "                                                                name=self.cr_informal_name, dtype='category')\n",
    "\n",
    "                # Get the column percentages table for visualization\n",
    "                self.colsum_series = self.crosstab_df.sum(axis=0)\n",
    "                self.colpct_df = self.crosstab_df/self.colsum_series\n",
    "\n",
    "                # Get the chi squared statistic\n",
    "                self.chi2_contingency_tuple = chi2_contingency(self.crosstab_df)\n",
    "                self.chi_squared_statistic = self.chi2_contingency_tuple[0]\n",
    "                self.chi_squared_statement = str('%.2f' % self.chi_squared_statistic)\n",
    "                self.p_value = self.chi2_contingency_tuple[1]\n",
    "                \n",
    "                # Figure out the adverbs\n",
    "                self.dof_statement = str('%d' % self.chi2_contingency_tuple[2])\n",
    "                if self.p_value < 0.0001:\n",
    "                    self.pvalue_statement = '&lt;0.0001'\n",
    "                else:\n",
    "                    self.pvalue_statement = '=' + str('%.4f' % self.p_value)\n",
    "                if self.p_value < 0.05:\n",
    "                    self.adverb_significant = 'significantly'\n",
    "                else:\n",
    "                    self.adverb_significant = 'insignificantly'\n",
    "            \n",
    "            self.bonferroni_threshold = 0.1/self.number_of_categories*(self.number_of_categories+1)\n",
    "        \n",
    "        elif self.test_type is 'ols':\n",
    "            self.sd_df = self.df.groupby(self.ce_column_name).std()\n",
    "            self.mean_df = self.df.groupby(self.ce_column_name).mean()\n",
    "            \n",
    "            # Use ols function for calculating the F-statistic and associated p value\n",
    "            self.fitted = smf.ols(formula=self.qr_column_name+' ~ C('+self.ce_column_name+')',\n",
    "                                  data=self.df).fit()\n",
    "            self.df_model_statement = str('%d' % self.fitted.df_model)\n",
    "            self.df_resid_statement = str('%d' % self.fitted.df_resid)\n",
    "            self.fvalue_statement = str('%.2f' % self.fitted.fvalue)\n",
    "            self.f_pvalue = self.fitted.f_pvalue\n",
    "            if self.f_pvalue < 0.0001:\n",
    "                self.f_pvalue_statement = '&lt;0.0001'\n",
    "            else:\n",
    "                self.f_pvalue_statement = '=' + str('%.4f' % self.f_pvalue)\n",
    "            \n",
    "            # Figure out the adverbs\n",
    "            if self.f_pvalue <= 0.05:\n",
    "                self.adverb_significant = 'a significantly'\n",
    "            else:\n",
    "                self.adverb_significant = 'an insignificantly'\n",
    "            \n",
    "            if len(self.df[self.ce_column_name].unique()) == 2:\n",
    "                self.low_mean = self.mean_df.loc[0, self.qr_column_name]\n",
    "                self.low_mean_statement = str('%.1f' % self.low_mean)\n",
    "                self.high_mean = self.mean_df.loc[1, self.qr_column_name]\n",
    "                self.high_mean_statement = str('%.1f' % self.high_mean)\n",
    "                if self.high_mean > self.low_mean:\n",
    "                    self.adverb_higher = 'higher'\n",
    "                else:\n",
    "                    self.adverb_higher = 'lower'\n",
    "                self.low_sd = self.sd_df.loc[0, self.qr_column_name]\n",
    "                self.low_sd_statement = str('%.1f' % self.low_sd)\n",
    "                self.high_sd = self.sd_df.loc[1, self.qr_column_name]\n",
    "                self.high_sd_statement = str('%.1f' % self.high_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "def model_interpretation(sts, sample_name='the population', verbose=False):\n",
    "        \n",
    "    if verbose:\n",
    "        \n",
    "        # Pearson’s Correlation Coefficient\n",
    "        if sts.test_type == 'pearsonr':\n",
    "            sts.verbose_html += ('<p>Association between ' + \n",
    "                             sts.qe_informal_name + ' and ' + \n",
    "                             sts.qr_informal_name + ', with ' + \n",
    "                             sts.md_informal_name + ' moderator ' +\n",
    "                             sts.moderator_statement + '<ul>')\n",
    "            for x, y in zip(sts.explanation_list, sts.pearsonr_tuple):\n",
    "                sts.verbose_html += '<li>{}: {}</li>'.format(x, y)\n",
    "            sts.verbose_html += '</ul></p>'\n",
    "        \n",
    "        # Chi Square test of Independence\n",
    "        elif sts.test_type == 'chi2':\n",
    "            \n",
    "            sts.verbose_html += ('<p>Contingency table of observed counts' +\n",
    "                                 sts.crosstab_df.to_html() + '</p><p>Column percentages' +\n",
    "                                 sts.colpct_df.to_html() + '</p><p>Chi-square<ul>')\n",
    "            for x, y in zip(sts.explanation_list, sts.chi2_contingency_tuple):\n",
    "                sts.verbose_html += '<li>{}: {}</li>'.format(x, y)\n",
    "            sts.verbose_html += '</ul></p>'\n",
    "        \n",
    "    # Pearson’s Correlation Coefficient\n",
    "    if sts.test_type == 'pearsonr':\n",
    "        if sts.low_high == 'both':\n",
    "            sts.test_type_html = ('<h3>Model Interpretation for Pearson’s Correlation Coefficient Tests, ' +\n",
    "                                  'Testing a Potential Moderator:</h3>' +\n",
    "                                  '<p>The Pearson’s Correlation Coefficient revealed that among ')\n",
    "        else:\n",
    "            sts.test_type_html = ('<p>The Pearson’s Correlation Coefficient exclusive to the ' +\n",
    "                                  sts.low_high + ' ' +\n",
    "                                  sts.moderator_statement + ' of the ' +\n",
    "                                  sts.md_informal_name + ' moderator revealed that among ')\n",
    "        sts.test_type_html += (sample_name + ', ' +\n",
    "                               sts.qe_formal_name + ' (quantitative explanatory variable), and ' +\n",
    "                               sts.qr_formal_name + ' (quantitative response variable) were ' +\n",
    "                               sts.adverb_significant + ' associated, in a ' +\n",
    "                               sts.adverb_strong + ' ' +\n",
    "                               sts.adjective_positive + r' manner, $r=' +\n",
    "                               sts.pearsonr_statement + ', p' +\n",
    "                               sts.pvalue_statement + r'$. This means that if we know the ' +\n",
    "                               sts.qe_informal_name + ' of ' +\n",
    "                               sts.low_high + ' ' +\n",
    "                               sts.md_informal_name + ' ' +\n",
    "                               sts.moderator_statement + ', we can predict ' +\n",
    "                               sts.percent_statement + '% of the ' +\n",
    "                               sts.qr_informal_name + '.</p>')\n",
    "\n",
    "    # Chi Square test of Independence\n",
    "    elif sts.test_type == 'chi2':\n",
    "        if sts.low_high == 'both':\n",
    "            sts.test_type_html = ('<h3>Model Interpretation for Chi-Square Tests, Testing a Potential Moderator:</h3>' +\n",
    "                                  '<p>A Chi Square test of Independence revealed that among ')\n",
    "        else:\n",
    "            sts.test_type_html = ('<p>The Chi Square test of Independence exclusive to the ' +\n",
    "                                  sts.low_high + ' ' +\n",
    "                                  sts.moderator_statement + ' of the ' +\n",
    "                                  sts.md_informal_name + ' moderator revealed that among ')\n",
    "        sts.test_type_html += (sample_name + ', ' +\n",
    "                               sts.ce_formal_name + ' (categorical explanatory variable), and ' +\n",
    "                               sts.cr_formal_name + ' (binary categorical response variable) were ' +\n",
    "                               sts.adverb_significant + ' associated, X2=' +\n",
    "                               sts.chi_squared_statement + ', ' +\n",
    "                               sts.dof_statement + ' df, p' +\n",
    "                               sts.pvalue_statement + '.</p>')\n",
    "        sts.test_type_html += ('<p>The categorical explanatory variable is collapsed into these ' +\n",
    "                               str(sts.number_of_categories) + ' ordered categories:' +\n",
    "                               pd.DataFrame(data=sts.category_labels_dict,\n",
    "                                            index=[sts.ce_informal_name+', group average']).T.to_html() + '</p>')\n",
    "        if sts.p_value < 0.05:\n",
    "\n",
    "            # Get post hoc statement: assume at least one pair will be statistically significant\n",
    "            sts.test_type_html += ('<p>Post hoc comparisons of ' +\n",
    "                                   sts.cr_informal_name + ' by pairs of ' +\n",
    "                                   sts.ce_informal_name + ' categories revealed that that the prevalence of ' +\n",
    "                                   sts.cr_informal_name + ' was statistically dissimilar between ')\n",
    "            similar_list = []\n",
    "            dissimilar_list = []\n",
    "            for left_category in sts.category_list:\n",
    "                for right_category in sts.category_list[1:]:\n",
    "                    if left_category < right_category:\n",
    "\n",
    "                        # Match the left or right categories; recodes don't work because of floating point error\n",
    "                        mask_series = (sts.df[sts.ce_column_name] == left_category)\n",
    "                        mask_series = mask_series | (sts.df[sts.ce_column_name] == right_category)\n",
    "                        binary_series = sts.df[mask_series][sts.ce_column_name]\n",
    "\n",
    "                        # Get the binary cross tab\n",
    "                        binary_df = pd.crosstab(sts.df[sts.cr_column_name], binary_series)\n",
    "                        binary_df = binary_df.loc[:, (binary_df != 0).any(axis=0)]\n",
    "\n",
    "                        if binary_df.shape[0] > 0:\n",
    "\n",
    "                            # Get the binary p value\n",
    "                            chi2_contingency_tuple = chi2_contingency(binary_df)\n",
    "                            binary_pvalue = chi2_contingency_tuple[1]\n",
    "\n",
    "                            # Put them in similar or dssimilar groups\n",
    "                            grouping_html = ('the <code>' +\n",
    "                                             str(sts.category_labels_dict[left_category]+' (%.3f)' % left_category) +\n",
    "                                             '</code> group compared to the <code>' +\n",
    "                                             str(sts.category_labels_dict[right_category]+' (%.3f)' % right_category) +\n",
    "                                             '</code> group')\n",
    "                            if binary_pvalue < sts.bonferroni_threshold:\n",
    "                                dissimilar_list.append(grouping_html)\n",
    "                            else:\n",
    "                                similar_list.append(grouping_html)\n",
    "\n",
    "            if len(dissimilar_list):\n",
    "                sts.test_type_html += conjunctify_list(dissimilar_list) + '.'\n",
    "            else:\n",
    "                sts.test_type_html += 'no groups at the Bonferroni adjustment of p=' + str(sts.bonferroni_threshold) + '.'\n",
    "            if len(similar_list):\n",
    "                sts.test_type_html += (' In comparison, prevalence of ' +\n",
    "                                       sts.cr_informal_name + ' was statistically similar among ' +\n",
    "                                       conjunctify_list(similar_list) + '.')\n",
    "            sts.test_type_html += '</p>'\n",
    "        \n",
    "    # Analysis of Variance (ANOVA)\n",
    "    elif sts.test_type is 'ols':\n",
    "        if sts.low_high == 'both':\n",
    "            sts.test_type_html = ('<h3>Model Interpretation for ANOVA, Testing a Potential Moderator:</h3>' +\n",
    "                                  '<p>When examining the association between ' +\n",
    "                                  sts.qr_formal_name + ' (the quantitative response variable) and ' +\n",
    "                                  sts.ce_formal_name + ' (the bi-categorical explanatory variable), ' +\n",
    "                                  'an Analysis of Variance (ANOVA) ')\n",
    "        else:\n",
    "            sts.test_type_html = ('<p>The Analysis of Variance (ANOVA) exclusive to the ' +\n",
    "                                  sts.low_high + ' ' +\n",
    "                                  sts.moderator_statement + ' of the ' +\n",
    "                                  sts.md_informal_name + ' moderator ')\n",
    "        sts.test_type_html += ('revealed that among ' +\n",
    "                               sample_name + ', those countries with a higher ' +\n",
    "                               sts.ce_informal_name + ' reported ' +\n",
    "                               sts.adverb_significant + ' ' +\n",
    "                               sts.adverb_higher + ' ' +\n",
    "                               sts.qr_informal_name + ' (Mean=' +\n",
    "                               sts.high_mean_statement + ', s.d. &plusmn;' +\n",
    "                               sts.high_sd_statement + ') compared to those with a lower ' +\n",
    "                               sts.ce_informal_name + ' (Mean=' +\n",
    "                               sts.low_mean_statement + ', s.d. &plusmn;' +\n",
    "                               sts.low_sd_statement + '), F(' +\n",
    "                               sts.df_model_statement + ', ' +\n",
    "                               sts.df_resid_statement + ')=' +\n",
    "                               sts.fvalue_statement + ', p' +\n",
    "                               sts.f_pvalue_statement + '.</p>')\n",
    "    \n",
    "    if verbose:\n",
    "        \n",
    "        # Pearson’s Correlation Coefficient\n",
    "        if sts.test_type == 'pearsonr':\n",
    "            plot_Image = add_joint_plot(sts)\n",
    "        \n",
    "        # Chi Square test of Independence\n",
    "        elif sts.test_type == 'chi2':\n",
    "            plot_Image = add_facet_grid(sts)\n",
    "        \n",
    "        # Analysis of Variance (ANOVA)\n",
    "        elif sts.test_type == 'ols':\n",
    "            sts.fitted.summary()\n",
    "            \n",
    "            # From http://www.1x1px.me/\n",
    "            file_path = '../../data/png/FFFFFF-0.0.png'\n",
    "            plot_Image = Image(data=file_path)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # From http://www.1x1px.me/\n",
    "        file_path = '../../data/png/FFFFFF-0.0.png'\n",
    "        plot_Image = Image(data=file_path)\n",
    "    \n",
    "    return HTML(sts.test_type_html + sts.verbose_html), plot_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# A response variable corresponds to a dependent variable while\n",
    "# an explanatory variable corresponds to an independent variable\n",
    "def moderator_conclusion(both_sts, low_sts, high_sts):\n",
    "    summary_html = ('<h3>Summary</h3><p>The effect of the moderating (' +\n",
    "                    both_sts.md_informal_name + ') variable is characterized ' +\n",
    "                    'statistically as an interaction; that is, a categorical (in this case low or high) ' +\n",
    "                    'variable that affects the strength of the relation between the dependent (')\n",
    "        \n",
    "    # Pearson’s Correlation Coefficient\n",
    "    if sts.test_type == 'pearsonr':\n",
    "        if high_sts.coefficient_of_determination > both_sts.coefficient_of_determination:\n",
    "            verb_high_increased = 'increased'\n",
    "        elif high_sts.coefficient_of_determination < both_sts.coefficient_of_determination:\n",
    "            verb_high_increased = 'decreased'\n",
    "        else:\n",
    "            verb_high_increased = 'stayed the same'\n",
    "        if low_sts.coefficient_of_determination > both_sts.coefficient_of_determination:\n",
    "            verb_low_increased = 'increased'\n",
    "        elif low_sts.coefficient_of_determination < both_sts.coefficient_of_determination:\n",
    "            verb_low_increased = 'decreased'\n",
    "        else:\n",
    "            verb_low_increased = 'stayed the same'\n",
    "        summary_html += (both_sts.qr_informal_name + ') variable and the independent (' +\n",
    "                         both_sts.qe_informal_name + ') variable. In our study, the strength of the relation ' +\n",
    "                         '(coefficient of determination) between ' +\n",
    "                         both_sts.qr_informal_name + ' and ' +\n",
    "                         both_sts.qe_informal_name + ', when considering only the high ' +\n",
    "                         high_sts.moderator_statement + ' of ' +\n",
    "                         high_sts.md_informal_name + ', ' +\n",
    "                         verb_high_increased + ' from ' +\n",
    "                         both_sts.cod_statement + ' to ' +\n",
    "                         high_sts.cod_statement + '. Similarly, when considering only the low ' +\n",
    "                         low_sts.moderator_statement + ' of ' +\n",
    "                         low_sts.md_informal_name + ', the strength of the relation ' +\n",
    "                         verb_low_increased + ' from ' +\n",
    "                         both_sts.cod_statement + ' to ' +\n",
    "                         low_sts.cod_statement + '.')\n",
    "        \n",
    "    # Chi Square test of Independence\n",
    "    elif sts.test_type == 'chi2':\n",
    "        if high_sts.chi_squared_statistic > both_sts.chi_squared_statistic:\n",
    "            verb_high_increased = 'got more associated'\n",
    "        elif high_sts.chi_squared_statistic < both_sts.chi_squared_statistic:\n",
    "            verb_high_increased = 'got less associated'\n",
    "        else:\n",
    "            verb_high_increased = 'stayed the same'\n",
    "        if low_sts.chi_squared_statistic > both_sts.chi_squared_statistic:\n",
    "            verb_low_increased = 'got more associated'\n",
    "        elif low_sts.chi_squared_statistic < both_sts.chi_squared_statistic:\n",
    "            verb_low_increased = 'got less associated'\n",
    "        else:\n",
    "            verb_low_increased = 'stayed the same'\n",
    "        summary_html += (both_sts.cr_informal_name + ') variable and the independent (' +\n",
    "                         both_sts.ce_informal_name + ') variable. In our study, the strength of the association ' +\n",
    "                         '(Chi Square test of Independence) between ' +\n",
    "                         both_sts.cr_informal_name + ' and ' +\n",
    "                         both_sts.ce_informal_name + ', when considering only the high ' +\n",
    "                         high_sts.moderator_statement + ' of ' +\n",
    "                         high_sts.md_informal_name + ', ' +\n",
    "                         verb_high_increased + ' from X2=' +\n",
    "                         both_sts.chi_squared_statement + ' to X2=' +\n",
    "                         high_sts.chi_squared_statement + '. Similarly, when considering only the low ' +\n",
    "                         low_sts.moderator_statement + ' of ' +\n",
    "                         low_sts.md_informal_name + ', the strength of the association ' +\n",
    "                         verb_low_increased + ' from X2=' +\n",
    "                         both_sts.chi_squared_statement + ' to X2=' +\n",
    "                         low_sts.chi_squared_statement + '.')\n",
    "        \n",
    "    # Analysis of Variance (ANOVA)\n",
    "    elif sts.test_type == 'ols':\n",
    "        if high_sts.fitted.fvalue > both_sts.fitted.fvalue:\n",
    "            verb_high_increased = 'got more independent'\n",
    "        elif high_sts.fitted.fvalue < both_sts.fitted.fvalue:\n",
    "            verb_high_increased = 'got less independent'\n",
    "        else:\n",
    "            verb_high_increased = 'stayed the same'\n",
    "        if low_sts.fitted.fvalue > both_sts.fitted.fvalue:\n",
    "            verb_low_increased = 'got more independent'\n",
    "        elif low_sts.fitted.fvalue < both_sts.fitted.fvalue:\n",
    "            verb_low_increased = 'got less independent'\n",
    "        else:\n",
    "            verb_low_increased = 'stayed the same'\n",
    "        summary_html += (both_sts.qr_informal_name + ') variable and the independent (' +\n",
    "                         both_sts.ce_informal_name + ') variable. In our study, the strength of the association ' +\n",
    "                         '(Chi Square test of Independence) between ' +\n",
    "                         both_sts.qr_informal_name + ' and ' +\n",
    "                         both_sts.ce_informal_name + ', when considering only the high ' +\n",
    "                         high_sts.moderator_statement + ' of ' +\n",
    "                         high_sts.md_informal_name + ', ' +\n",
    "                         verb_high_increased + ' from F=' +\n",
    "                         both_sts.fvalue_statement + ' to F=' +\n",
    "                         high_sts.fvalue_statement + '. Similarly, when considering only the low ' +\n",
    "                         low_sts.moderator_statement + ' of ' +\n",
    "                         low_sts.md_informal_name + ', the strength of the association ' +\n",
    "                         verb_low_increased + ' from F=' +\n",
    "                         both_sts.fvalue_statement + ' to F=' +\n",
    "                         low_sts.fvalue_statement + '.')\n",
    "    \n",
    "    return HTML(summary_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import IPython.display\n",
    "\n",
    "def pearsons_with_moderator(sts, source_name='GapMinder.org', verbose=False):\n",
    "    \n",
    "    # Interpret both groups\n",
    "    both_df = create_binned_categories(sts.df, 2, sts.md_column_name,'md')\n",
    "    both_sts = Statements(both_df, qe_column_name=sts.qe_column_name, qr_column_name=sts.qr_column_name,\n",
    "                          md_column_name=sts.md_column_name)\n",
    "    sample_name = 'the sample of ' + str(both_sts.row_count) + ' countries from ' + source_name\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(both_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    # Interpret the low group\n",
    "    mask_series = (both_df['md_categories'] == both_df['md_categories'].min())\n",
    "    low_df = both_df[mask_series].copy()\n",
    "    low_sts = Statements(low_df, qe_column_name=sts.qe_column_name, qr_column_name=sts.qr_column_name,\n",
    "                         md_column_name=sts.md_column_name, low_high='low')\n",
    "    sample_name = 'the sample of ' + str(low_sts.row_count) + ' low category countries from ' + source_name\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(low_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    # Interpret the high group\n",
    "    mask_series = (both_df['md_categories'] == both_df['md_categories'].max())\n",
    "    high_df = both_df[mask_series].copy()\n",
    "    high_sts = Statements(high_df, qe_column_name=sts.qe_column_name, qr_column_name=sts.qr_column_name,\n",
    "                          md_column_name=sts.md_column_name, low_high='high')\n",
    "    sample_name = 'the sample of ' + str(high_sts.row_count) + ' high category countries from ' + source_name\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(high_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    display(moderator_conclusion(both_sts, low_sts, high_sts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import IPython.display\n",
    "\n",
    "def chi_square_with_moderator(sts, source_name='GapMinder.org', verbose=False):\n",
    "    \n",
    "    explanation_list = ['The test statistic', 'The p-value of the test', 'Degrees of freedom',\n",
    "                        'The expected frequencies (based on the marginal sums of the table)']\n",
    "    \n",
    "    # Interpret both groups\n",
    "    both_df = create_binned_categories(sts.df, 2, sts.md_column_name,'md')\n",
    "    both_df = create_binned_categories(both_df, sts.number_of_categories, sts.ce_column_name, 'ce')\n",
    "    both_df = create_binned_categories(both_df, 2, sts.cr_column_name,'cr')\n",
    "    both_sts = Statements(both_df, number_of_categories=5, explanation_list=explanation_list,\n",
    "                          test_type='chi2',\n",
    "                          ce_column_name='ce_categories', cr_column_name='cr_categories',\n",
    "                          md_column_name=sts.md_column_name,\n",
    "                          ce_formal_name=sts.ce_formal_name, cr_formal_name=sts.cr_formal_name,\n",
    "                          ce_informal_name=sts.ce_informal_name, cr_informal_name=sts.cr_informal_name)\n",
    "    sample_name = 'the sample of ' + str(both_sts.row_count) + ' countries from ' + source_name\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(both_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    # Interpret the low group\n",
    "    mask_series = (both_df['md_categories'] == both_df['md_categories'].min())\n",
    "    low_df = both_df[mask_series].copy()\n",
    "    low_sts = Statements(low_df, number_of_categories=5, explanation_list=explanation_list,\n",
    "                         test_type='chi2',\n",
    "                         ce_column_name='ce_categories', cr_column_name='cr_categories',\n",
    "                         md_column_name=sts.md_column_name,\n",
    "                         ce_formal_name=sts.ce_formal_name, cr_formal_name=sts.cr_formal_name,\n",
    "                         ce_informal_name=sts.ce_informal_name, cr_informal_name=sts.cr_informal_name,\n",
    "                         low_high='low')\n",
    "    sample_name = ('the sample of ' +\n",
    "                   str(low_sts.row_count) + ' ' +\n",
    "                   low_sts.low_high + ' ' +\n",
    "                   low_sts.md_informal_name + ' category countries from ' +\n",
    "                   source_name)\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(low_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    # Interpret the high group\n",
    "    mask_series = (both_df['md_categories'] == both_df['md_categories'].max())\n",
    "    high_df = both_df[mask_series].copy()\n",
    "    high_sts = Statements(high_df, number_of_categories=5, explanation_list=explanation_list,\n",
    "                          test_type='chi2',\n",
    "                          ce_column_name='ce_categories', cr_column_name='cr_categories',\n",
    "                          md_column_name=sts.md_column_name,\n",
    "                          ce_formal_name=sts.ce_formal_name, cr_formal_name=sts.cr_formal_name,\n",
    "                          ce_informal_name=sts.ce_informal_name, cr_informal_name=sts.cr_informal_name,\n",
    "                          low_high='high')\n",
    "    sample_name = ('the sample of ' +\n",
    "                   str(high_sts.row_count) + ' ' +\n",
    "                   high_sts.low_high + ' ' +\n",
    "                   high_sts.md_informal_name + ' category countries from ' +\n",
    "                   source_name)\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(high_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    display(moderator_conclusion(both_sts, low_sts, high_sts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import IPython.display\n",
    "\n",
    "def anova_with_moderator(sts, source_name='GapMinder.org', verbose=False):\n",
    "    \n",
    "    # Interpret both groups\n",
    "    both_df = create_binned_categories(sts.df, 2, sts.md_column_name,'md')\n",
    "    both_df = create_binned_categories(both_df, 2, sts.ce_column_name,'ce')\n",
    "    both_sts = Statements(both_df, number_of_categories=2, test_type='ols',\n",
    "                          ce_column_name='ce_categories', qr_column_name=sts.qr_column_name,\n",
    "                          md_column_name=sts.md_column_name,\n",
    "                          ce_formal_name=sts.ce_formal_name, ce_informal_name=sts.ce_informal_name)\n",
    "    sample_name = 'the sample of ' + str(both_sts.row_count) + ' countries from ' + source_name\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(both_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    # Interpret the low group\n",
    "    mask_series = (both_df['md_categories'] == both_df['md_categories'].min())\n",
    "    low_df = both_df[mask_series].copy()\n",
    "    low_sts = Statements(low_df, number_of_categories=2, test_type='ols',\n",
    "                         ce_column_name='ce_categories', qr_column_name=sts.qr_column_name,\n",
    "                         md_column_name=sts.md_column_name,\n",
    "                         ce_formal_name=sts.ce_formal_name, ce_informal_name=sts.ce_informal_name, low_high='low')\n",
    "    sample_name = 'the sample of ' + str(low_sts.row_count) + ' low category countries from ' + source_name\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(low_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    # Interpret the high group\n",
    "    mask_series = (both_df['md_categories'] == both_df['md_categories'].max())\n",
    "    high_df = both_df[mask_series].copy()\n",
    "    high_sts = Statements(high_df, number_of_categories=2, test_type='ols',\n",
    "                          ce_column_name='ce_categories', qr_column_name=sts.qr_column_name,\n",
    "                          md_column_name=sts.md_column_name,\n",
    "                          ce_formal_name=sts.ce_formal_name, ce_informal_name=sts.ce_informal_name, low_high='high')\n",
    "    sample_name = 'the sample of ' + str(high_sts.row_count) + ' high category countries from ' + source_name\n",
    "    model_interpretation_HTML, plot_Image = model_interpretation(high_sts, sample_name, verbose)\n",
    "    display(model_interpretation_HTML)\n",
    "    display(plot_Image)\n",
    "    \n",
    "    display(moderator_conclusion(both_sts, low_sts, high_sts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
